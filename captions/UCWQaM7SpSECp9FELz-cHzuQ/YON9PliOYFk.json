[
  {
    "start": "0",
    "end": "19000"
  },
  {
    "text": "postgres sometimes referred to as the",
    "start": "0",
    "end": "2460"
  },
  {
    "text": "Toyota Camry of databases is one of the",
    "start": "2460",
    "end": "4620"
  },
  {
    "text": "most popular SQL databases used by",
    "start": "4620",
    "end": "6420"
  },
  {
    "text": "Developers but it's slow isn't it",
    "start": "6420",
    "end": "9120"
  },
  {
    "text": "shouldn't we use something fancy like",
    "start": "9120",
    "end": "10980"
  },
  {
    "text": "mongodb for Speed wrong",
    "start": "10980",
    "end": "13860"
  },
  {
    "text": "after learning these five approaches",
    "start": "13860",
    "end": "16020"
  },
  {
    "text": "you'll be able to make postgres Zoom",
    "start": "16020",
    "end": "17820"
  },
  {
    "text": "like a Bugatti",
    "start": "17820",
    "end": "19740"
  },
  {
    "start": "19000",
    "end": "71000"
  },
  {
    "text": "when you're working with long-running",
    "start": "19740",
    "end": "21180"
  },
  {
    "text": "services that talk to postgres such as a",
    "start": "21180",
    "end": "23160"
  },
  {
    "text": "rest API then you're likely making the",
    "start": "23160",
    "end": "25019"
  },
  {
    "text": "same queries over and over again",
    "start": "25019",
    "end": "27180"
  },
  {
    "text": "in this case you can increase the",
    "start": "27180",
    "end": "28800"
  },
  {
    "text": "performance of postgres by using",
    "start": "28800",
    "end": "30180"
  },
  {
    "text": "prepared statements which allows you to",
    "start": "30180",
    "end": "32220"
  },
  {
    "text": "generate queries once and reuse them",
    "start": "32220",
    "end": "34079"
  },
  {
    "text": "with different inputs",
    "start": "34079",
    "end": "35399"
  },
  {
    "text": "by doing this it reduces the need for",
    "start": "35399",
    "end": "37320"
  },
  {
    "text": "postgres to parse and plan your queries",
    "start": "37320",
    "end": "39059"
  },
  {
    "text": "every time as it reuses the initial",
    "start": "39059",
    "end": "40920"
  },
  {
    "text": "generated statement",
    "start": "40920",
    "end": "42480"
  },
  {
    "text": "creating a prepared statement is as",
    "start": "42480",
    "end": "44340"
  },
  {
    "text": "simple as using the prepare command in",
    "start": "44340",
    "end": "46260"
  },
  {
    "text": "SQL followed by the name of the",
    "start": "46260",
    "end": "48000"
  },
  {
    "text": "statement and the query we Define any",
    "start": "48000",
    "end": "50340"
  },
  {
    "text": "input parameters we expect by using the",
    "start": "50340",
    "end": "52260"
  },
  {
    "text": "dollar prefix on a numbered index",
    "start": "52260",
    "end": "53820"
  },
  {
    "text": "starting from one",
    "start": "53820",
    "end": "55800"
  },
  {
    "text": "with our prepared query created we can",
    "start": "55800",
    "end": "58079"
  },
  {
    "text": "then use the execute command to go ahead",
    "start": "58079",
    "end": "59940"
  },
  {
    "text": "and run it passing in any parameters",
    "start": "59940",
    "end": "61680"
  },
  {
    "text": "we're expecting",
    "start": "61680",
    "end": "63660"
  },
  {
    "text": "let's go ahead and Benchmark this shall",
    "start": "63660",
    "end": "65400"
  },
  {
    "text": "we",
    "start": "65400",
    "end": "66780"
  },
  {
    "text": "here you can see the difference between",
    "start": "66780",
    "end": "68159"
  },
  {
    "text": "using the prepared statements versus not",
    "start": "68159",
    "end": "70140"
  },
  {
    "text": "using them",
    "start": "70140",
    "end": "71340"
  },
  {
    "start": "71000",
    "end": "103000"
  },
  {
    "text": "whilst prepared statements provide an",
    "start": "71340",
    "end": "73320"
  },
  {
    "text": "easy performance boost there are a",
    "start": "73320",
    "end": "75119"
  },
  {
    "text": "couple of caveats to be aware of",
    "start": "75119",
    "end": "76920"
  },
  {
    "text": "the first is that a prepared statement",
    "start": "76920",
    "end": "78720"
  },
  {
    "text": "only lasts for the duration of the",
    "start": "78720",
    "end": "80700"
  },
  {
    "text": "current database session",
    "start": "80700",
    "end": "82140"
  },
  {
    "text": "which means they'll need to be recreated",
    "start": "82140",
    "end": "83520"
  },
  {
    "text": "if the session is terminated",
    "start": "83520",
    "end": "85560"
  },
  {
    "text": "the second is that a single prepared",
    "start": "85560",
    "end": "87479"
  },
  {
    "text": "statement is only scoped to that session",
    "start": "87479",
    "end": "89100"
  },
  {
    "text": "that means that multiple connections",
    "start": "89100",
    "end": "90840"
  },
  {
    "text": "cannot use the same statement instead",
    "start": "90840",
    "end": "92820"
  },
  {
    "text": "each client must create its own",
    "start": "92820",
    "end": "94200"
  },
  {
    "text": "statements",
    "start": "94200",
    "end": "95400"
  },
  {
    "text": "fortunately the popular postgres clients",
    "start": "95400",
    "end": "97500"
  },
  {
    "text": "for most languages will manage prepared",
    "start": "97500",
    "end": "99119"
  },
  {
    "text": "statements under the hood as long as",
    "start": "99119",
    "end": "100920"
  },
  {
    "text": "you're using the correct query syntax",
    "start": "100920",
    "end": "102600"
  },
  {
    "text": "this means you don't have to create them",
    "start": "102600",
    "end": "104579"
  },
  {
    "start": "103000",
    "end": "217000"
  },
  {
    "text": "manually or worry about the connection",
    "start": "104579",
    "end": "105960"
  },
  {
    "text": "logic prepared statements offer the most",
    "start": "105960",
    "end": "108540"
  },
  {
    "text": "performance Improvement when it comes to",
    "start": "108540",
    "end": "110040"
  },
  {
    "text": "long-running sessions performing the",
    "start": "110040",
    "end": "111540"
  },
  {
    "text": "same queries over and over again however",
    "start": "111540",
    "end": "114060"
  },
  {
    "text": "if you have a large amount of data in",
    "start": "114060",
    "end": "115380"
  },
  {
    "text": "your table then prepared statements",
    "start": "115380",
    "end": "117060"
  },
  {
    "text": "aren't going to help as much instead",
    "start": "117060",
    "end": "118560"
  },
  {
    "text": "you're going to want to use indexing",
    "start": "118560",
    "end": "120899"
  },
  {
    "text": "by default whenever you run a query",
    "start": "120899",
    "end": "123000"
  },
  {
    "text": "against a table postgres will look at",
    "start": "123000",
    "end": "124979"
  },
  {
    "text": "each row to see if it matches your where",
    "start": "124979",
    "end": "126479"
  },
  {
    "text": "Clause this is known as a sequential",
    "start": "126479",
    "end": "128880"
  },
  {
    "text": "scan",
    "start": "128880",
    "end": "130080"
  },
  {
    "text": "whilst sequential scans are fine for",
    "start": "130080",
    "end": "132480"
  },
  {
    "text": "smaller tables as the number of rows",
    "start": "132480",
    "end": "134400"
  },
  {
    "text": "increases the query becomes slower and",
    "start": "134400",
    "end": "136140"
  },
  {
    "text": "slower the solution to this is to use",
    "start": "136140",
    "end": "138060"
  },
  {
    "text": "indexing which enables queries to",
    "start": "138060",
    "end": "139920"
  },
  {
    "text": "perform well even as the table size",
    "start": "139920",
    "end": "141720"
  },
  {
    "text": "grows the basic idea between indexing is",
    "start": "141720",
    "end": "144120"
  },
  {
    "text": "similar to that of a directory or a",
    "start": "144120",
    "end": "145620"
  },
  {
    "text": "lookup table it works by storing the",
    "start": "145620",
    "end": "147420"
  },
  {
    "text": "associated columns in a data structure",
    "start": "147420",
    "end": "149099"
  },
  {
    "text": "with a reference to the applicable row",
    "start": "149099",
    "end": "151020"
  },
  {
    "text": "IDs in the associated table when a query",
    "start": "151020",
    "end": "153360"
  },
  {
    "text": "is made that can be served by the index",
    "start": "153360",
    "end": "155099"
  },
  {
    "text": "either completely or partially then the",
    "start": "155099",
    "end": "157980"
  },
  {
    "text": "index is used to find rows that match",
    "start": "157980",
    "end": "159780"
  },
  {
    "text": "that condition to see this in action",
    "start": "159780",
    "end": "161819"
  },
  {
    "text": "I've loaded a table with about 20",
    "start": "161819",
    "end": "163620"
  },
  {
    "text": "million users each that has an age value",
    "start": "163620",
    "end": "166140"
  },
  {
    "text": "Associated if we search for all users",
    "start": "166140",
    "end": "168360"
  },
  {
    "text": "that fall within the range of 21 to 35",
    "start": "168360",
    "end": "170459"
  },
  {
    "text": "you can see that it takes a long time to",
    "start": "170459",
    "end": "172140"
  },
  {
    "text": "run in fact we can measure the time",
    "start": "172140",
    "end": "174000"
  },
  {
    "text": "taken by using the explain analyze",
    "start": "174000",
    "end": "175920"
  },
  {
    "text": "command before our query which also",
    "start": "175920",
    "end": "177780"
  },
  {
    "text": "tells us what it's doing under the hood",
    "start": "177780",
    "end": "179220"
  },
  {
    "text": "notice the sex scan which stands for",
    "start": "179220",
    "end": "181440"
  },
  {
    "text": "sequential scan let's optimize this by",
    "start": "181440",
    "end": "183540"
  },
  {
    "text": "creating an index for our age field we",
    "start": "183540",
    "end": "186180"
  },
  {
    "text": "first use the create index command",
    "start": "186180",
    "end": "188220"
  },
  {
    "text": "followed by the index name and the table",
    "start": "188220",
    "end": "190140"
  },
  {
    "text": "we wish to create it on we then specify",
    "start": "190140",
    "end": "192540"
  },
  {
    "text": "the columns we want to use for our index",
    "start": "192540",
    "end": "194459"
  },
  {
    "text": "in this case we're just going to use the",
    "start": "194459",
    "end": "196140"
  },
  {
    "text": "age column creating an index can take a",
    "start": "196140",
    "end": "198659"
  },
  {
    "text": "while depending on the index complexity",
    "start": "198659",
    "end": "200280"
  },
  {
    "text": "and the amount of data in our table once",
    "start": "200280",
    "end": "202680"
  },
  {
    "text": "our index is created we can go ahead and",
    "start": "202680",
    "end": "204300"
  },
  {
    "text": "run the same command again here it's in",
    "start": "204300",
    "end": "206159"
  },
  {
    "text": "a fraction of the time if we run explain",
    "start": "206159",
    "end": "208019"
  },
  {
    "text": "analyze again you can see it's using the",
    "start": "208019",
    "end": "209879"
  },
  {
    "text": "index for the lookup instead of",
    "start": "209879",
    "end": "211200"
  },
  {
    "text": "sequential scanning now there are a",
    "start": "211200",
    "end": "213300"
  },
  {
    "text": "number of caveats with indexing as well",
    "start": "213300",
    "end": "215099"
  },
  {
    "text": "which I'll cover in other videos however",
    "start": "215099",
    "end": "217440"
  },
  {
    "start": "217000",
    "end": "294000"
  },
  {
    "text": "the major one to note is that indexing",
    "start": "217440",
    "end": "219360"
  },
  {
    "text": "increases the time to write to a table",
    "start": "219360",
    "end": "221400"
  },
  {
    "text": "as any new rows also have to be added to",
    "start": "221400",
    "end": "223799"
  },
  {
    "text": "the index and then balanced fortunately",
    "start": "223799",
    "end": "226080"
  },
  {
    "text": "there are a couple of ways to improve",
    "start": "226080",
    "end": "227819"
  },
  {
    "text": "right performance",
    "start": "227819",
    "end": "229680"
  },
  {
    "text": "when you have a large database table",
    "start": "229680",
    "end": "231420"
  },
  {
    "text": "operations performed against it will",
    "start": "231420",
    "end": "233159"
  },
  {
    "text": "have to run against more rows which",
    "start": "233159",
    "end": "234959"
  },
  {
    "text": "obviously impacts performance if the",
    "start": "234959",
    "end": "237360"
  },
  {
    "text": "data is able to be segmented then a",
    "start": "237360",
    "end": "239159"
  },
  {
    "text": "viable approach to improved performance",
    "start": "239159",
    "end": "240780"
  },
  {
    "text": "is to use table partitioning what",
    "start": "240780",
    "end": "243120"
  },
  {
    "text": "partitioning does is breaks up the",
    "start": "243120",
    "end": "244860"
  },
  {
    "text": "larger table into smaller tables based",
    "start": "244860",
    "end": "247140"
  },
  {
    "text": "on a column segmentation",
    "start": "247140",
    "end": "249060"
  },
  {
    "text": "these smaller tables are logically",
    "start": "249060",
    "end": "251040"
  },
  {
    "text": "grouped which allows them to operate as",
    "start": "251040",
    "end": "252780"
  },
  {
    "text": "one single table when it comes to",
    "start": "252780",
    "end": "254400"
  },
  {
    "text": "queries you can partition a table based",
    "start": "254400",
    "end": "256859"
  },
  {
    "text": "on any column that it has for example on",
    "start": "256859",
    "end": "259919"
  },
  {
    "text": "the range of a timestamp or on the hash",
    "start": "259919",
    "end": "262320"
  },
  {
    "text": "of some bytes to creative table",
    "start": "262320",
    "end": "264240"
  },
  {
    "text": "partition is as easy as calling our",
    "start": "264240",
    "end": "266220"
  },
  {
    "text": "create table statement and adding the",
    "start": "266220",
    "end": "268020"
  },
  {
    "text": "Partition by keyword at the end with the",
    "start": "268020",
    "end": "270300"
  },
  {
    "text": "type of partition to use and the column",
    "start": "270300",
    "end": "272220"
  },
  {
    "text": "to partition on in our example we're",
    "start": "272220",
    "end": "274620"
  },
  {
    "text": "going to use a range partition on the",
    "start": "274620",
    "end": "276900"
  },
  {
    "text": "event timestamp",
    "start": "276900",
    "end": "278580"
  },
  {
    "text": "once the partition has been created we",
    "start": "278580",
    "end": "280919"
  },
  {
    "text": "then create the individual partition",
    "start": "280919",
    "end": "282360"
  },
  {
    "text": "tables using the partition of command",
    "start": "282360",
    "end": "284820"
  },
  {
    "text": "followed by the range we wish to",
    "start": "284820",
    "end": "286500"
  },
  {
    "text": "Partition by in our case we're going to",
    "start": "286500",
    "end": "288419"
  },
  {
    "text": "do it by day",
    "start": "288419",
    "end": "290040"
  },
  {
    "text": "we can now perform operations on our",
    "start": "290040",
    "end": "291780"
  },
  {
    "text": "table as we would do normally",
    "start": "291780",
    "end": "293940"
  },
  {
    "text": "for reference here's a couple of",
    "start": "293940",
    "end": "295500"
  },
  {
    "start": "294000",
    "end": "402000"
  },
  {
    "text": "benchmarks for the performance",
    "start": "295500",
    "end": "297120"
  },
  {
    "text": "Improvement or partitioning in this case",
    "start": "297120",
    "end": "299280"
  },
  {
    "text": "when sequentially reading data from the",
    "start": "299280",
    "end": "301080"
  },
  {
    "text": "table that only hits a single partition",
    "start": "301080",
    "end": "302880"
  },
  {
    "text": "and in this case adding data to the",
    "start": "302880",
    "end": "304860"
  },
  {
    "text": "table when it has a number of indexes",
    "start": "304860",
    "end": "306540"
  },
  {
    "text": "you can also run these benchmarks",
    "start": "306540",
    "end": "308400"
  },
  {
    "text": "Yourself by downloading the source code",
    "start": "308400",
    "end": "310139"
  },
  {
    "text": "of course as with anything related to",
    "start": "310139",
    "end": "312240"
  },
  {
    "text": "database optimization there are some",
    "start": "312240",
    "end": "313979"
  },
  {
    "text": "caveats with partitioning as well the",
    "start": "313979",
    "end": "315900"
  },
  {
    "text": "first is that it can make queries slower",
    "start": "315900",
    "end": "317580"
  },
  {
    "text": "if they have to scan a large number of",
    "start": "317580",
    "end": "319199"
  },
  {
    "text": "multiple partitions instead of scanning",
    "start": "319199",
    "end": "321360"
  },
  {
    "text": "a single partition",
    "start": "321360",
    "end": "322800"
  },
  {
    "text": "and the second you'll need to manage the",
    "start": "322800",
    "end": "325080"
  },
  {
    "text": "creation of new partitions typically",
    "start": "325080",
    "end": "326820"
  },
  {
    "text": "with a Cron job there are some postgres",
    "start": "326820",
    "end": "328919"
  },
  {
    "text": "extensions which can help with this such",
    "start": "328919",
    "end": "330539"
  },
  {
    "text": "as PG partman and PG cron overall",
    "start": "330539",
    "end": "333419"
  },
  {
    "text": "partitions can be very useful technique",
    "start": "333419",
    "end": "335220"
  },
  {
    "text": "for improving database performance",
    "start": "335220",
    "end": "336600"
  },
  {
    "text": "however if you have a large amount of",
    "start": "336600",
    "end": "338340"
  },
  {
    "text": "data to insert in bulk there is another",
    "start": "338340",
    "end": "340199"
  },
  {
    "text": "way to improve right performance",
    "start": "340199",
    "end": "342720"
  },
  {
    "text": "if you have a large amount of data to",
    "start": "342720",
    "end": "344280"
  },
  {
    "text": "write to a table using individual",
    "start": "344280",
    "end": "346020"
  },
  {
    "text": "inserts can take a long time instead",
    "start": "346020",
    "end": "348240"
  },
  {
    "text": "it's possible to use the copy command",
    "start": "348240",
    "end": "350160"
  },
  {
    "text": "which allows you to insert a large",
    "start": "350160",
    "end": "351419"
  },
  {
    "text": "amount of data from a file or the",
    "start": "351419",
    "end": "353280"
  },
  {
    "text": "standard input",
    "start": "353280",
    "end": "354539"
  },
  {
    "text": "postgres supports either a CSV text or",
    "start": "354539",
    "end": "357720"
  },
  {
    "text": "binary format for the copy command as",
    "start": "357720",
    "end": "359639"
  },
  {
    "text": "structured data CSV tends to be",
    "start": "359639",
    "end": "362039"
  },
  {
    "text": "preferable in my experience as it",
    "start": "362039",
    "end": "363660"
  },
  {
    "text": "balances speed and compatibility",
    "start": "363660",
    "end": "366479"
  },
  {
    "text": "by using copy we can increase the speed",
    "start": "366479",
    "end": "368520"
  },
  {
    "text": "of writing data to our table by orders",
    "start": "368520",
    "end": "370320"
  },
  {
    "text": "of magnitude to show the speed",
    "start": "370320",
    "end": "372000"
  },
  {
    "text": "Improvement we get with copy here we're",
    "start": "372000",
    "end": "374100"
  },
  {
    "text": "using it to add 100 000 rows to our",
    "start": "374100",
    "end": "376680"
  },
  {
    "text": "events table on the left with the copy",
    "start": "376680",
    "end": "379320"
  },
  {
    "text": "command and on the right with a batched",
    "start": "379320",
    "end": "381720"
  },
  {
    "text": "insert you know the drill now but",
    "start": "381720",
    "end": "383940"
  },
  {
    "text": "there's a couple of caveats when using",
    "start": "383940",
    "end": "385560"
  },
  {
    "text": "copy the first is that if the data is",
    "start": "385560",
    "end": "388380"
  },
  {
    "text": "malformed at any point during the copy",
    "start": "388380",
    "end": "390300"
  },
  {
    "text": "then the entire operation will fail",
    "start": "390300",
    "end": "392819"
  },
  {
    "text": "and the second is you can only really",
    "start": "392819",
    "end": "395220"
  },
  {
    "text": "copy from the client side via sddn",
    "start": "395220",
    "end": "397620"
  },
  {
    "text": "otherwise the data file has to live on",
    "start": "397620",
    "end": "399539"
  },
  {
    "text": "the server most postgres drivers provide",
    "start": "399539",
    "end": "401819"
  },
  {
    "text": "for this however whilst copy is good for",
    "start": "401819",
    "end": "404160"
  },
  {
    "start": "402000",
    "end": "491000"
  },
  {
    "text": "large batches of data there is another",
    "start": "404160",
    "end": "405840"
  },
  {
    "text": "way to increase both read and write",
    "start": "405840",
    "end": "407699"
  },
  {
    "text": "performance which is to actually",
    "start": "407699",
    "end": "409380"
  },
  {
    "text": "separate the two tasks",
    "start": "409380",
    "end": "411360"
  },
  {
    "text": "in the majority of cases the number of",
    "start": "411360",
    "end": "413699"
  },
  {
    "text": "reads to a table typically outweighs the",
    "start": "413699",
    "end": "415680"
  },
  {
    "text": "number of writes because of this",
    "start": "415680",
    "end": "417300"
  },
  {
    "text": "performance can be improved by a concept",
    "start": "417300",
    "end": "419160"
  },
  {
    "text": "called separation of concerns where you",
    "start": "419160",
    "end": "421740"
  },
  {
    "text": "have multiple instances of your database",
    "start": "421740",
    "end": "423419"
  },
  {
    "text": "each dedicated to a different task",
    "start": "423419",
    "end": "425160"
  },
  {
    "text": "reading or writing with postgres",
    "start": "425160",
    "end": "427560"
  },
  {
    "text": "separation of concerns can be achieved",
    "start": "427560",
    "end": "429419"
  },
  {
    "text": "by using read replicas this is where you",
    "start": "429419",
    "end": "431819"
  },
  {
    "text": "have a primary database which enables",
    "start": "431819",
    "end": "433560"
  },
  {
    "text": "rights and replica databases which are",
    "start": "433560",
    "end": "435780"
  },
  {
    "text": "read-only the primary database will sync",
    "start": "435780",
    "end": "438539"
  },
  {
    "text": "its data with the read replicas this",
    "start": "438539",
    "end": "440460"
  },
  {
    "text": "enables multiple instances to be read",
    "start": "440460",
    "end": "442620"
  },
  {
    "text": "from which can horizontally scale your",
    "start": "442620",
    "end": "444599"
  },
  {
    "text": "performance",
    "start": "444599",
    "end": "446160"
  },
  {
    "text": "creating a read replica is too much for",
    "start": "446160",
    "end": "448139"
  },
  {
    "text": "this video but fortunately a lot of",
    "start": "448139",
    "end": "449880"
  },
  {
    "text": "cloud providers do allow read",
    "start": "449880",
    "end": "451080"
  },
  {
    "text": "replication there's also a Docker",
    "start": "451080",
    "end": "453120"
  },
  {
    "text": "compose in the source code which can",
    "start": "453120",
    "end": "454500"
  },
  {
    "text": "help get you started with it the main",
    "start": "454500",
    "end": "456120"
  },
  {
    "text": "caveat when using read replicas is that",
    "start": "456120",
    "end": "458280"
  },
  {
    "text": "it does introduce additional complexity",
    "start": "458280",
    "end": "459900"
  },
  {
    "text": "to your system and requires careful",
    "start": "459900",
    "end": "461759"
  },
  {
    "text": "setup and monitoring to ensure that the",
    "start": "461759",
    "end": "463740"
  },
  {
    "text": "rib replicas are working correctly there",
    "start": "463740",
    "end": "465900"
  },
  {
    "text": "are a couple of tools such as PG bouncer",
    "start": "465900",
    "end": "467880"
  },
  {
    "text": "or PG pool 2 to help abstract some of",
    "start": "467880",
    "end": "470400"
  },
  {
    "text": "this complexity away with these five",
    "start": "470400",
    "end": "472620"
  },
  {
    "text": "approaches you can turn postgres not",
    "start": "472620",
    "end": "474780"
  },
  {
    "text": "only into a Bugatti but also a Lambo or",
    "start": "474780",
    "end": "477300"
  },
  {
    "text": "a Ferrari",
    "start": "477300",
    "end": "478979"
  },
  {
    "text": "we only touched on the surface of what",
    "start": "478979",
    "end": "480960"
  },
  {
    "text": "can be achieved with these approaches",
    "start": "480960",
    "end": "482340"
  },
  {
    "text": "and I'll be doing more in-depth videos",
    "start": "482340",
    "end": "484139"
  },
  {
    "text": "in the future until then however you can",
    "start": "484139",
    "end": "486599"
  },
  {
    "text": "lower the roof and feel the wind in your",
    "start": "486599",
    "end": "488280"
  },
  {
    "text": "hair",
    "start": "488280",
    "end": "490460"
  }
]