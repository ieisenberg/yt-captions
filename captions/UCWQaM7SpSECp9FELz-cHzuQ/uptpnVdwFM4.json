[
  {
    "start": "0",
    "end": "67000"
  },
  {
    "text": "it's no secret that I'm a fan of Reddus",
    "start": "160",
    "end": "3679"
  },
  {
    "text": "or at least I was a fan of Reddus until",
    "start": "3679",
    "end": "6400"
  },
  {
    "text": "they went and forked things up these",
    "start": "6400",
    "end": "9120"
  },
  {
    "text": "days I'm now pinning pictures of",
    "start": "9120",
    "end": "10960"
  },
  {
    "text": "Valkyrie on my wall instead in any case",
    "start": "10960",
    "end": "14320"
  },
  {
    "text": "I've been using Reddus for a good number",
    "start": "14320",
    "end": "16400"
  },
  {
    "text": "of years and through that time I've",
    "start": "16400",
    "end": "19199"
  },
  {
    "text": "always been fascinated with how",
    "start": "19199",
    "end": "20720"
  },
  {
    "text": "incredibly fast both Reddus and its",
    "start": "20720",
    "end": "22960"
  },
  {
    "text": "forks such as Valkyrie can be in fact by",
    "start": "22960",
    "end": "26320"
  },
  {
    "text": "just running a single instance of Valky",
    "start": "26320",
    "end": "28560"
  },
  {
    "text": "on a bare metal machine you can easily",
    "start": "28560",
    "end": "30880"
  },
  {
    "text": "exceed 100,000 requests per second which",
    "start": "30880",
    "end": "34239"
  },
  {
    "text": "is a lot of throughput whilst 100,000",
    "start": "34239",
    "end": "37600"
  },
  {
    "text": "requests per second is pretty impressive",
    "start": "37600",
    "end": "40239"
  },
  {
    "text": "i wanted to see how difficult it would",
    "start": "40239",
    "end": "41920"
  },
  {
    "text": "be to push this even further say 10",
    "start": "41920",
    "end": "45040"
  },
  {
    "text": "times further to 1 million requests per",
    "start": "45040",
    "end": "48320"
  },
  {
    "text": "second so how does one go about",
    "start": "48320",
    "end": "50559"
  },
  {
    "text": "increasing the number of requests per",
    "start": "50559",
    "end": "52480"
  },
  {
    "text": "second or RPS that Reddis can handle to",
    "start": "52480",
    "end": "55840"
  },
  {
    "text": "greater than 1 million well as it turns",
    "start": "55840",
    "end": "59039"
  },
  {
    "text": "out there are a number of different ways",
    "start": "59039",
    "end": "61120"
  },
  {
    "text": "to do so each with their own pros and",
    "start": "61120",
    "end": "64080"
  },
  {
    "text": "cons depending on the situation that you",
    "start": "64080",
    "end": "66479"
  },
  {
    "text": "find yourself in the first approach I",
    "start": "66479",
    "end": "69040"
  },
  {
    "start": "67000",
    "end": "303000"
  },
  {
    "text": "decided to take when it came to scaling",
    "start": "69040",
    "end": "70640"
  },
  {
    "text": "Reddis or Valky in this case was to just",
    "start": "70640",
    "end": "73360"
  },
  {
    "text": "run it on a bigger machine this is known",
    "start": "73360",
    "end": "76960"
  },
  {
    "text": "as vertical scaling and whilst it can be",
    "start": "76960",
    "end": "79920"
  },
  {
    "text": "effective for some software when it",
    "start": "79920",
    "end": "82159"
  },
  {
    "text": "comes to reddis it's a little bit more",
    "start": "82159",
    "end": "84080"
  },
  {
    "text": "complicated to show what I mean I",
    "start": "84080",
    "end": "86320"
  },
  {
    "text": "decided to deploy a Valky instance onto",
    "start": "86320",
    "end": "88720"
  },
  {
    "text": "three separate machines the first being",
    "start": "88720",
    "end": "91280"
  },
  {
    "text": "a Blink Mini S12 which has a lowowered 4",
    "start": "91280",
    "end": "95119"
  },
  {
    "text": "core CPU the Intel",
    "start": "95119",
    "end": "97560"
  },
  {
    "text": "N95 which is the least powerful of the",
    "start": "97560",
    "end": "100640"
  },
  {
    "text": "three the second machine that I",
    "start": "100640",
    "end": "102400"
  },
  {
    "text": "installed Valkyrie on I've defined as",
    "start": "102400",
    "end": "104400"
  },
  {
    "text": "the mid machine which is yet another",
    "start": "104400",
    "end": "106880"
  },
  {
    "text": "Beink mini PC this one the STR6 which",
    "start": "106880",
    "end": "110960"
  },
  {
    "text": "runs an AMD 7735 CPU with eight cores",
    "start": "110960",
    "end": "115360"
  },
  {
    "text": "and 16 threads so it's a little bit more",
    "start": "115360",
    "end": "118320"
  },
  {
    "text": "powerful the final machine in this",
    "start": "118320",
    "end": "120159"
  },
  {
    "text": "testing is the big boy this is the AMD",
    "start": "120159",
    "end": "123280"
  },
  {
    "text": "Thread Ripper",
    "start": "123280",
    "end": "124680"
  },
  {
    "text": "3970X with a massive 32 cores and 64",
    "start": "124680",
    "end": "128319"
  },
  {
    "text": "threads as well as boasting 128 gigs of",
    "start": "128319",
    "end": "131360"
  },
  {
    "text": "RAM to test how much throughput each of",
    "start": "131360",
    "end": "133760"
  },
  {
    "text": "these machines can handle I'm going to",
    "start": "133760",
    "end": "135520"
  },
  {
    "text": "go ahead and use the following mem",
    "start": "135520",
    "end": "137440"
  },
  {
    "text": "benchmark command which is pretty much",
    "start": "137440",
    "end": "139920"
  },
  {
    "text": "the standard tool when it comes to",
    "start": "139920",
    "end": "141599"
  },
  {
    "text": "benchmarking Reddus and its forks",
    "start": "141599",
    "end": "144000"
  },
  {
    "text": "additionally I'm running this tool on",
    "start": "144000",
    "end": "145760"
  },
  {
    "text": "another machine and sending the commands",
    "start": "145760",
    "end": "147760"
  },
  {
    "text": "over the network in order to simulate",
    "start": "147760",
    "end": "149959"
  },
  {
    "text": "realworld usage and when I run this",
    "start": "149959",
    "end": "152560"
  },
  {
    "text": "command you can see that the mini PC is",
    "start": "152560",
    "end": "155200"
  },
  {
    "text": "handling a huge amount of requests per",
    "start": "155200",
    "end": "157920"
  },
  {
    "text": "second around 200,000 which honestly is",
    "start": "157920",
    "end": "161840"
  },
  {
    "text": "pretty impressive this high number is",
    "start": "161840",
    "end": "164239"
  },
  {
    "text": "already happening because I'm running on",
    "start": "164239",
    "end": "165920"
  },
  {
    "text": "my local network and running the Valky",
    "start": "165920",
    "end": "168319"
  },
  {
    "text": "instance on bare metal which both Reddus",
    "start": "168319",
    "end": "170879"
  },
  {
    "text": "and Valky handle really well if instead",
    "start": "170879",
    "end": "173760"
  },
  {
    "text": "I was making use of a couple of VPS",
    "start": "173760",
    "end": "175519"
  },
  {
    "text": "instances one to host the instance and",
    "start": "175519",
    "end": "178160"
  },
  {
    "text": "one to actually perform the test in the",
    "start": "178160",
    "end": "180560"
  },
  {
    "text": "same data center you'll see that the",
    "start": "180560",
    "end": "182319"
  },
  {
    "text": "request count drops significantly and if",
    "start": "182319",
    "end": "185440"
  },
  {
    "text": "I try to go ahead and benchmark this",
    "start": "185440",
    "end": "187040"
  },
  {
    "text": "from my machine to one of these VPS",
    "start": "187040",
    "end": "188959"
  },
  {
    "text": "instances over the internet then it",
    "start": "188959",
    "end": "191599"
  },
  {
    "text": "drops even further so that's one thing",
    "start": "191599",
    "end": "193920"
  },
  {
    "text": "to consider if you want to scale Reddis",
    "start": "193920",
    "end": "196159"
  },
  {
    "text": "bare metal is best but also colllocating",
    "start": "196159",
    "end": "199519"
  },
  {
    "text": "or running in the same data center is",
    "start": "199519",
    "end": "202159"
  },
  {
    "text": "really important if you're sending",
    "start": "202159",
    "end": "204560"
  },
  {
    "text": "commands halfway across the world then",
    "start": "204560",
    "end": "206560"
  },
  {
    "text": "you're going to have a bad time in any",
    "start": "206560",
    "end": "208319"
  },
  {
    "text": "case going back to my local area network",
    "start": "208319",
    "end": "210400"
  },
  {
    "text": "setup the four core machine did pretty",
    "start": "210400",
    "end": "213040"
  },
  {
    "text": "well so let's go ahead and see how it",
    "start": "213040",
    "end": "215120"
  },
  {
    "text": "works when it comes to the midlevel",
    "start": "215120",
    "end": "217200"
  },
  {
    "text": "machine as you can see we're actually",
    "start": "217200",
    "end": "219440"
  },
  {
    "text": "getting around the same number of",
    "start": "219440",
    "end": "221120"
  },
  {
    "text": "operations per second which at first may",
    "start": "221120",
    "end": "224959"
  },
  {
    "text": "feel a little surprising this however is",
    "start": "224959",
    "end": "227680"
  },
  {
    "text": "actually expected when it comes to",
    "start": "227680",
    "end": "229360"
  },
  {
    "text": "Reddus this is because Reddus and some",
    "start": "229360",
    "end": "232000"
  },
  {
    "text": "of its forks such as Valky only make use",
    "start": "232000",
    "end": "234720"
  },
  {
    "text": "of a single thread when it comes to",
    "start": "234720",
    "end": "236560"
  },
  {
    "text": "handling commands meaning that more",
    "start": "236560",
    "end": "239040"
  },
  {
    "text": "cores or more CPUs isn't going to",
    "start": "239040",
    "end": "241920"
  },
  {
    "text": "increase performance in fact in some",
    "start": "241920",
    "end": "244560"
  },
  {
    "text": "cases it can actually hinder it for",
    "start": "244560",
    "end": "247040"
  },
  {
    "text": "example if I go ahead and run the",
    "start": "247040",
    "end": "248400"
  },
  {
    "text": "benchmark against my Valkyrie instance",
    "start": "248400",
    "end": "250400"
  },
  {
    "text": "on the 32 core machine you can see now",
    "start": "250400",
    "end": "253360"
  },
  {
    "text": "we're actually producing less operations",
    "start": "253360",
    "end": "255920"
  },
  {
    "text": "down around 1/4 what we were before this",
    "start": "255920",
    "end": "258479"
  },
  {
    "text": "is because the 32 core Thread Ripper",
    "start": "258479",
    "end": "260720"
  },
  {
    "text": "machine actually has worse single core",
    "start": "260720",
    "end": "263199"
  },
  {
    "text": "performance than the other two and",
    "start": "263199",
    "end": "265280"
  },
  {
    "text": "because Reddus is so dependent on single",
    "start": "265280",
    "end": "267440"
  },
  {
    "text": "core performance then it's having a",
    "start": "267440",
    "end": "269680"
  },
  {
    "text": "negative impact now there are both forks",
    "start": "269680",
    "end": "272400"
  },
  {
    "text": "of Reddus and Reddis compatible",
    "start": "272400",
    "end": "274160"
  },
  {
    "text": "solutions that are better able to make",
    "start": "274160",
    "end": "276080"
  },
  {
    "text": "use of multiple cores on a machine with",
    "start": "276080",
    "end": "278800"
  },
  {
    "text": "one such solution being Dragonfly DB who",
    "start": "278800",
    "end": "282160"
  },
  {
    "text": "are also the sponsors of today's video",
    "start": "282160",
    "end": "284560"
  },
  {
    "text": "we'll talk a little bit more about",
    "start": "284560",
    "end": "285680"
  },
  {
    "text": "Dragonfly DB later on and how they",
    "start": "285680",
    "end": "288479"
  },
  {
    "text": "provide increased performance on",
    "start": "288479",
    "end": "290080"
  },
  {
    "text": "multi-core machines however for the",
    "start": "290080",
    "end": "292160"
  },
  {
    "text": "meantime we're going to go ahead and",
    "start": "292160",
    "end": "293360"
  },
  {
    "text": "focus on Reddus or Valky and see how we",
    "start": "293360",
    "end": "296720"
  },
  {
    "text": "can get it to perform millions of",
    "start": "296720",
    "end": "298320"
  },
  {
    "text": "requests per second using a single",
    "start": "298320",
    "end": "300639"
  },
  {
    "text": "threaded instance one approach to doing",
    "start": "300639",
    "end": "303600"
  },
  {
    "start": "303000",
    "end": "452000"
  },
  {
    "text": "so is to make use of something called",
    "start": "303600",
    "end": "306320"
  },
  {
    "text": "pipelining pipelining is a technique for",
    "start": "306320",
    "end": "308960"
  },
  {
    "text": "improving performance by issuing",
    "start": "308960",
    "end": "310560"
  },
  {
    "text": "multiple commands at once without",
    "start": "310560",
    "end": "312639"
  },
  {
    "text": "waiting for the response to each",
    "start": "312639",
    "end": "314320"
  },
  {
    "text": "individual command to return it's very",
    "start": "314320",
    "end": "316720"
  },
  {
    "text": "similar to the concept of batching to",
    "start": "316720",
    "end": "319039"
  },
  {
    "text": "show the performance improvement of",
    "start": "319039",
    "end": "320479"
  },
  {
    "text": "using pipelining if I go ahead and use",
    "start": "320479",
    "end": "322400"
  },
  {
    "text": "the benchmark test tool again this time",
    "start": "322400",
    "end": "324880"
  },
  {
    "text": "passing in the d--peline flag setting it",
    "start": "324880",
    "end": "327680"
  },
  {
    "text": "to be two so we're sending a pipeline of",
    "start": "327680",
    "end": "330160"
  },
  {
    "text": "two commands at once as you can see",
    "start": "330160",
    "end": "332479"
  },
  {
    "text": "we're now effectively doubling our",
    "start": "332479",
    "end": "334080"
  },
  {
    "text": "throughput without making any hardware",
    "start": "334080",
    "end": "336320"
  },
  {
    "text": "changes to our instance this is pretty",
    "start": "336320",
    "end": "339199"
  },
  {
    "text": "great but how far can we actually push",
    "start": "339199",
    "end": "341280"
  },
  {
    "text": "it well if I go ahead and set a pipeline",
    "start": "341280",
    "end": "343680"
  },
  {
    "text": "to be an order of magnitude higher from",
    "start": "343680",
    "end": "346080"
  },
  {
    "text": "2 to 10 you can see now we're pushing",
    "start": "346080",
    "end": "348400"
  },
  {
    "text": "over 1 million requests a second give or",
    "start": "348400",
    "end": "351199"
  },
  {
    "text": "take pretty cool however we don't just",
    "start": "351199",
    "end": "353520"
  },
  {
    "text": "have to stop here and we can actually",
    "start": "353520",
    "end": "355360"
  },
  {
    "text": "push pipelining a little further let's",
    "start": "355360",
    "end": "357600"
  },
  {
    "text": "say we go ahead and set a pipeline of",
    "start": "357600",
    "end": "359759"
  },
  {
    "text": "100 this time we're now maxing out at",
    "start": "359759",
    "end": "362639"
  },
  {
    "text": "about 2.5 million requests a second",
    "start": "362639",
    "end": "365759"
  },
  {
    "text": "whilst this is incredibly fast you'll",
    "start": "365759",
    "end": "367759"
  },
  {
    "text": "notice it's slightly less than we would",
    "start": "367759",
    "end": "369759"
  },
  {
    "text": "expect given that we were hitting 1",
    "start": "369759",
    "end": "371759"
  },
  {
    "text": "million operations when it came to a",
    "start": "371759",
    "end": "373360"
  },
  {
    "text": "pipeline size of 10 we should expect to",
    "start": "373360",
    "end": "375600"
  },
  {
    "text": "see 10 million operations when it came",
    "start": "375600",
    "end": "377280"
  },
  {
    "text": "to a pipeline size of 100 unfortunately",
    "start": "377280",
    "end": "379680"
  },
  {
    "text": "however we're actually hitting a",
    "start": "379680",
    "end": "381280"
  },
  {
    "text": "bottleneck which is caused by the",
    "start": "381280",
    "end": "383280"
  },
  {
    "text": "available bandwidth on my local network",
    "start": "383280",
    "end": "386080"
  },
  {
    "text": "either way reaching 2.5 million",
    "start": "386080",
    "end": "388080"
  },
  {
    "text": "operations per second is impressive and",
    "start": "388080",
    "end": "390560"
  },
  {
    "text": "pipelining is an effective way to",
    "start": "390560",
    "end": "392400"
  },
  {
    "text": "achieve this especially as it's",
    "start": "392400",
    "end": "394080"
  },
  {
    "text": "supported by most Reddis clients already",
    "start": "394080",
    "end": "396560"
  },
  {
    "text": "and it's pretty easy to perform in Go",
    "start": "396560",
    "end": "399120"
  },
  {
    "text": "you can achieve this by using the",
    "start": "399120",
    "end": "400400"
  },
  {
    "text": "pipeline method as follows then sending",
    "start": "400400",
    "end": "402639"
  },
  {
    "text": "commands to this pipeline followed by",
    "start": "402639",
    "end": "404639"
  },
  {
    "text": "executing it and all of the responses",
    "start": "404639",
    "end": "406800"
  },
  {
    "text": "will be in the same order you pass them",
    "start": "406800",
    "end": "408800"
  },
  {
    "text": "in despite being simple to implement",
    "start": "408800",
    "end": "411120"
  },
  {
    "text": "there is unfortunately a catch",
    "start": "411120",
    "end": "413360"
  },
  {
    "text": "pipelining isn't always possible for",
    "start": "413360",
    "end": "415440"
  },
  {
    "text": "every use case when it comes to working",
    "start": "415440",
    "end": "417360"
  },
  {
    "text": "with Reddus as it requires you to have a",
    "start": "417360",
    "end": "419840"
  },
  {
    "text": "batch of commands in order to send up in",
    "start": "419840",
    "end": "423039"
  },
  {
    "text": "some situations this is actually going",
    "start": "423039",
    "end": "425120"
  },
  {
    "text": "to be the case for example if you want",
    "start": "425120",
    "end": "427280"
  },
  {
    "text": "to send multiple commands at once such",
    "start": "427280",
    "end": "429039"
  },
  {
    "text": "as if you're enriching a bunch of data",
    "start": "429039",
    "end": "430800"
  },
  {
    "text": "and need to perform a get command for a",
    "start": "430800",
    "end": "433360"
  },
  {
    "text": "large number of keys you can effectively",
    "start": "433360",
    "end": "435759"
  },
  {
    "text": "send all of these keys up to Reddus in a",
    "start": "435759",
    "end": "437840"
  },
  {
    "text": "batch of say 100 however for most cases",
    "start": "437840",
    "end": "440800"
  },
  {
    "text": "when it comes to Reddus pipelining just",
    "start": "440800",
    "end": "442880"
  },
  {
    "text": "doesn't really make sense as you don't",
    "start": "442880",
    "end": "445199"
  },
  {
    "text": "have a batch of commands that you can",
    "start": "445199",
    "end": "446800"
  },
  {
    "text": "send at once so whilst it is a great way",
    "start": "446800",
    "end": "449440"
  },
  {
    "text": "to improve performance it doesn't work",
    "start": "449440",
    "end": "451280"
  },
  {
    "text": "for every case not only this but there's",
    "start": "451280",
    "end": "453599"
  },
  {
    "start": "452000",
    "end": "499000"
  },
  {
    "text": "also some other limitations when it",
    "start": "453599",
    "end": "454960"
  },
  {
    "text": "comes to Reddus that pipelining can't",
    "start": "454960",
    "end": "456880"
  },
  {
    "text": "resolve specifically when it comes to",
    "start": "456880",
    "end": "459199"
  },
  {
    "text": "resources as we've seen already Reddus",
    "start": "459199",
    "end": "461919"
  },
  {
    "text": "uses a single thread when it comes to",
    "start": "461919",
    "end": "463840"
  },
  {
    "text": "handling commands which means even",
    "start": "463840",
    "end": "466000"
  },
  {
    "text": "though it's incredibly fast there's",
    "start": "466000",
    "end": "467759"
  },
  {
    "text": "going to be an upper limit as to what a",
    "start": "467759",
    "end": "469599"
  },
  {
    "text": "single instance can do not only this but",
    "start": "469599",
    "end": "472319"
  },
  {
    "text": "the network stack itself can also be a",
    "start": "472319",
    "end": "474879"
  },
  {
    "text": "bottleneck especially when dealing with",
    "start": "474879",
    "end": "476720"
  },
  {
    "text": "operations over the internet that we saw",
    "start": "476720",
    "end": "478800"
  },
  {
    "text": "already lastly one thing that's really",
    "start": "478800",
    "end": "481039"
  },
  {
    "text": "important when it comes to Reddus and",
    "start": "481039",
    "end": "482560"
  },
  {
    "text": "Valky is system memory given that Reddus",
    "start": "482560",
    "end": "485440"
  },
  {
    "text": "is an in-memory data store and more",
    "start": "485440",
    "end": "487840"
  },
  {
    "text": "memory means more data stored and less",
    "start": "487840",
    "end": "490280"
  },
  {
    "text": "evictions therefore whilst pipelining is",
    "start": "490280",
    "end": "492800"
  },
  {
    "text": "a great way to get more performance out",
    "start": "492800",
    "end": "494479"
  },
  {
    "text": "of your Reddus instance it's not going",
    "start": "494479",
    "end": "496560"
  },
  {
    "text": "to work when it comes to true scaling so",
    "start": "496560",
    "end": "499599"
  },
  {
    "start": "499000",
    "end": "544000"
  },
  {
    "text": "how can we achieve 1 million operations",
    "start": "499599",
    "end": "501759"
  },
  {
    "text": "per second without needing to use",
    "start": "501759",
    "end": "503639"
  },
  {
    "text": "pipelining well that's where another",
    "start": "503639",
    "end": "506080"
  },
  {
    "text": "approach comes in horizontal scaling",
    "start": "506080",
    "end": "509199"
  },
  {
    "text": "horizontal scaling is where you increase",
    "start": "509199",
    "end": "511280"
  },
  {
    "text": "the performance of a system by scaling",
    "start": "511280",
    "end": "513440"
  },
  {
    "text": "the number of instances rather than the",
    "start": "513440",
    "end": "515839"
  },
  {
    "text": "amount of resources per instance",
    "start": "515839",
    "end": "518320"
  },
  {
    "text": "basically you're deploying multiple",
    "start": "518320",
    "end": "520000"
  },
  {
    "text": "instances of Reddus or Valky across",
    "start": "520000",
    "end": "522640"
  },
  {
    "text": "multiple machines however just deploying",
    "start": "522640",
    "end": "525040"
  },
  {
    "text": "these across multiple machines doesn't",
    "start": "525040",
    "end": "526880"
  },
  {
    "text": "really do that much by itself instead",
    "start": "526880",
    "end": "529200"
  },
  {
    "text": "you need to couple these deployments",
    "start": "529200",
    "end": "530800"
  },
  {
    "text": "with a horizontal scaling strategy in",
    "start": "530800",
    "end": "533600"
  },
  {
    "text": "order to determine how data is both",
    "start": "533600",
    "end": "535440"
  },
  {
    "text": "stored and retrieved when it comes to",
    "start": "535440",
    "end": "537680"
  },
  {
    "text": "Reddus and well most data storage",
    "start": "537680",
    "end": "540360"
  },
  {
    "text": "applications there are two horizontal",
    "start": "540360",
    "end": "542560"
  },
  {
    "text": "scaling strategies that you can take the",
    "start": "542560",
    "end": "545200"
  },
  {
    "start": "544000",
    "end": "800000"
  },
  {
    "text": "first horizontal scaling strategy is",
    "start": "545200",
    "end": "547360"
  },
  {
    "text": "known as read",
    "start": "547360",
    "end": "549160"
  },
  {
    "text": "replication this is where you deploy a",
    "start": "549160",
    "end": "551839"
  },
  {
    "text": "single instance known as the primary and",
    "start": "551839",
    "end": "554880"
  },
  {
    "text": "a number of other instances called",
    "start": "554880",
    "end": "556959"
  },
  {
    "text": "replicas these replicas are constrained",
    "start": "556959",
    "end": "560399"
  },
  {
    "text": "to read operations only with the only",
    "start": "560399",
    "end": "563600"
  },
  {
    "text": "instance that allows data to be written",
    "start": "563600",
    "end": "565600"
  },
  {
    "text": "to it being the primary when data is",
    "start": "565600",
    "end": "568160"
  },
  {
    "text": "then written to this primary instance",
    "start": "568160",
    "end": "570080"
  },
  {
    "text": "it's then synchronized to the other",
    "start": "570080",
    "end": "571920"
  },
  {
    "text": "replicas in the replica set to set up",
    "start": "571920",
    "end": "574720"
  },
  {
    "text": "read replication in Reddus and Valky is",
    "start": "574720",
    "end": "577680"
  },
  {
    "text": "actually incredibly simple you can",
    "start": "577680",
    "end": "579680"
  },
  {
    "text": "either do so in the configuration or by",
    "start": "579680",
    "end": "582160"
  },
  {
    "text": "sending the replica of command which you",
    "start": "582160",
    "end": "584320"
  },
  {
    "text": "can do through the CLI in my case I",
    "start": "584320",
    "end": "586720"
  },
  {
    "text": "decided to set this up on both my small",
    "start": "586720",
    "end": "588640"
  },
  {
    "text": "instance and on my thread ripper to",
    "start": "588640",
    "end": "590959"
  },
  {
    "text": "become replicas of the mid instance as",
    "start": "590959",
    "end": "593600"
  },
  {
    "text": "you can see I'm using the replica of",
    "start": "593600",
    "end": "595279"
  },
  {
    "text": "command to achieve this passing in the",
    "start": "595279",
    "end": "597720"
  },
  {
    "text": "midhost name if your primary instance",
    "start": "597720",
    "end": "600560"
  },
  {
    "text": "requires authentication then there are a",
    "start": "600560",
    "end": "602800"
  },
  {
    "text": "couple of other steps you need to take i",
    "start": "602800",
    "end": "604959"
  },
  {
    "text": "recommend reading the Reddus or Valky",
    "start": "604959",
    "end": "606880"
  },
  {
    "text": "documentation for whichever version",
    "start": "606880",
    "end": "608640"
  },
  {
    "text": "you're using in any case upon executing",
    "start": "608640",
    "end": "611040"
  },
  {
    "text": "these commands replication is now set up",
    "start": "611040",
    "end": "614160"
  },
  {
    "text": "and if I go ahead and make a right to my",
    "start": "614160",
    "end": "616000"
  },
  {
    "text": "primary instance you'll see that this",
    "start": "616000",
    "end": "617920"
  },
  {
    "text": "key is now available on the two replicas",
    "start": "617920",
    "end": "620560"
  },
  {
    "text": "as well therefore we can now go ahead",
    "start": "620560",
    "end": "623120"
  },
  {
    "text": "and make use of these in order to",
    "start": "623120",
    "end": "624720"
  },
  {
    "text": "improve the throughput of our Reddus",
    "start": "624720",
    "end": "626959"
  },
  {
    "text": "deployment in order to test how much",
    "start": "626959",
    "end": "629040"
  },
  {
    "text": "throughput we now have we can modify our",
    "start": "629040",
    "end": "631600"
  },
  {
    "text": "benchmark command so that we only write",
    "start": "631600",
    "end": "633839"
  },
  {
    "text": "data to the primary and read from the",
    "start": "633839",
    "end": "635959"
  },
  {
    "text": "replicas which is done using the",
    "start": "635959",
    "end": "638399"
  },
  {
    "text": "following three commands setting the",
    "start": "638399",
    "end": "640399"
  },
  {
    "text": "ratio for the primary to be write only",
    "start": "640399",
    "end": "642959"
  },
  {
    "text": "and setting the ratio for the replicas",
    "start": "642959",
    "end": "645040"
  },
  {
    "text": "to be read only now if I go ahead and",
    "start": "645040",
    "end": "646959"
  },
  {
    "text": "run this for about 60 seconds you can",
    "start": "646959",
    "end": "649279"
  },
  {
    "text": "see the performance is pretty good we're",
    "start": "649279",
    "end": "651600"
  },
  {
    "text": "hitting around 300,000 requests per",
    "start": "651600",
    "end": "653839"
  },
  {
    "text": "second using three instances with two",
    "start": "653839",
    "end": "656160"
  },
  {
    "text": "replicas so all it would take to reach 1",
    "start": "656160",
    "end": "658800"
  },
  {
    "text": "million would be adding in maybe another",
    "start": "658800",
    "end": "661040"
  },
  {
    "text": "seven whilst this is perfectly",
    "start": "661040",
    "end": "663040"
  },
  {
    "text": "achievable when it comes to realworld",
    "start": "663040",
    "end": "665519"
  },
  {
    "text": "setups read replication isn't always",
    "start": "665519",
    "end": "668160"
  },
  {
    "text": "viable for starters whilst our total",
    "start": "668160",
    "end": "670640"
  },
  {
    "text": "performance across the three nodes has",
    "start": "670640",
    "end": "672480"
  },
  {
    "text": "increased we haven't actually improved",
    "start": "672480",
    "end": "674560"
  },
  {
    "text": "our write performance only our reads",
    "start": "674560",
    "end": "677360"
  },
  {
    "text": "this is because we're constrained to",
    "start": "677360",
    "end": "678880"
  },
  {
    "text": "only being able to write to a single",
    "start": "678880",
    "end": "680800"
  },
  {
    "text": "node which means our performance is",
    "start": "680800",
    "end": "683040"
  },
  {
    "text": "constrained to this one instance in some",
    "start": "683040",
    "end": "686079"
  },
  {
    "text": "setups this is actually okay especially",
    "start": "686079",
    "end": "688880"
  },
  {
    "text": "when it comes to more read workflows",
    "start": "688880",
    "end": "691360"
  },
  {
    "text": "where having multiple instances or",
    "start": "691360",
    "end": "693360"
  },
  {
    "text": "multiple replicas where you can read",
    "start": "693360",
    "end": "694959"
  },
  {
    "text": "from will directly scale performance",
    "start": "694959",
    "end": "697440"
  },
  {
    "text": "however there are still some trade-offs",
    "start": "697440",
    "end": "699440"
  },
  {
    "text": "when it comes to using this horizontal",
    "start": "699440",
    "end": "701200"
  },
  {
    "text": "scaling strategy for one thing this",
    "start": "701200",
    "end": "703920"
  },
  {
    "text": "approach is more expensive when it comes",
    "start": "703920",
    "end": "706160"
  },
  {
    "text": "to memory as we're effectively having to",
    "start": "706160",
    "end": "708399"
  },
  {
    "text": "replicate our entire data set across",
    "start": "708399",
    "end": "710560"
  },
  {
    "text": "multiple nodes this means when it comes",
    "start": "710560",
    "end": "712640"
  },
  {
    "text": "to scaling the actual storage or the",
    "start": "712640",
    "end": "714800"
  },
  {
    "text": "amount of memory that we have to store",
    "start": "714800",
    "end": "716640"
  },
  {
    "text": "keys in our instances we're back to",
    "start": "716640",
    "end": "719279"
  },
  {
    "text": "vertical scaling and we can only",
    "start": "719279",
    "end": "721440"
  },
  {
    "text": "increase this performance by increasing",
    "start": "721440",
    "end": "723680"
  },
  {
    "text": "the size of memory available on each of",
    "start": "723680",
    "end": "725839"
  },
  {
    "text": "our nodes additionally replication also",
    "start": "725839",
    "end": "728639"
  },
  {
    "text": "comes with another caveat called lag",
    "start": "728639",
    "end": "731680"
  },
  {
    "text": "this is the time delta where data is",
    "start": "731680",
    "end": "733760"
  },
  {
    "text": "written to the primary before it's",
    "start": "733760",
    "end": "735360"
  },
  {
    "text": "available to the replicas and in some",
    "start": "735360",
    "end": "737600"
  },
  {
    "text": "cases can cause data integrity issues",
    "start": "737600",
    "end": "740800"
  },
  {
    "text": "this means that read replication has",
    "start": "740800",
    "end": "742720"
  },
  {
    "text": "what's known as eventual consistency",
    "start": "742720",
    "end": "745120"
  },
  {
    "text": "which is something you don't normally",
    "start": "745120",
    "end": "746560"
  },
  {
    "text": "have to worry about when running on",
    "start": "746560",
    "end": "748399"
  },
  {
    "text": "standalone mode not only this but",
    "start": "748399",
    "end": "750720"
  },
  {
    "text": "there's also a single point of failure",
    "start": "750720",
    "end": "752639"
  },
  {
    "text": "when it comes to this setup the primary",
    "start": "752639",
    "end": "756000"
  },
  {
    "text": "if this instance happens to go down then",
    "start": "756000",
    "end": "758800"
  },
  {
    "text": "we're no longer able to write any data",
    "start": "758800",
    "end": "761040"
  },
  {
    "text": "to our entire Reddus system fortunately",
    "start": "761040",
    "end": "764320"
  },
  {
    "text": "there is a solution provided to this by",
    "start": "764320",
    "end": "766560"
  },
  {
    "text": "both Reddis and Valky which is known as",
    "start": "766560",
    "end": "769519"
  },
  {
    "text": "Sentinel this solution monitors both the",
    "start": "769519",
    "end": "772320"
  },
  {
    "text": "replicas and the primary and will",
    "start": "772320",
    "end": "774560"
  },
  {
    "text": "promote a replica in the event that a",
    "start": "774560",
    "end": "776480"
  },
  {
    "text": "primary goes down sentinel is actually",
    "start": "776480",
    "end": "779200"
  },
  {
    "text": "really awesome when it comes to ensuring",
    "start": "779200",
    "end": "780880"
  },
  {
    "text": "high availability on a Reddus",
    "start": "780880",
    "end": "783120"
  },
  {
    "text": "installation so much so that it actually",
    "start": "783120",
    "end": "785519"
  },
  {
    "text": "deserves its own dedicated video in any",
    "start": "785519",
    "end": "788160"
  },
  {
    "text": "case whilst read replication is a simple",
    "start": "788160",
    "end": "790800"
  },
  {
    "text": "approach to horizontal scaling and is",
    "start": "790800",
    "end": "793600"
  },
  {
    "text": "really powerful when it comes to more",
    "start": "793600",
    "end": "794959"
  },
  {
    "text": "read heavy workflows it still doesn't",
    "start": "794959",
    "end": "797279"
  },
  {
    "text": "solve some of the other issues that",
    "start": "797279",
    "end": "798959"
  },
  {
    "text": "we've mentioned therefore this is where",
    "start": "798959",
    "end": "801200"
  },
  {
    "start": "800000",
    "end": "1029000"
  },
  {
    "text": "a second approach to horizontally",
    "start": "801200",
    "end": "802959"
  },
  {
    "text": "scaling Reddis comes in known as Reddis",
    "start": "802959",
    "end": "806399"
  },
  {
    "text": "cluster reddis or Valky cluster provides",
    "start": "806399",
    "end": "809600"
  },
  {
    "text": "a way to run an installation where data",
    "start": "809600",
    "end": "812160"
  },
  {
    "text": "is automatically sharded across multiple",
    "start": "812160",
    "end": "814480"
  },
  {
    "text": "nodes this allows you to distribute your",
    "start": "814480",
    "end": "817120"
  },
  {
    "text": "requests across multiple instances which",
    "start": "817120",
    "end": "819760"
  },
  {
    "text": "means you can effectively scale CPU",
    "start": "819760",
    "end": "822000"
  },
  {
    "text": "memory and networking to an infinite",
    "start": "822000",
    "end": "824839"
  },
  {
    "text": "amount not really there is still a",
    "start": "824839",
    "end": "827760"
  },
  {
    "text": "finite limit regardless the way that",
    "start": "827760",
    "end": "830000"
  },
  {
    "text": "this is achieved is by sharding data",
    "start": "830000",
    "end": "832079"
  },
  {
    "text": "across multiple nodes meaning rather",
    "start": "832079",
    "end": "834320"
  },
  {
    "text": "than each node having the full data set",
    "start": "834320",
    "end": "836560"
  },
  {
    "text": "it splits across each node inside of the",
    "start": "836560",
    "end": "838639"
  },
  {
    "text": "cluster this is done by using a sharding",
    "start": "838639",
    "end": "841720"
  },
  {
    "text": "algorithm the way the algorithm works is",
    "start": "841720",
    "end": "844480"
  },
  {
    "text": "actually kind of simple the idea is that",
    "start": "844480",
    "end": "846880"
  },
  {
    "text": "the cluster has",
    "start": "846880",
    "end": "849560"
  },
  {
    "text": "16,384 different hash slots you can",
    "start": "849560",
    "end": "852720"
  },
  {
    "text": "think of these as being a bucket of keys",
    "start": "852720",
    "end": "856000"
  },
  {
    "text": "each of these slots or buckets is then",
    "start": "856000",
    "end": "858480"
  },
  {
    "text": "distributed across the nodes evenly then",
    "start": "858480",
    "end": "861440"
  },
  {
    "text": "in order to determine which bucket or",
    "start": "861440",
    "end": "863920"
  },
  {
    "text": "slot a key belongs to the key itself is",
    "start": "863920",
    "end": "866880"
  },
  {
    "text": "then hashed using",
    "start": "866880",
    "end": "868279"
  },
  {
    "text": "CRC16 followed by then taking that",
    "start": "868279",
    "end": "870560"
  },
  {
    "text": "result and modulusing it by the number",
    "start": "870560",
    "end": "872800"
  },
  {
    "text": "of hash slots i.e",
    "start": "872800",
    "end": "876279"
  },
  {
    "text": "16,384 this then returns the slot that",
    "start": "876279",
    "end": "879279"
  },
  {
    "text": "the key belongs to allowing you to then",
    "start": "879279",
    "end": "881360"
  },
  {
    "text": "distribute it to the node that owns that",
    "start": "881360",
    "end": "883360"
  },
  {
    "text": "hash slot whilst setting up cluster mode",
    "start": "883360",
    "end": "886160"
  },
  {
    "text": "is a little more involved than read",
    "start": "886160",
    "end": "887839"
  },
  {
    "text": "replication it's still not that",
    "start": "887839",
    "end": "889839"
  },
  {
    "text": "difficult to do so you first need to add",
    "start": "889839",
    "end": "892800"
  },
  {
    "text": "in the following three options into your",
    "start": "892800",
    "end": "895160"
  },
  {
    "text": "Valky/Rice configuration these are",
    "start": "895160",
    "end": "898160"
  },
  {
    "text": "cluster enabled cluster config file and",
    "start": "898160",
    "end": "901199"
  },
  {
    "text": "cluster node timeout with those three",
    "start": "901199",
    "end": "903920"
  },
  {
    "text": "configuration options applied for each",
    "start": "903920",
    "end": "905760"
  },
  {
    "text": "of the Valky instances you wish to",
    "start": "905760",
    "end": "907600"
  },
  {
    "text": "clusterize all that remains is to use",
    "start": "907600",
    "end": "910000"
  },
  {
    "text": "the following cluster create command on",
    "start": "910000",
    "end": "912399"
  },
  {
    "text": "one of the nodes passing in the host",
    "start": "912399",
    "end": "914720"
  },
  {
    "text": "port combinations of all of the",
    "start": "914720",
    "end": "916480"
  },
  {
    "text": "instances you wish to form the cluster",
    "start": "916480",
    "end": "918880"
  },
  {
    "text": "with this will then present you with the",
    "start": "918880",
    "end": "921600"
  },
  {
    "text": "following screen which will show you the",
    "start": "921600",
    "end": "923839"
  },
  {
    "text": "distribution of hash slots across each",
    "start": "923839",
    "end": "926000"
  },
  {
    "text": "of the nodes as well as prompting you",
    "start": "926000",
    "end": "928079"
  },
  {
    "text": "for confirmation upon doing so the",
    "start": "928079",
    "end": "930399"
  },
  {
    "text": "cluster will then be set up hopefully",
    "start": "930399",
    "end": "932399"
  },
  {
    "text": "and should let you know when everything",
    "start": "932399",
    "end": "934000"
  },
  {
    "text": "is working which we can then go ahead",
    "start": "934000",
    "end": "936320"
  },
  {
    "text": "and confirm using the cluster info",
    "start": "936320",
    "end": "938320"
  },
  {
    "text": "command on one or all of our instances",
    "start": "938320",
    "end": "941519"
  },
  {
    "text": "with the cluster setup if I now go ahead",
    "start": "941519",
    "end": "943680"
  },
  {
    "text": "and run the MEM tier benchmark command",
    "start": "943680",
    "end": "945760"
  },
  {
    "text": "again this time making sure it's set to",
    "start": "945760",
    "end": "948160"
  },
  {
    "text": "cluster mode by using the following flag",
    "start": "948160",
    "end": "950720"
  },
  {
    "text": "you can see that both the read and write",
    "start": "950720",
    "end": "952800"
  },
  {
    "text": "throughput is now substantially",
    "start": "952800",
    "end": "954800"
  },
  {
    "text": "increased hitting around 400,000",
    "start": "954800",
    "end": "957360"
  },
  {
    "text": "requests a second very cool as you can",
    "start": "957360",
    "end": "960560"
  },
  {
    "text": "see this is similar to the throughput we",
    "start": "960560",
    "end": "962880"
  },
  {
    "text": "were getting when it came to using read",
    "start": "962880",
    "end": "964880"
  },
  {
    "text": "replicas however the benefit here is",
    "start": "964880",
    "end": "967759"
  },
  {
    "text": "that we're able to both read from and",
    "start": "967759",
    "end": "969920"
  },
  {
    "text": "write to all three of our instances",
    "start": "969920",
    "end": "972320"
  },
  {
    "text": "instead of just only being able to write",
    "start": "972320",
    "end": "974480"
  },
  {
    "text": "to one not only does this mean that we",
    "start": "974480",
    "end": "976480"
  },
  {
    "text": "have improved performance when it comes",
    "start": "976480",
    "end": "978160"
  },
  {
    "text": "to our write operations but it also",
    "start": "978160",
    "end": "980560"
  },
  {
    "text": "means we can make use of the available",
    "start": "980560",
    "end": "982160"
  },
  {
    "text": "resources on each of our machines by",
    "start": "982160",
    "end": "984880"
  },
  {
    "text": "deploying multiple Valky instances on",
    "start": "984880",
    "end": "987199"
  },
  {
    "text": "each node for example here I've gone and",
    "start": "987199",
    "end": "990160"
  },
  {
    "text": "deployed another seven instances on my",
    "start": "990160",
    "end": "992399"
  },
  {
    "text": "midtier machine bringing the total",
    "start": "992399",
    "end": "994639"
  },
  {
    "text": "number of nodes in my Valkyrie cluster",
    "start": "994639",
    "end": "996399"
  },
  {
    "text": "to 10 this means that the cluster should",
    "start": "996399",
    "end": "998720"
  },
  {
    "text": "be making better use of all of the",
    "start": "998720",
    "end": "1000160"
  },
  {
    "text": "available hardware on my mid-tier",
    "start": "1000160",
    "end": "1002000"
  },
  {
    "text": "machine which if I now go ahead and run",
    "start": "1002000",
    "end": "1004160"
  },
  {
    "text": "a benchmark test against you can see I'm",
    "start": "1004160",
    "end": "1006639"
  },
  {
    "text": "hitting 1 million requests per second",
    "start": "1006639",
    "end": "1010000"
  },
  {
    "text": "hooray of course this is in perfect",
    "start": "1010000",
    "end": "1013079"
  },
  {
    "text": "conditions running on my local network",
    "start": "1013079",
    "end": "1015759"
  },
  {
    "text": "on bare metal so in order to really",
    "start": "1015759",
    "end": "1019199"
  },
  {
    "text": "complete this challenge then we're going",
    "start": "1019199",
    "end": "1021199"
  },
  {
    "text": "to want to take a look at how we can",
    "start": "1021199",
    "end": "1022560"
  },
  {
    "text": "achieve 1 million operations per second",
    "start": "1022560",
    "end": "1025038"
  },
  {
    "text": "whilst running on the cloud using VPS",
    "start": "1025039",
    "end": "1028120"
  },
  {
    "text": "instances before we do that however",
    "start": "1028120",
    "end": "1030558"
  },
  {
    "start": "1029000",
    "end": "1258000"
  },
  {
    "text": "let's first talk about some of the",
    "start": "1030559",
    "end": "1031918"
  },
  {
    "text": "caveats associated with cluster mode",
    "start": "1031919",
    "end": "1034480"
  },
  {
    "text": "because there are a few the first of",
    "start": "1034480",
    "end": "1036959"
  },
  {
    "text": "which is that in order to be able to",
    "start": "1036959",
    "end": "1038720"
  },
  {
    "text": "send commands to it you need to use a",
    "start": "1038720",
    "end": "1041120"
  },
  {
    "text": "clusteraware client now to be fair this",
    "start": "1041120",
    "end": "1044079"
  },
  {
    "text": "isn't too much of an issue as most",
    "start": "1044079",
    "end": "1046000"
  },
  {
    "text": "Reddus clients provide support for",
    "start": "1046000",
    "end": "1047839"
  },
  {
    "text": "cluster mode however it does mean that",
    "start": "1047839",
    "end": "1049919"
  },
  {
    "text": "any existing code that makes use of",
    "start": "1049919",
    "end": "1051520"
  },
  {
    "text": "Reddus does need to be modified at least",
    "start": "1051520",
    "end": "1054000"
  },
  {
    "text": "slightly for example if I try to connect",
    "start": "1054000",
    "end": "1056080"
  },
  {
    "text": "to an instance in my cluster using the",
    "start": "1056080",
    "end": "1058080"
  },
  {
    "text": "Valky CLI and try to pull out the",
    "start": "1058080",
    "end": "1060559"
  },
  {
    "text": "following key you'll see I get an error",
    "start": "1060559",
    "end": "1062480"
  },
  {
    "text": "letting me know that the key has been",
    "start": "1062480",
    "end": "1064320"
  },
  {
    "text": "moved therefore in order to be able to",
    "start": "1064320",
    "end": "1067039"
  },
  {
    "text": "use the Valky CLI to send commands to",
    "start": "1067039",
    "end": "1069520"
  },
  {
    "text": "the Valky cluster I would need to make",
    "start": "1069520",
    "end": "1071679"
  },
  {
    "text": "use of the - C flag in order to connect",
    "start": "1071679",
    "end": "1073919"
  },
  {
    "text": "to cluster mode and my client will then",
    "start": "1073919",
    "end": "1076640"
  },
  {
    "text": "be routed to the correct node that",
    "start": "1076640",
    "end": "1078480"
  },
  {
    "text": "contains this key in addition to",
    "start": "1078480",
    "end": "1080799"
  },
  {
    "text": "ensuring that the client connects to the",
    "start": "1080799",
    "end": "1082400"
  },
  {
    "text": "cluster mode correctly there are some",
    "start": "1082400",
    "end": "1084320"
  },
  {
    "text": "other caveats as well specifically when",
    "start": "1084320",
    "end": "1086799"
  },
  {
    "text": "it comes to multikey operations such as",
    "start": "1086799",
    "end": "1089760"
  },
  {
    "text": "working with transaction pipelines or",
    "start": "1089760",
    "end": "1092320"
  },
  {
    "text": "reddis lure scripts in each of these",
    "start": "1092320",
    "end": "1095039"
  },
  {
    "text": "cases you need to ensure that any",
    "start": "1095039",
    "end": "1097039"
  },
  {
    "text": "related keys will belong to the same key",
    "start": "1097039",
    "end": "1099760"
  },
  {
    "text": "slot otherwise any scripts or",
    "start": "1099760",
    "end": "1101600"
  },
  {
    "text": "transactions won't be able to be used",
    "start": "1101600",
    "end": "1104400"
  },
  {
    "text": "fortunately Reddus and Valky provide a",
    "start": "1104400",
    "end": "1106559"
  },
  {
    "text": "way to achieve this which is to make use",
    "start": "1106559",
    "end": "1108799"
  },
  {
    "text": "of a hashtag not the social media kind",
    "start": "1108799",
    "end": "1111600"
  },
  {
    "text": "of hashtags instead a hashtag is defined",
    "start": "1111600",
    "end": "1114400"
  },
  {
    "text": "within an actual key as follows",
    "start": "1114400",
    "end": "1117200"
  },
  {
    "text": "specifying the ID that you want to be",
    "start": "1117200",
    "end": "1119600"
  },
  {
    "text": "hashed using the following syntax in",
    "start": "1119600",
    "end": "1122720"
  },
  {
    "text": "this case the key being hashed is 1 2 3",
    "start": "1122720",
    "end": "1125919"
  },
  {
    "text": "rather than the actual full key itself",
    "start": "1125919",
    "end": "1128320"
  },
  {
    "text": "by doing this it means that any keys",
    "start": "1128320",
    "end": "1130400"
  },
  {
    "text": "that share the same hashtag will be",
    "start": "1130400",
    "end": "1132320"
  },
  {
    "text": "placed inside of the same hash slot",
    "start": "1132320",
    "end": "1134320"
  },
  {
    "text": "which means you're then able to perform",
    "start": "1134320",
    "end": "1135919"
  },
  {
    "text": "any multi-key operations such as",
    "start": "1135919",
    "end": "1138400"
  },
  {
    "text": "transactions or scripts whilst hashtags",
    "start": "1138400",
    "end": "1141280"
  },
  {
    "text": "solve the issue of key distribution",
    "start": "1141280",
    "end": "1143679"
  },
  {
    "text": "there are still many other problems when",
    "start": "1143679",
    "end": "1145440"
  },
  {
    "text": "it comes to running a distributed system",
    "start": "1145440",
    "end": "1147440"
  },
  {
    "text": "such as a Valky cluster with perhaps the",
    "start": "1147440",
    "end": "1150000"
  },
  {
    "text": "most major one being how to ensure",
    "start": "1150000",
    "end": "1152080"
  },
  {
    "text": "reliability in the event that a node",
    "start": "1152080",
    "end": "1154559"
  },
  {
    "text": "goes down now to be fair Reddis cluster",
    "start": "1154559",
    "end": "1157200"
  },
  {
    "text": "does provide some resilience when it",
    "start": "1157200",
    "end": "1159280"
  },
  {
    "text": "comes to availability if a node is",
    "start": "1159280",
    "end": "1161280"
  },
  {
    "text": "dropped from a cluster then those key",
    "start": "1161280",
    "end": "1162960"
  },
  {
    "text": "slots will be redistributed however the",
    "start": "1162960",
    "end": "1165679"
  },
  {
    "text": "data can be lost fortunately cluster",
    "start": "1165679",
    "end": "1168320"
  },
  {
    "text": "mode also provides the ability to set up",
    "start": "1168320",
    "end": "1170679"
  },
  {
    "text": "replication however this differs",
    "start": "1170679",
    "end": "1173200"
  },
  {
    "text": "slightly from the replication we saw",
    "start": "1173200",
    "end": "1175039"
  },
  {
    "text": "before in that rather than replicating",
    "start": "1175039",
    "end": "1177360"
  },
  {
    "text": "the entire data set these replicas",
    "start": "1177360",
    "end": "1180000"
  },
  {
    "text": "instead contain a copy of their",
    "start": "1180000",
    "end": "1181760"
  },
  {
    "text": "respective shards or different hash",
    "start": "1181760",
    "end": "1183840"
  },
  {
    "text": "slots additionally this means you can",
    "start": "1183840",
    "end": "1185679"
  },
  {
    "text": "have multiple replicas per shard",
    "start": "1185679",
    "end": "1188000"
  },
  {
    "text": "providing you high availability and",
    "start": "1188000",
    "end": "1190240"
  },
  {
    "text": "reducing the risk of data loss in",
    "start": "1190240",
    "end": "1192080"
  },
  {
    "text": "cluster mode this does mean however that",
    "start": "1192080",
    "end": "1194559"
  },
  {
    "text": "you'll want to ensure that each of these",
    "start": "1194559",
    "end": "1196080"
  },
  {
    "text": "replicas is on a different machine than",
    "start": "1196080",
    "end": "1198240"
  },
  {
    "text": "the primary and ideally from each other",
    "start": "1198240",
    "end": "1200400"
  },
  {
    "text": "as well and ultimately means your total",
    "start": "1200400",
    "end": "1203280"
  },
  {
    "text": "reddis system is going to become more",
    "start": "1203280",
    "end": "1205679"
  },
  {
    "text": "complex fortunately there are tools out",
    "start": "1205679",
    "end": "1208160"
  },
  {
    "text": "there such as IA Kubernetes or managed",
    "start": "1208160",
    "end": "1211120"
  },
  {
    "text": "providers to help make this complexity",
    "start": "1211120",
    "end": "1213360"
  },
  {
    "text": "more manageable in my case when it came",
    "start": "1213360",
    "end": "1216000"
  },
  {
    "text": "to deploying a cluster onto a number of",
    "start": "1216000",
    "end": "1218240"
  },
  {
    "text": "VPS instances I ended up writing the",
    "start": "1218240",
    "end": "1220960"
  },
  {
    "text": "following Terraform configuration well",
    "start": "1220960",
    "end": "1223120"
  },
  {
    "text": "actually it's an open Tofu configuration",
    "start": "1223120",
    "end": "1225360"
  },
  {
    "text": "who I'm now pinning on my wall instead",
    "start": "1225360",
    "end": "1228000"
  },
  {
    "text": "regardless this configuration allows me",
    "start": "1228000",
    "end": "1230080"
  },
  {
    "text": "to deploy a Valky cluster onto one of",
    "start": "1230080",
    "end": "1232640"
  },
  {
    "text": "two different providers either Digital",
    "start": "1232640",
    "end": "1235039"
  },
  {
    "text": "Ocean or Hzner which I used to see if I",
    "start": "1235039",
    "end": "1238240"
  },
  {
    "text": "could reach 1 million operations per",
    "start": "1238240",
    "end": "1240400"
  },
  {
    "text": "second as it turned out it was a little",
    "start": "1240400",
    "end": "1243600"
  },
  {
    "text": "bit more challenging than I thought",
    "start": "1243600",
    "end": "1245679"
  },
  {
    "text": "however before we take a look at whether",
    "start": "1245679",
    "end": "1246960"
  },
  {
    "text": "or not I was able to achieve this on the",
    "start": "1246960",
    "end": "1249039"
  },
  {
    "text": "public cloud let's take a quick look at",
    "start": "1249039",
    "end": "1251440"
  },
  {
    "text": "another way to achieve 1 million",
    "start": "1251440",
    "end": "1253520"
  },
  {
    "text": "requests per second one that is actually",
    "start": "1253520",
    "end": "1256080"
  },
  {
    "text": "a lot more simple than setting up a",
    "start": "1256080",
    "end": "1257919"
  },
  {
    "text": "Valkyrie cluster this is through using",
    "start": "1257919",
    "end": "1260000"
  },
  {
    "start": "1258000",
    "end": "1381000"
  },
  {
    "text": "the sponsor of today's video Dragonfly",
    "start": "1260000",
    "end": "1262880"
  },
  {
    "text": "DB who as I mentioned before provide a",
    "start": "1262880",
    "end": "1265679"
  },
  {
    "text": "drop-in replacement for Reddis that",
    "start": "1265679",
    "end": "1267440"
  },
  {
    "text": "boasts greater performance to show how",
    "start": "1267440",
    "end": "1269679"
  },
  {
    "text": "much performance improvement Dragonfly",
    "start": "1269679",
    "end": "1271360"
  },
  {
    "text": "DB has if I go ahead and deploy an",
    "start": "1271360",
    "end": "1273520"
  },
  {
    "text": "instance of it onto my small machine",
    "start": "1273520",
    "end": "1276240"
  },
  {
    "text": "followed by performing the following",
    "start": "1276240",
    "end": "1277760"
  },
  {
    "text": "benchmark test we've been using before",
    "start": "1277760",
    "end": "1279840"
  },
  {
    "text": "you can see I'm hitting about 250,000",
    "start": "1279840",
    "end": "1282080"
  },
  {
    "text": "requests per second which isn't that",
    "start": "1282080",
    "end": "1284400"
  },
  {
    "text": "much of an improvement compared to the",
    "start": "1284400",
    "end": "1286080"
  },
  {
    "text": "existing instance I was using before",
    "start": "1286080",
    "end": "1288240"
  },
  {
    "text": "however if I go ahead and now deploy",
    "start": "1288240",
    "end": "1289760"
  },
  {
    "text": "this on my mid machine you can see the",
    "start": "1289760",
    "end": "1292720"
  },
  {
    "text": "performance improvement is now",
    "start": "1292720",
    "end": "1294480"
  },
  {
    "text": "substantial this time I'm running about",
    "start": "1294480",
    "end": "1297039"
  },
  {
    "text": "twice as fast as I was before which",
    "start": "1297039",
    "end": "1299440"
  },
  {
    "text": "makes a lot of sense given there's twice",
    "start": "1299440",
    "end": "1300960"
  },
  {
    "text": "as many cores on this machine as you can",
    "start": "1300960",
    "end": "1303200"
  },
  {
    "text": "see by using Dragonfly DB which makes",
    "start": "1303200",
    "end": "1305360"
  },
  {
    "text": "better use of the available resources on",
    "start": "1305360",
    "end": "1307520"
  },
  {
    "text": "a system we're able to now vertically",
    "start": "1307520",
    "end": "1309760"
  },
  {
    "text": "scale our system compared to just using",
    "start": "1309760",
    "end": "1311840"
  },
  {
    "text": "a single core implementation like we",
    "start": "1311840",
    "end": "1314000"
  },
  {
    "text": "were before so let's see what happens if",
    "start": "1314000",
    "end": "1316400"
  },
  {
    "text": "we run Dragonfly on the big boy Thread",
    "start": "1316400",
    "end": "1318640"
  },
  {
    "text": "Ripper can we hit 1 million requests per",
    "start": "1318640",
    "end": "1321440"
  },
  {
    "text": "second by just running a single instance",
    "start": "1321440",
    "end": "1323919"
  },
  {
    "text": "in standalone mode turns out we can by",
    "start": "1323919",
    "end": "1328080"
  },
  {
    "text": "just a hair so 1 million requests",
    "start": "1328080",
    "end": "1330720"
  },
  {
    "text": "achieved by just using vertical scaling",
    "start": "1330720",
    "end": "1333600"
  },
  {
    "text": "and this number can actually go even",
    "start": "1333600",
    "end": "1335520"
  },
  {
    "text": "further in fact the team at Dragonfly",
    "start": "1335520",
    "end": "1338000"
  },
  {
    "text": "managed to reach 6 million requests per",
    "start": "1338000",
    "end": "1340240"
  },
  {
    "text": "second when it came to running on a VPS",
    "start": "1340240",
    "end": "1342880"
  },
  {
    "text": "all of this is achieved without having",
    "start": "1342880",
    "end": "1344799"
  },
  {
    "text": "to worry about setting up multiple",
    "start": "1344799",
    "end": "1346480"
  },
  {
    "text": "instances or any of the caveats that",
    "start": "1346480",
    "end": "1348799"
  },
  {
    "text": "come from using cluster mode that being",
    "start": "1348799",
    "end": "1351840"
  },
  {
    "text": "said there are still some good reasons",
    "start": "1351840",
    "end": "1353760"
  },
  {
    "text": "to embrace the complexity of clustering",
    "start": "1353760",
    "end": "1356480"
  },
  {
    "text": "such as when you want to distribute your",
    "start": "1356480",
    "end": "1358080"
  },
  {
    "text": "key set across multiple machines either",
    "start": "1358080",
    "end": "1360559"
  },
  {
    "text": "for scaling memory or for redundancy",
    "start": "1360559",
    "end": "1363120"
  },
  {
    "text": "dragonfly itself does provide a way to",
    "start": "1363120",
    "end": "1365360"
  },
  {
    "text": "horizontally scale using their own swarm",
    "start": "1365360",
    "end": "1368240"
  },
  {
    "text": "mode which was announced a short while",
    "start": "1368240",
    "end": "1370559"
  },
  {
    "text": "before filming this video so if you're",
    "start": "1370559",
    "end": "1373039"
  },
  {
    "text": "interested in Dragonfly DB as a drop-in",
    "start": "1373039",
    "end": "1375520"
  },
  {
    "text": "replacement for Reddus that offers",
    "start": "1375520",
    "end": "1377280"
  },
  {
    "text": "greater performance then check them out",
    "start": "1377280",
    "end": "1379360"
  },
  {
    "text": "using the link below okay so now that",
    "start": "1379360",
    "end": "1382000"
  },
  {
    "start": "1381000",
    "end": "1681000"
  },
  {
    "text": "we've seen how to reach 1 million",
    "start": "1382000",
    "end": "1383600"
  },
  {
    "text": "requests per second in perfect",
    "start": "1383600",
    "end": "1385520"
  },
  {
    "text": "conditions let's take a look at how",
    "start": "1385520",
    "end": "1387360"
  },
  {
    "text": "difficult it is to achieve this on",
    "start": "1387360",
    "end": "1389200"
  },
  {
    "text": "something less perfect the cloud as I",
    "start": "1389200",
    "end": "1392320"
  },
  {
    "text": "mentioned before I'd managed to set up a",
    "start": "1392320",
    "end": "1394000"
  },
  {
    "text": "Terraform or Open Tofu configuration for",
    "start": "1394000",
    "end": "1396799"
  },
  {
    "text": "both Digital Ocean and Hzner which you",
    "start": "1396799",
    "end": "1399520"
  },
  {
    "text": "can actually download yourself from",
    "start": "1399520",
    "end": "1400880"
  },
  {
    "text": "GitHub if you want to try it out just",
    "start": "1400880",
    "end": "1403440"
  },
  {
    "text": "remember don't leave these instances",
    "start": "1403440",
    "end": "1405280"
  },
  {
    "text": "running for a long time or you'll end up",
    "start": "1405280",
    "end": "1408000"
  },
  {
    "text": "with a large bill at the end of the",
    "start": "1408000",
    "end": "1410159"
  },
  {
    "text": "month so there are some instructions on",
    "start": "1410159",
    "end": "1412159"
  },
  {
    "text": "how to actually deploy this Terraform",
    "start": "1412159",
    "end": "1413919"
  },
  {
    "text": "configuration on the actual repo itself",
    "start": "1413919",
    "end": "1416400"
  },
  {
    "text": "but at a high level you mainly just need",
    "start": "1416400",
    "end": "1418320"
  },
  {
    "text": "to set your API token and SSH key values",
    "start": "1418320",
    "end": "1421120"
  },
  {
    "text": "in the TF vase for whichever cloud",
    "start": "1421120",
    "end": "1424159"
  },
  {
    "text": "provider you want to use followed by",
    "start": "1424159",
    "end": "1426400"
  },
  {
    "text": "then running the Tofu apply command once",
    "start": "1426400",
    "end": "1429679"
  },
  {
    "text": "you're done with your testing in order",
    "start": "1429679",
    "end": "1431520"
  },
  {
    "text": "to clean everything up go ahead and use",
    "start": "1431520",
    "end": "1433200"
  },
  {
    "text": "the tofu destroy command just to make",
    "start": "1433200",
    "end": "1435360"
  },
  {
    "text": "sure you don't go bankrupt as I",
    "start": "1435360",
    "end": "1437600"
  },
  {
    "text": "mentioned I ended up writing an open",
    "start": "1437600",
    "end": "1439520"
  },
  {
    "text": "Tofu configuration to deploy a Valky",
    "start": "1439520",
    "end": "1442159"
  },
  {
    "text": "cluster to either Digital Ocean or",
    "start": "1442159",
    "end": "1444280"
  },
  {
    "text": "Hetner however this wasn't my original",
    "start": "1444280",
    "end": "1447520"
  },
  {
    "text": "plan as I had intended to only use one",
    "start": "1447520",
    "end": "1450559"
  },
  {
    "text": "of these providers HNER but for some",
    "start": "1450559",
    "end": "1454240"
  },
  {
    "text": "reason I couldn't seem to get anywhere",
    "start": "1454240",
    "end": "1456320"
  },
  {
    "text": "close to 1 million operations per second",
    "start": "1456320",
    "end": "1459679"
  },
  {
    "text": "instead barely only exceeding 100,000 no",
    "start": "1459679",
    "end": "1463520"
  },
  {
    "text": "matter what I did or how I had the",
    "start": "1463520",
    "end": "1465520"
  },
  {
    "text": "cluster configured overall it was kind",
    "start": "1465520",
    "end": "1468640"
  },
  {
    "text": "of strange and my best guess as to why",
    "start": "1468640",
    "end": "1471520"
  },
  {
    "text": "this was happening was because I was",
    "start": "1471520",
    "end": "1473200"
  },
  {
    "text": "using shared vCPUs so I went about",
    "start": "1473200",
    "end": "1476159"
  },
  {
    "text": "migrating to dedicated ones instead",
    "start": "1476159",
    "end": "1479120"
  },
  {
    "text": "however because my account was too new",
    "start": "1479120",
    "end": "1481360"
  },
  {
    "text": "to request a limit increase I was unable",
    "start": "1481360",
    "end": "1484000"
  },
  {
    "text": "to provision any more dedicated vCPUs so",
    "start": "1484000",
    "end": "1487200"
  },
  {
    "text": "when it came to hitting a million Valky",
    "start": "1487200",
    "end": "1489360"
  },
  {
    "text": "operations per second using Hzner I was",
    "start": "1489360",
    "end": "1492159"
  },
  {
    "text": "out of moves therefore I instead decided",
    "start": "1492159",
    "end": "1495039"
  },
  {
    "text": "to try with Digital Ocean first reaching",
    "start": "1495039",
    "end": "1497919"
  },
  {
    "text": "out to support in order to get access to",
    "start": "1497919",
    "end": "1499760"
  },
  {
    "text": "the larger instance sizes so that I",
    "start": "1499760",
    "end": "1502400"
  },
  {
    "text": "could run benchmarking without having",
    "start": "1502400",
    "end": "1504080"
  },
  {
    "text": "any bottlenecking once I had my compute",
    "start": "1504080",
    "end": "1506640"
  },
  {
    "text": "limits increased I then deployed the",
    "start": "1506640",
    "end": "1508480"
  },
  {
    "text": "cluster using Tofu Apply and SSH into my",
    "start": "1508480",
    "end": "1511679"
  },
  {
    "text": "benchmark box once I had everything set",
    "start": "1511679",
    "end": "1514559"
  },
  {
    "text": "up and the cluster was deployed I then",
    "start": "1514559",
    "end": "1517120"
  },
  {
    "text": "went about running the memier benchmark",
    "start": "1517120",
    "end": "1519520"
  },
  {
    "text": "command and on my initial attempt which",
    "start": "1519520",
    "end": "1522240"
  },
  {
    "text": "had nine Valkyrie nodes inside of the",
    "start": "1522240",
    "end": "1524159"
  },
  {
    "text": "cluster and using a 16 vcpu machine for",
    "start": "1524159",
    "end": "1526960"
  },
  {
    "text": "the benchmarking tool I was hitting",
    "start": "1526960",
    "end": "1528880"
  },
  {
    "text": "around 450,000 requests per second not",
    "start": "1528880",
    "end": "1532640"
  },
  {
    "text": "bad after confirming that it was the",
    "start": "1532640",
    "end": "1534559"
  },
  {
    "text": "benchmark tool that was bottlenecking I",
    "start": "1534559",
    "end": "1536640"
  },
  {
    "text": "decided to scale up the instance it was",
    "start": "1536640",
    "end": "1538400"
  },
  {
    "text": "running on to one with 32 vCPUs this",
    "start": "1538400",
    "end": "1542080"
  },
  {
    "text": "time when I ran the benchmarking tool",
    "start": "1542080",
    "end": "1543919"
  },
  {
    "text": "with 32 threads I was getting around",
    "start": "1543919",
    "end": "1547080"
  },
  {
    "text": "900,000 off the get- go however as time",
    "start": "1547080",
    "end": "1550400"
  },
  {
    "text": "went on this number would start to",
    "start": "1550400",
    "end": "1552159"
  },
  {
    "text": "decrease down to about",
    "start": "1552159",
    "end": "1554520"
  },
  {
    "text": "800,000 so I decided to go allin and",
    "start": "1554520",
    "end": "1558159"
  },
  {
    "text": "scaled up the number of Valky nodes I",
    "start": "1558159",
    "end": "1560159"
  },
  {
    "text": "had from 9 to 15 after doing a quick",
    "start": "1560159",
    "end": "1563600"
  },
  {
    "text": "tofu apply and seeing all of the",
    "start": "1563600",
    "end": "1565520"
  },
  {
    "text": "instances come through on the digital",
    "start": "1565520",
    "end": "1567279"
  },
  {
    "text": "ocean dashboard I sshed in and ran the",
    "start": "1567279",
    "end": "1570679"
  },
  {
    "text": "ment",
    "start": "1570679",
    "end": "1573559"
  },
  {
    "text": "again success i was now hitting a",
    "start": "1573559",
    "end": "1576320"
  },
  {
    "text": "sustained 1 million operations per",
    "start": "1576320",
    "end": "1579039"
  },
  {
    "text": "second",
    "start": "1579039",
    "end": "1582039"
  },
  {
    "text": "with that my goal had been achieved and",
    "start": "1582640",
    "end": "1584799"
  },
  {
    "text": "all it took was for me to deploy 15",
    "start": "1584799",
    "end": "1586880"
  },
  {
    "text": "Valky instances on premium Intel nodes",
    "start": "1586880",
    "end": "1590080"
  },
  {
    "text": "which had I left running would have only",
    "start": "1590080",
    "end": "1591679"
  },
  {
    "text": "cost me around $1,100 a month yeah a",
    "start": "1591679",
    "end": "1595520"
  },
  {
    "text": "little out of my infrastructure budget",
    "start": "1595520",
    "end": "1598400"
  },
  {
    "text": "just for fun I decided to see how much",
    "start": "1598400",
    "end": "1600320"
  },
  {
    "text": "throughput I could get by using a",
    "start": "1600320",
    "end": "1602240"
  },
  {
    "text": "pipeline of 100 when it came to this",
    "start": "1602240",
    "end": "1604559"
  },
  {
    "text": "setup which ended up producing around 14",
    "start": "1604559",
    "end": "1607520"
  },
  {
    "text": "million operations per second so yeah",
    "start": "1607520",
    "end": "1610240"
  },
  {
    "text": "that just goes to show how much",
    "start": "1610240",
    "end": "1611520"
  },
  {
    "text": "improvement you can get when it comes to",
    "start": "1611520",
    "end": "1613200"
  },
  {
    "text": "reducing the round trip time by using",
    "start": "1613200",
    "end": "1615440"
  },
  {
    "text": "pipelining with Reddus in any case all",
    "start": "1615440",
    "end": "1618720"
  },
  {
    "text": "that remained was to tear down my setup",
    "start": "1618720",
    "end": "1620480"
  },
  {
    "text": "using Tofu Destroy and with that I had",
    "start": "1620480",
    "end": "1623120"
  },
  {
    "text": "managed to achieve my goal hitting 1",
    "start": "1623120",
    "end": "1625679"
  },
  {
    "text": "million requests both using bare metal",
    "start": "1625679",
    "end": "1628000"
  },
  {
    "text": "on my home network which honestly is",
    "start": "1628000",
    "end": "1629919"
  },
  {
    "text": "kind of cheating and through using a VPS",
    "start": "1629919",
    "end": "1633279"
  },
  {
    "text": "instance on Digital Ocean using private",
    "start": "1633279",
    "end": "1636080"
  },
  {
    "text": "networking in the data center in any",
    "start": "1636080",
    "end": "1638720"
  },
  {
    "text": "case I want to give a big thank you to",
    "start": "1638720",
    "end": "1640159"
  },
  {
    "text": "Dragonfly DB for sponsoring this video",
    "start": "1640159",
    "end": "1642559"
  },
  {
    "text": "and making all of this happen without",
    "start": "1642559",
    "end": "1644799"
  },
  {
    "text": "them I wouldn't have been able to spend",
    "start": "1644799",
    "end": "1646320"
  },
  {
    "text": "so much cash on VPS instances for",
    "start": "1646320",
    "end": "1648799"
  },
  {
    "text": "testing additionally if you want to use",
    "start": "1648799",
    "end": "1650720"
  },
  {
    "text": "a high performance Reddit alternative",
    "start": "1650720",
    "end": "1652799"
  },
  {
    "text": "without managing your own infrastructure",
    "start": "1652799",
    "end": "1654960"
  },
  {
    "text": "then Dragonfly also offers a fully",
    "start": "1654960",
    "end": "1657440"
  },
  {
    "text": "managed service Dragonfly Cloud which",
    "start": "1657440",
    "end": "1660640"
  },
  {
    "text": "runs the same code as if you were",
    "start": "1660640",
    "end": "1662360"
  },
  {
    "text": "self-hosting but just handles all of the",
    "start": "1662360",
    "end": "1664880"
  },
  {
    "text": "operational heavy lifting for you so if",
    "start": "1664880",
    "end": "1667919"
  },
  {
    "text": "you want a hassle-free caching solution",
    "start": "1667919",
    "end": "1670240"
  },
  {
    "text": "then check it out using the link in the",
    "start": "1670240",
    "end": "1672240"
  },
  {
    "text": "description below otherwise I want to",
    "start": "1672240",
    "end": "1674720"
  },
  {
    "text": "give a big thank you for watching and",
    "start": "1674720",
    "end": "1676320"
  },
  {
    "text": "I'll see you on the next one",
    "start": "1676320",
    "end": "1680440"
  }
]