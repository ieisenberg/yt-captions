[
  {
    "text": "There hasn't really been much progress made on the shard manager or really anything cluster",
    "start": "3733",
    "end": "9933"
  },
  {
    "text": "related since the last time we streamed because I've been working on something else, but there should be more progress being made this",
    "start": "9966",
    "end": "16633"
  },
  {
    "text": "week since I'm finishing up that other project.",
    "start": "16633",
    "end": "19533"
  },
  {
    "text": "At least we can talk about some of this stuff.",
    "start": "21833",
    "end": "25466"
  },
  {
    "text": "And there has been some interesting additions to the effect",
    "start": "32433",
    "end": "37666"
  },
  {
    "text": "core library, at least working progress that maybe we can use in cluster.",
    "start": "37666",
    "end": "45866"
  },
  {
    "text": "Hopefully soon. Referring to the mailbox. Yeah.",
    "start": "46133",
    "end": "51533"
  },
  {
    "text": "Yeah. Tim, for those of you who don't know, Tim has been putting in a lot of work on a mailbox",
    "start": "55433",
    "end": "62133"
  },
  {
    "text": "implementation for the effect core library. And it's not released",
    "start": "63033",
    "end": "68132"
  },
  {
    "text": "yet, but it's close, I think. And we're probably going to try to",
    "start": "68133",
    "end": "74333"
  },
  {
    "text": "leverage that within cluster for the mailbox within entities so that we don't have to roll our own.",
    "start": "74333",
    "end": "80966"
  },
  {
    "text": "May have to wrap it with a few methods, but yeah, we'll probably",
    "start": "82699",
    "end": "90366"
  },
  {
    "text": "try to leverage that within cluster since it's kind of the perfect use case.",
    "start": "90366",
    "end": "94966"
  },
  {
    "text": "But yeah, so as I was saying, I haven't made a ton of progress on cluster since last time we met,",
    "start": "95900",
    "end": "101133"
  },
  {
    "text": "but Mike did figure out the performance issue that we were having in tests, at least the last",
    "start": "101466",
    "end": "106633"
  },
  {
    "text": "time that we spoke, which was actually nothing to do with the hashable data types.",
    "start": "106633",
    "end": "111766"
  },
  {
    "text": "It was actually to do with the way that sorting happened in the",
    "start": "111766",
    "end": "117333"
  },
  {
    "text": "array.sortBy or sortWith, I forget which method it was. But after fixing that, the tests seem to",
    "start": "117333",
    "end": "124500"
  },
  {
    "text": "run relatively well. I'm still having a small issue with...",
    "start": "124500",
    "end": "130366"
  },
  {
    "text": "Maybe we can open up the tests and I can just show you guys.",
    "start": "133966",
    "end": "137433"
  },
  {
    "text": "So I run the test suite.",
    "start": "140666",
    "end": "142966"
  },
  {
    "text": "Packages effect cluster.",
    "start": "147599",
    "end": "151366"
  },
  {
    "text": "Of course, packages cluster test.",
    "start": "153199",
    "end": "155533"
  },
  {
    "text": "So these simulation tests still take quite a substantial amount of time",
    "start": "164533",
    "end": "169698"
  },
  {
    "text": "for three seconds each. So I'm running them concurrently right now to try to mitigate that. But when I was looking into...",
    "start": "170566",
    "end": "179666"
  },
  {
    "text": "Again, it's kind of hard to debug this a little bit, but when I was looking into where this was occurring, and I haven't",
    "start": "180633",
    "end": "186465"
  },
  {
    "text": "quite figured it out yet, but it seems like the issue is happening around the adjustment of the",
    "start": "186466",
    "end": "191966"
  },
  {
    "text": "test clock, which I thought was kind of interesting. So I was trying to figure out how much work the shard manager is",
    "start": "191966",
    "end": "198433"
  },
  {
    "text": "actually doing during the time that the test clock's being adjusted. Because once we adjust the test clock,",
    "start": "198433",
    "end": "204733"
  },
  {
    "text": "once we do this adjustment, there's a bunch of rebalances that occur. But rebalances are reasonably quick.",
    "start": "206066",
    "end": "213500"
  },
  {
    "text": "I think the slowest one that I measured was like only 200 milliseconds or something like that.",
    "start": "213533",
    "end": "218699"
  },
  {
    "text": "And I think that's when there's just a ton of things that need to be rebalanced at once that it obviously takes longer when there's more stuff to rebalance.",
    "start": "219533",
    "end": "225333"
  },
  {
    "text": "But I mean, I can prove this kind of with our console of time.",
    "start": "226233",
    "end": "232133"
  },
  {
    "text": "Ideally, worst case scenario is every shard is on the wrong pod.",
    "start": "232433",
    "end": "236800"
  },
  {
    "text": "So you can see that this seems to be where the time is being taken around the adjustment.",
    "start": "247533",
    "end": "255000"
  },
  {
    "text": "So all the work that's being done within the shard manager could",
    "start": "255266",
    "end": "260866"
  },
  {
    "text": "be taking out this time, or we could have potentially an issue within the test clock, or we could have an issue",
    "start": "260866",
    "end": "265933"
  },
  {
    "text": "within the shard manager. I haven't actually spent any time looking into this yet.",
    "start": "265933",
    "end": "270633"
  },
  {
    "text": "But if we look into--",
    "start": "271533",
    "end": "275332"
  },
  {
    "text": "You mentioned last time that if you were to comment out some fiber forking--",
    "start": "277433",
    "end": "283733"
  },
  {
    "text": "You're right. You're right. I forgot. If we go into the shard manager, where is it?",
    "start": "283733",
    "end": "289066"
  },
  {
    "text": "So if I go to the bottom, where are you?",
    "start": "292566",
    "end": "297599"
  },
  {
    "text": "Maybe it's here. I'm trying to remember what it was. Maybe the persistence of--",
    "start": "303666",
    "end": "309233"
  },
  {
    "text": "No, it's not persistence because persistence in the test is a no-op, at least for this one. Oh, yeah, but it's",
    "start": "309633",
    "end": "315433"
  },
  {
    "text": "scheduled, if I remember. So maybe it's that that is firing as well when you adjust the clock.",
    "start": "315433",
    "end": "320832"
  },
  {
    "text": "Yeah, the pod health-- So the two fibers that are running to",
    "start": "321199",
    "end": "326266"
  },
  {
    "text": "check pod health on a schedule and to rebalance on a schedule are actually what seems to",
    "start": "326266",
    "end": "331533"
  },
  {
    "text": "be taking up the most time. So if I disable this, the",
    "start": "331533",
    "end": "341533"
  },
  {
    "text": "tests finish much faster. And then if I additionally disable this, they finish almost instantly.",
    "start": "341533",
    "end": "348599"
  },
  {
    "text": "So I thought that was kind of interesting. Can we check the time now?",
    "start": "351233",
    "end": "356633"
  },
  {
    "text": "How much time is spent during the yield? Oh, yeah. You mean around the test clock?",
    "start": "356633",
    "end": "362366"
  },
  {
    "text": "Yeah, I mean the console log that you added. I think I removed it. I'll put it back.",
    "start": "364633",
    "end": "369066"
  },
  {
    "text": "One of those things, right? Can you please just change the test clock",
    "start": "386133",
    "end": "391333"
  },
  {
    "text": "adjustment to instead just be an effect.yield now?",
    "start": "391333",
    "end": "397965"
  },
  {
    "text": "Because what I'm thinking is this doesn't really have anything to do with the test clock itself.",
    "start": "403833",
    "end": "412199"
  },
  {
    "text": "Rather, this fiber yields to--",
    "start": "412733",
    "end": "418166"
  },
  {
    "text": "Whenever you do an adjustment, probably there's some yield that happens, and it gives space to the other",
    "start": "420933",
    "end": "428466"
  },
  {
    "text": "background fibers to do work. And there can be synchronous work that",
    "start": "428466",
    "end": "434066"
  },
  {
    "text": "takes up the three seconds in something there.",
    "start": "434066",
    "end": "440766"
  },
  {
    "text": "Most likely that's the case.",
    "start": "441733",
    "end": "445266"
  },
  {
    "text": "I mean, how would we go about--",
    "start": "457833",
    "end": "463066"
  },
  {
    "text": "Then what else do you want me to do within the shard manager itself to try to--",
    "start": "463433",
    "end": "468432"
  },
  {
    "text": "I would say first of all, try to yield a few times because this is still fast.",
    "start": "468466",
    "end": "474966"
  },
  {
    "text": "Also, it would be probably useful to just enable that single test so we don't have the best.",
    "start": "476433",
    "end": "483666"
  },
  {
    "text": "Let's see if that works. Hang on. I don't know why we use",
    "start": "493599",
    "end": "499633"
  },
  {
    "text": "the effect.only method. It doesn't seem to-- I have to explicitly skip them.",
    "start": "499633",
    "end": "506133"
  },
  {
    "text": "Let's look into that.",
    "start": "507866",
    "end": "508633"
  },
  {
    "text": "I think it depends how you run the tests. I mean, it turned off concurrent, but--",
    "start": "515033",
    "end": "519932"
  },
  {
    "text": "But you're running tests only on that file? Yes.",
    "start": "521433",
    "end": "524566"
  },
  {
    "text": "I only triggered it for this particular file. But anyways, we're only",
    "start": "530666",
    "end": "537066"
  },
  {
    "text": "running this test now. And maybe one other thing I can do is",
    "start": "537066",
    "end": "542500"
  },
  {
    "text": "let's just break out of this.",
    "start": "542500",
    "end": "546733"
  },
  {
    "text": "Let's do this.",
    "start": "550666",
    "end": "551566"
  },
  {
    "text": "Let's try to yield a few times now instead of just one.",
    "start": "568266",
    "end": "572300"
  },
  {
    "text": "I mean do you want me to re-comment in all the fibers that are supposed to be running in the shard manager?",
    "start": "580500",
    "end": "586000"
  },
  {
    "text": "Fine.",
    "start": "587866",
    "end": "588199"
  },
  {
    "text": "Let's try to revert to the test clock adjustment.",
    "start": "605500",
    "end": "609600"
  },
  {
    "text": "And don't just only enable a few fibers,",
    "start": "611600",
    "end": "620032"
  },
  {
    "text": "like one fiber at a time. And let's monitor the",
    "start": "620033",
    "end": "625665"
  },
  {
    "text": "time spent in that fiber. So why don't we start with maybe the rebalance fiber?",
    "start": "625666",
    "end": "632365"
  },
  {
    "text": "I mean, there's other stuff going on in the shard manager, but this is the one that takes the most time. So maybe we can just leave that one.",
    "start": "642899",
    "end": "648865"
  },
  {
    "text": "It's just like this. And if you disable this one, you can take that.",
    "start": "659166",
    "end": "663766"
  },
  {
    "text": "How does Simulate actually perform the simulation?",
    "start": "665100",
    "end": "668300"
  },
  {
    "text": "Does it cause rebalance? First fully calls rebalance? No, Simulate is just a helper to for each",
    "start": "670399",
    "end": "678466"
  },
  {
    "text": "over a set of events. And HandleEvent just calls the shard manager to register",
    "start": "678466",
    "end": "684366"
  },
  {
    "text": "unregister pods at the moment. There's no other events that-- OK. That are being done.",
    "start": "684399",
    "end": "688333"
  },
  {
    "text": "So if we go back to where the--",
    "start": "689433",
    "end": "691899"
  },
  {
    "text": "That's all Simulate does. So I'm creating 20 pods right now, registering 20 pods.",
    "start": "697133",
    "end": "701333"
  },
  {
    "text": "Can we log every single repetition of the",
    "start": "703133",
    "end": "709933"
  },
  {
    "text": "rebalance with a time? So at least we know each call to rebalance.",
    "start": "709933",
    "end": "716166"
  },
  {
    "text": "How long does it take? It's going to use the test clock to give us the time, though.",
    "start": "716166",
    "end": "721566"
  },
  {
    "text": "Do I-- Well, you could just make a quick time",
    "start": "724899",
    "end": "732333"
  },
  {
    "text": "that uses console time later.",
    "start": "732333",
    "end": "738333"
  },
  {
    "text": "Actually, what did I just do?",
    "start": "748166",
    "end": "750233"
  },
  {
    "text": "So you want to log the time that the rebalance takes? This fact right here?",
    "start": "753666",
    "end": "757566"
  },
  {
    "text": "Yeah, and then a top after this.",
    "start": "770033",
    "end": "771933"
  },
  {
    "text": "Oh, also remember that calling register unregister actually calls rebalance.",
    "start": "781533",
    "end": "788199"
  },
  {
    "text": "Yeah, I mean-- It may be something like when you call",
    "start": "794266",
    "end": "799833"
  },
  {
    "text": "register unregister, you fork a rebalance. And that uses semaphore.",
    "start": "799833",
    "end": "805033"
  },
  {
    "text": "And I think that maybe that semaphore is kind of being held by too many triggered",
    "start": "805833",
    "end": "813600"
  },
  {
    "text": "rebalance processes.",
    "start": "813600",
    "end": "815800"
  },
  {
    "text": "Maybe I'm on the front path, but--",
    "start": "818733",
    "end": "820800"
  },
  {
    "text": "If you look at the register and unregister methods of the shard manager,",
    "start": "827233",
    "end": "831899"
  },
  {
    "text": "over some conditions, they actually trigger rebalance.",
    "start": "832633",
    "end": "838866"
  },
  {
    "text": "And then they are forked on a different fiber inside of the same--",
    "start": "841633",
    "end": "848365"
  },
  {
    "text": "inside of the manager scope, OK? And the rebalance thing uses a semaphore.",
    "start": "849100",
    "end": "856866"
  },
  {
    "text": "So I am interested in seeing how many times rebalance is actually triggered.",
    "start": "857000",
    "end": "864566"
  },
  {
    "text": "So maybe we can put something like an effect.log inside the actual rebalance method and see,",
    "start": "864766",
    "end": "870733"
  },
  {
    "text": "because I would assume that you are doing something like having 20 times rebalance.",
    "start": "871100",
    "end": "878366"
  },
  {
    "text": "I mean, somewhere, the time has to be spent. Yeah.",
    "start": "882933",
    "end": "885433"
  },
  {
    "text": "There's no body in that function.",
    "start": "889466",
    "end": "891366"
  },
  {
    "text": "OK. And that's exactly what I was expecting. And we have one at a time, because the",
    "start": "900666",
    "end": "908399"
  },
  {
    "text": "rebalance should use a semaphore in order to disallow having concurrent rebalances.",
    "start": "908399",
    "end": "914966"
  },
  {
    "text": "Also, the rebalance uses a synchronized ref.",
    "start": "914966",
    "end": "918966"
  },
  {
    "text": "Yeah, but just does it get on the synchronized ref? Yeah. That's not going to lock.",
    "start": "920733",
    "end": "926833"
  },
  {
    "text": "Yeah. It does get, yeah. But then everything should be-- you know, one inside",
    "start": "930233",
    "end": "935833"
  },
  {
    "text": "are rebalance semaphore. Yeah, rebalance semaphore with permits one.",
    "start": "935833",
    "end": "940233"
  },
  {
    "text": "So if The rebalance process takes some time.",
    "start": "941300",
    "end": "945933"
  },
  {
    "text": "I would assume that basically we are queuing one rebalance call after the other.",
    "start": "946933",
    "end": "952500"
  },
  {
    "text": "Yeah, but if they only take-- like if the longest rebalance takes 200",
    "start": "953933",
    "end": "960000"
  },
  {
    "text": "millis, and then the rest are taking like a millisecond or less, like is this really where the time's being spent?",
    "start": "960000",
    "end": "964699"
  },
  {
    "text": "Yeah, but I think we have too many console.time with x. That's impossible to know.",
    "start": "965833",
    "end": "971833"
  },
  {
    "text": "I would monitor every single rebalance. Yeah, exactly. We need to time the",
    "start": "971833",
    "end": "979466"
  },
  {
    "text": "rebalance methods and see.",
    "start": "979466",
    "end": "981733"
  },
  {
    "text": "No, no, just inside the rebalance. Do you want me to remove these now? Yes.",
    "start": "990566",
    "end": "995533"
  },
  {
    "text": "Basically, we have to monitor each rebalance. And that's not the only thing that calls rebalance.",
    "start": "997399",
    "end": "1003333"
  },
  {
    "text": "Yeah.",
    "start": "1004233",
    "end": "1004433"
  },
  {
    "text": "Do you want me to-- That's probably already inside the synchronize the-- We basically need-- when you do rebalance",
    "start": "1017633",
    "end": "1024865"
  },
  {
    "text": "semaphore block with permits, we need outside to do something like effect.time and see the total--",
    "start": "1024866",
    "end": "1032199"
  },
  {
    "text": "Or just this console time. Yeah, effect.time is not going to work, but we could do effect.zip.",
    "start": "1032233",
    "end": "1036366"
  },
  {
    "text": "This doesn't work, because here you are already inside the permit.",
    "start": "1037600",
    "end": "1041566"
  },
  {
    "text": "You want to wrap the whole thing. Yeah. The best thing is if you extract this",
    "start": "1042833",
    "end": "1048233"
  },
  {
    "text": "whole effect to a variable, like call it const",
    "start": "1048233",
    "end": "1054766"
  },
  {
    "text": "effect, and then just return-- Yeah.",
    "start": "1054766",
    "end": "1058933"
  },
  {
    "text": "So you monitor the whole thing, basically.",
    "start": "1061633",
    "end": "1063633"
  },
  {
    "text": "Right. Can we also add a counter? Yeah. That because otherwise, I think there's too many--",
    "start": "1104899",
    "end": "1110533"
  },
  {
    "text": "Rebalance. Sure. Let's go to--",
    "start": "1111333",
    "end": "1116733"
  },
  {
    "text": "Just when you pull the function, basically. Refuse them, I thought random, and use that as number.",
    "start": "1117133",
    "end": "1122566"
  },
  {
    "text": "Like you're talking about a global counter that just counts the number of times you rebalanced.",
    "start": "1127466",
    "end": "1131300"
  },
  {
    "text": "No, I was meaning the--",
    "start": "1133233",
    "end": "1135433"
  },
  {
    "text": "When you log-- The issue I see here is that you start, basically, n times the first console.time,",
    "start": "1139166",
    "end": "1146466"
  },
  {
    "text": "and then you do time n with the exact same label, so they could conflict.",
    "start": "1147066",
    "end": "1154033"
  },
  {
    "text": "I gotcha. Every call to rebalance should has its own identifier.",
    "start": "1154066",
    "end": "1159033"
  },
  {
    "text": "Got it. So the identifier has to be generated inside the-- as a first thing.",
    "start": "1160633",
    "end": "1165433"
  },
  {
    "text": "Not going to work there. You need something like-- As to the inside.",
    "start": "1172033",
    "end": "1177133"
  },
  {
    "text": "Yeah. The easiest is if you write a generator, and inside the generator, you",
    "start": "1178066",
    "end": "1184233"
  },
  {
    "text": "just have some random thing. And that's why I say that you always",
    "start": "1184233",
    "end": "1190433"
  },
  {
    "text": "write better code than me, because I would have think about acquire user release, and",
    "start": "1190433",
    "end": "1195866"
  },
  {
    "text": "acquire doing console.time, and create an identifier,",
    "start": "1195866",
    "end": "1202000"
  },
  {
    "text": "then in use perform the work, and then in release. And then it explodes.",
    "start": "1202366",
    "end": "1208132"
  },
  {
    "text": "Yeah, but that's the thing you're basically doing, right? Creating an--",
    "start": "1212533",
    "end": "1217866"
  },
  {
    "text": "Maxwell here is very precise. He's dropping console.time inside an effect.sync.",
    "start": "1219366",
    "end": "1224266"
  },
  {
    "text": "Yeah, precise. That's absolutely unnecessary. It really is. It really is. I don't know why I do this.",
    "start": "1224933",
    "end": "1230433"
  },
  {
    "text": "I don't know why I do this. Just remove the yields.",
    "start": "1230633",
    "end": "1236266"
  },
  {
    "text": "Yeah. I will. Give me one sec. Even the one around",
    "start": "1237000",
    "end": "1243033"
  },
  {
    "text": "will change less, yeah. Well, it works. It's fine.",
    "start": "1243033",
    "end": "1247133"
  },
  {
    "text": "Return. No, no. Yeah, you have to yield the effect inside, yeah.",
    "start": "1248600",
    "end": "1254500"
  },
  {
    "text": "Yeah. I think we're just returning void.",
    "start": "1256166",
    "end": "1260933"
  },
  {
    "text": "Right. But whatever. This is still fine. Doesn't matter.",
    "start": "1261600",
    "end": "1265266"
  },
  {
    "text": "OK. In one second, I just reran this. OK.",
    "start": "1269866",
    "end": "1274898"
  },
  {
    "text": "OK, maybe. OK. OK. Yeah, that random wasn't the best choice.",
    "start": "1278133",
    "end": "1281466"
  },
  {
    "text": "Well. Well, it's fine. OK.",
    "start": "1284966",
    "end": "1290065"
  },
  {
    "text": "But it's something that I kind of expected. We have about 400",
    "start": "1290100",
    "end": "1295898"
  },
  {
    "text": "milliseconds spent rebalancing.",
    "start": "1295899",
    "end": "1298533"
  },
  {
    "text": "That would make sense. Because on the first rebalance, if I remember correctly,",
    "start": "1301600",
    "end": "1307866"
  },
  {
    "text": "it won't rebalance the whole thing. It will rebalance just a percentage of the total amount of pods to rebalance.",
    "start": "1308566",
    "end": "1315733"
  },
  {
    "text": "And then the second rebalance would perform the rest of rebalancement.",
    "start": "1316766",
    "end": "1321600"
  },
  {
    "text": "And ideally, after the second or the third execution, we should have a balanced system.",
    "start": "1322199",
    "end": "1326899"
  },
  {
    "text": "That's why the other holes take so little.",
    "start": "1327333",
    "end": "1331633"
  },
  {
    "text": "They should do mostly anything.",
    "start": "1332399",
    "end": "1335199"
  },
  {
    "text": "Now the thing is how we went from 400 milliseconds to basically two seconds.",
    "start": "1338566",
    "end": "1348466"
  },
  {
    "text": "It was the whole time. Four seconds. Just about three.",
    "start": "1348733",
    "end": "1352333"
  },
  {
    "text": "Yeah. Two seconds and a half. Yeah. We are spending two seconds somewhere.",
    "start": "1355033",
    "end": "1360066"
  },
  {
    "text": "Do we do anything? Two seconds and a half. In rebalance. Do we call?",
    "start": "1360366",
    "end": "1365433"
  },
  {
    "text": "Yeah, but the rebalance calls are timed there. Yeah. So ideally, I would have expected",
    "start": "1368033",
    "end": "1375733"
  },
  {
    "text": "the sum of those times to be almost two seconds, something like that, if the problem was",
    "start": "1375733",
    "end": "1381600"
  },
  {
    "text": "in the actual rebalance algorithm. Would it be that since we're locking the synchronized ref",
    "start": "1381600",
    "end": "1391600"
  },
  {
    "text": "within a rebalance that-- Well, maybe you can try to move out--",
    "start": "1391600",
    "end": "1397033"
  },
  {
    "text": "you can try to move out the rebalance semaphore from the effect body to your time.",
    "start": "1398399",
    "end": "1405366"
  },
  {
    "text": "But I don't think that that's the issue, because the time--",
    "start": "1405366",
    "end": "1411366"
  },
  {
    "text": "Each single call is pretty fast. Try to adjust for like--",
    "start": "1411366",
    "end": "1418466"
  },
  {
    "text": "One minute. One minute instead of 10.",
    "start": "1419399",
    "end": "1421933"
  },
  {
    "text": "It reduced. Yeah.",
    "start": "1438966",
    "end": "1441433"
  },
  {
    "text": "Which is the default configuration for the rebalance interval right now, I don't remember?",
    "start": "1444166",
    "end": "1449833"
  },
  {
    "text": "No. There it is. This is the default configuration that I have right now.",
    "start": "1457966",
    "end": "1464066"
  },
  {
    "text": "Rebalance interval 10 seconds. OK, so let's try to adjust for 20 seconds.",
    "start": "1464666",
    "end": "1469666"
  },
  {
    "text": "It should do something like just to-- The retry interval or just the interval?",
    "start": "1470166",
    "end": "1475433"
  },
  {
    "text": "The regular rebalance interval is 20 seconds. Leave it 10 seconds, that interval.",
    "start": "1477266",
    "end": "1482333"
  },
  {
    "text": "And in our test, instead of adjusting one minute, let's try to adjust 20 seconds. Ideally, we should",
    "start": "1483666",
    "end": "1488766"
  },
  {
    "text": "have only two rebalances.",
    "start": "1488766",
    "end": "1490233"
  },
  {
    "text": "There you go, ideally. OK, but why?",
    "start": "1494866",
    "end": "1500766"
  },
  {
    "text": "Well, there's other stuff happening like down here, like with rebalancing, which I can comment out.",
    "start": "1501366",
    "end": "1508466"
  },
  {
    "text": "For now. OK.",
    "start": "1510100",
    "end": "1511100"
  },
  {
    "text": "It looks like there's many parked fibers. And when you adjust, that",
    "start": "1519266",
    "end": "1527166"
  },
  {
    "text": "means every time the test clock actually has some internal delay,",
    "start": "1527166",
    "end": "1532733"
  },
  {
    "text": "I don't remember how much. But it forces a normal delay.",
    "start": "1533500",
    "end": "1543399"
  },
  {
    "text": "Now we have three. If you bump the test clock dot adjust to,",
    "start": "1544733",
    "end": "1550132"
  },
  {
    "text": "let's say, one minute, we expect to have more",
    "start": "1550133",
    "end": "1557066"
  },
  {
    "text": "rebalance triggered, right? Or should it-- I don't know much about",
    "start": "1557066",
    "end": "1563333"
  },
  {
    "text": "the test clock dot adjust. But one issue could be",
    "start": "1563333",
    "end": "1570333"
  },
  {
    "text": "that the forked rebalance job was spaced out of an affixed interval.",
    "start": "1570333",
    "end": "1575500"
  },
  {
    "text": "And so adjusting for 10 minutes meant that we were rebalancing basically 60 times.",
    "start": "1576133",
    "end": "1581433"
  },
  {
    "text": "And all of those 60 times maybe were triggered basically in the same flash.",
    "start": "1584333",
    "end": "1591299"
  },
  {
    "text": "One after the other. Even if we are rebalancing 60 times, after the initial one",
    "start": "1593133",
    "end": "1599266"
  },
  {
    "text": "or two rebalances here, it's taking a millisecond to do the actual work of rebalancing.",
    "start": "1599266",
    "end": "1605600"
  },
  {
    "text": "So even if we do that 60 times, it's only 60 milliseconds. Or 58 more milliseconds that we're adding",
    "start": "1605833",
    "end": "1612833"
  },
  {
    "text": "to our initial 200 or 300 for the first two rebalances.",
    "start": "1612833",
    "end": "1617600"
  },
  {
    "text": "So-- The test clock, every time it suspends,",
    "start": "1619600",
    "end": "1627333"
  },
  {
    "text": "it waits five milliseconds. Five real milliseconds.",
    "start": "1627333",
    "end": "1632533"
  },
  {
    "text": "If you open the test clock dot TS file in the effect core repo,",
    "start": "1634366",
    "end": "1642966"
  },
  {
    "text": "there's an effect dot sleep.",
    "start": "1643000",
    "end": "1649866"
  },
  {
    "text": "And just search for dot sleep. No, it's with the-- Oh, it's in the implementation.",
    "start": "1651466",
    "end": "1656899"
  },
  {
    "text": "Overcase effect. No, all implementation is there.",
    "start": "1657300",
    "end": "1661398"
  },
  {
    "text": "Sorry. One sec. Oh, yeah.",
    "start": "1663600",
    "end": "1667132"
  },
  {
    "text": "Try to have these at like one millisecond instead of five.",
    "start": "1671100",
    "end": "1676266"
  },
  {
    "text": "Let me go back to-- There is also the same thing, I think, on",
    "start": "1693066",
    "end": "1698433"
  },
  {
    "text": "the same line of the test clock. It waits for 10 milliseconds",
    "start": "1698433",
    "end": "1704433"
  },
  {
    "text": "every time it awaits suspended.",
    "start": "1704433",
    "end": "1706733"
  },
  {
    "text": "Yes. Await suspended. There's another dot sleep.",
    "start": "1713266",
    "end": "1716366"
  },
  {
    "text": "That's five and 10. That's five and 10. And ideally--",
    "start": "1721300",
    "end": "1728633"
  },
  {
    "text": "Let's say one and-- try one and two. Oh, yeah. That seems",
    "start": "1729133",
    "end": "1736733"
  },
  {
    "text": "definitely more faster. Let's actually wait. Let's go back to this and this. Save.",
    "start": "1736733",
    "end": "1744132"
  },
  {
    "text": "Okay. Much slower.",
    "start": "1748733",
    "end": "1750333"
  },
  {
    "text": "Yeah. I'm not sure if it's fine. The internals",
    "start": "1763100",
    "end": "1769565"
  },
  {
    "text": "of these are unbelievably complex.",
    "start": "1769566",
    "end": "1772666"
  },
  {
    "text": "I mean, if we know that the majority of",
    "start": "1776766",
    "end": "1782000"
  },
  {
    "text": "the time is being spent just by the test clock internally, I'm okay with leaving the",
    "start": "1782000",
    "end": "1788500"
  },
  {
    "text": "tests running for a long period of time until we figure out how to make the test clock better.",
    "start": "1788500",
    "end": "1793033"
  },
  {
    "text": "No, but I'm not sure that there's any reason why the test clock should take",
    "start": "1795566",
    "end": "1800933"
  },
  {
    "text": "this long. Remember, there were some tests that were not passing and I was like, okay,",
    "start": "1800933",
    "end": "1805966"
  },
  {
    "text": "let's give it a few time more. But then we probably had some internal race conditions. Now I",
    "start": "1805966",
    "end": "1812366"
  },
  {
    "text": "tested locally with one and two milliseconds and the thing, all the tests pass.",
    "start": "1812366",
    "end": "1817733"
  },
  {
    "text": "Yeah. It's basically just giving one millisecond of tick to flash whatever is trying to do on the",
    "start": "1820666",
    "end": "1826266"
  },
  {
    "text": "scheduler. Yeah. But then it would just yield. Yeah. I could use it maybe",
    "start": "1826266",
    "end": "1832899"
  },
  {
    "text": "just yielding would work. I have no idea. Or rather",
    "start": "1832899",
    "end": "1839166"
  },
  {
    "text": "maybe yield to the timer. Yeah. But well, to the scope of the test,",
    "start": "1839166",
    "end": "1848799"
  },
  {
    "text": "we can assume that if we know the configuration of the",
    "start": "1848800",
    "end": "1853733"
  },
  {
    "text": "cluster manager, the shard manager, we could actually calculate how many",
    "start": "1853833",
    "end": "1858866"
  },
  {
    "text": "rebalance at maximum it would take to actually rebalance the whole thing. So we can for",
    "start": "1858866",
    "end": "1865966"
  },
  {
    "text": "now adjust that time to be something that it's reasonable. Maybe one minute for me is more than",
    "start": "1865966",
    "end": "1872833"
  },
  {
    "text": "enough. As I said, I think that after the third rebalance, you shouldn't do anything.",
    "start": "1872833",
    "end": "1877933"
  },
  {
    "text": "I mean, it's not even saving us much time to change the adjustment at this point.",
    "start": "1878600",
    "end": "1883033"
  },
  {
    "text": "You're still everything. It's 200 millis here. So I think we could probably leave the test alone.",
    "start": "1885733",
    "end": "1895666"
  },
  {
    "text": "This is down to a second from four seconds. And if I re-enable all the rest of the",
    "start": "1908733",
    "end": "1917766"
  },
  {
    "text": "tests and then turn off all the logging that we added,",
    "start": "1917766",
    "end": "1921433"
  },
  {
    "text": "I run these concurrently like I was before.",
    "start": "1925566",
    "end": "1928565"
  },
  {
    "text": "I mean, it's always a complex thing. It is, isn't it?",
    "start": "1931066",
    "end": "1936166"
  },
  {
    "text": "I mean, we already shaved off two seconds from the test. I'm just by modifying the test clock.",
    "start": "1936766",
    "end": "1942433"
  },
  {
    "text": "It makes sense because we rebalance every 10 seconds. So it means that",
    "start": "1951066",
    "end": "1958466"
  },
  {
    "text": "I should perform rebalance about a",
    "start": "1958466",
    "end": "1965299"
  },
  {
    "text": "minute, maybe 60 times. 60 times, yeah.",
    "start": "1965300",
    "end": "1971000"
  },
  {
    "text": "Why is it even doing that?",
    "start": "1975566",
    "end": "1983533"
  },
  {
    "text": "The good news is that rebalancing and scaling is working.",
    "start": "1991266",
    "end": "1995166"
  },
  {
    "text": "I'm hacking around the test clock now. Yeah, Mike is hacking the test clock. We",
    "start": "2000433",
    "end": "2005466"
  },
  {
    "text": "should probably watch him. Probably more interesting than fiddling around with these tests right now.",
    "start": "2005466",
    "end": "2011433"
  },
  {
    "text": "Because I don't even understand why a way to suspend it was yielding more.",
    "start": "2012733",
    "end": "2017366"
  },
  {
    "text": "More than suspend it, right? Yeah. I know if that was some kind of amount",
    "start": "2019300",
    "end": "2027366"
  },
  {
    "text": "decided just to give time, the system to do something, I would have expected that to be the same",
    "start": "2027366",
    "end": "2034033"
  },
  {
    "text": "or something like that. Yeah, I mean, now I've done something",
    "start": "2034033",
    "end": "2040233"
  },
  {
    "text": "unbelievably stupid and it actually works.",
    "start": "2040233",
    "end": "2043433"
  },
  {
    "text": "Something like calling just the scheduler or something like that should be. No, I quite literally had a set timeout",
    "start": "2045766",
    "end": "2052366"
  },
  {
    "text": "and a sync effect with a set timeout to zero. Oh, yes.",
    "start": "2052366",
    "end": "2056866"
  },
  {
    "text": "I'm going to make a PR and I'm going to show you how dumb this is. Yeah, steal the screen.",
    "start": "2058533",
    "end": "2063333"
  },
  {
    "text": "No, no, there's no point. I'll make a PR in a second and you can review the PR.",
    "start": "2065199",
    "end": "2071533"
  },
  {
    "text": "That's not so interesting.",
    "start": "2072766",
    "end": "2075100"
  },
  {
    "text": "Even today we made effect faster. At least when you're testing.",
    "start": "2079366",
    "end": "2083866"
  },
  {
    "text": "Everyone should test, right? Allegedly.",
    "start": "2087800",
    "end": "2091166"
  },
  {
    "text": "All right, so",
    "start": "2094566",
    "end": "2095599"
  },
  {
    "text": "that's good. I think that... Just looking to see if like...",
    "start": "2100466",
    "end": "2107433"
  },
  {
    "text": "So this is the test that scales... I'm trying to remind myself like the test I wrote. This is the test that's scaling up the",
    "start": "2107766",
    "end": "2117033"
  },
  {
    "text": "shard manager from an initial set of pods, adds another five pods and basically checks that rebalancing",
    "start": "2117033",
    "end": "2123666"
  },
  {
    "text": "doesn't happen like all at once. But that eventually the cluster will settle to a fully rebalanced state.",
    "start": "2123666",
    "end": "2130366"
  },
  {
    "text": "Similar test for scaling down. We set up 25 pods, then we scale down five pods.",
    "start": "2130866",
    "end": "2138266"
  },
  {
    "text": "We make sure rebalancing is happening. Or in this case, we just make sure that rebalancing",
    "start": "2138633",
    "end": "2144500"
  },
  {
    "text": "happens immediately because rebalancing should happen immediately for unregistering, right?",
    "start": "2144500",
    "end": "2149233"
  },
  {
    "text": "I don't even know. Shard manager.ts. Unregister.",
    "start": "2152166",
    "end": "2157800"
  },
  {
    "text": "I don't remember. I think that it should happen immediately only if it sees that there are unassigned pods. No, it",
    "start": "2162966",
    "end": "2169633"
  },
  {
    "text": "has to happen immediately. Unregistering pods. Unregistering pods. It has to happen immediately because otherwise you could be left",
    "start": "2169633",
    "end": "2175866"
  },
  {
    "text": "with shards sitting on pods that have now been unregistered. Yeah, exactly.",
    "start": "2175866",
    "end": "2179765"
  },
  {
    "text": "So this... So this is creating the issue of having unassigned shard IDs. Right. So this test is basically",
    "start": "2180899",
    "end": "2186933"
  },
  {
    "text": "testing that when we scale down, we rebalance immediately. So",
    "start": "2187100",
    "end": "2193066"
  },
  {
    "text": "we should have an immediate settling of the cluster back up to 15 shards per pod. And then this test is",
    "start": "2193066",
    "end": "2202466"
  },
  {
    "text": "basically testing that when we shut down the shard manager, when",
    "start": "2202466",
    "end": "2207866"
  },
  {
    "text": "it actually shuts down, like it runs its finalizers and everything, that it flushes its state",
    "start": "2207866",
    "end": "2213366"
  },
  {
    "text": "to whatever persistent storage is set up. And then we test essentially grabbing",
    "start": "2213366",
    "end": "2221666"
  },
  {
    "text": "that state from storage and making sure that we actually get something back. So clearly",
    "start": "2221666",
    "end": "2230432"
  },
  {
    "text": "there's a lot more tests that need to be added, which I think I mentioned last time, but this is already a good start for",
    "start": "2230433",
    "end": "2236033"
  },
  {
    "text": "at least the simple behavior of the cluster. Yeah, no, definitely. Okay. Did I re-enable",
    "start": "2236033",
    "end": "2242600"
  },
  {
    "text": "everything in the shard manager?",
    "start": "2242600",
    "end": "2245933"
  },
  {
    "text": "I actually don't think I did. I did not. Let me",
    "start": "2248866",
    "end": "2255366"
  },
  {
    "text": "re-enable all of the fibers.",
    "start": "2255366",
    "end": "2256932"
  },
  {
    "text": "By the way, I just posted a link of the PR. So this, yeah, perfect. Still taking",
    "start": "2262399",
    "end": "2271933"
  },
  {
    "text": "two milliseconds. Let me take a look,",
    "start": "2272066",
    "end": "2275765"
  },
  {
    "text": "I think you can even check with the",
    "start": "2278933",
    "end": "2290933"
  },
  {
    "text": "with the snapshot or just base the branch on this. I'm not too",
    "start": "2291033",
    "end": "2299433"
  },
  {
    "text": "familiar with the internals. I'm not too familiar with the internals, but just asking a question",
    "start": "2299433",
    "end": "2305466"
  },
  {
    "text": "that maybe you also could use for the viewers. Why don't we use",
    "start": "2305466",
    "end": "2311633"
  },
  {
    "text": "directly the scheduler dot? Is there any reason for that?",
    "start": "2311633",
    "end": "2317166"
  },
  {
    "text": "Because for example, there are some environments where setting a timeout with the environment",
    "start": "2317266",
    "end": "2323533"
  },
  {
    "text": "is not really happy with. Naming one, or yeah, native, I know.",
    "start": "2323899",
    "end": "2329500"
  },
  {
    "text": "Why would React Native have a problem with a set timeout? Using the old always, at least in older",
    "start": "2332766",
    "end": "2340833"
  },
  {
    "text": "versions, I think they fixed it recently with armus, but in older version, they complained",
    "start": "2340833",
    "end": "2347066"
  },
  {
    "text": "about having timer with the, and they instead forced you to move, warned you more than forced",
    "start": "2347066",
    "end": "2354800"
  },
  {
    "text": "warned you to move to something else like scheduler,",
    "start": "2354800",
    "end": "2360733"
  },
  {
    "text": "document, the name of the scheduler task, that API. I don't remember the name.",
    "start": "2361000",
    "end": "2368433"
  },
  {
    "text": "Well, the issue is if I yield with the scheduler, the scheduler doesn't really",
    "start": "2369966",
    "end": "2376333"
  },
  {
    "text": "yield to the timers. So the default scheduler",
    "start": "2376333",
    "end": "2382066"
  },
  {
    "text": "takes, like maybe I could,",
    "start": "2382066",
    "end": "2384699"
  },
  {
    "text": "well, I could try if it works even with yielding to the scheduler, because then it would be even",
    "start": "2387933",
    "end": "2395166"
  },
  {
    "text": "faster. But this was a way where basically this is the least. Before it",
    "start": "2395333",
    "end": "2406500"
  },
  {
    "text": "was sleeping five milliseconds and 10, this is the quickest you can",
    "start": "2406500",
    "end": "2412133"
  },
  {
    "text": "sleep. And I've erased a little bit more, but now I'm gonna check",
    "start": "2412133",
    "end": "2417866"
  },
  {
    "text": "if everything still passes with what you've just said, because",
    "start": "2418866",
    "end": "2425099"
  },
  {
    "text": "if that's the case, then that wouldn't really be...",
    "start": "2425100",
    "end": "2432033"
  },
  {
    "text": "Let me see, can do core with...",
    "start": "2432766",
    "end": "2435633"
  },
  {
    "text": "Again, it was just a curiosity, not on topic, man. I don't, I honestly have no idea.",
    "start": "2438866",
    "end": "2445399"
  },
  {
    "text": "It was just because we already have a scheduler which basically",
    "start": "2447633",
    "end": "2452533"
  },
  {
    "text": "is saying I'm scheduling things, tasks,",
    "start": "2454366",
    "end": "2458966"
  },
  {
    "text": "and run them as soon as possible. On the other side, it can't be canceled,",
    "start": "2459866",
    "end": "2464933"
  },
  {
    "text": "but I don't think that's even relevant. Yeah, exactly.",
    "start": "2464933",
    "end": "2467466"
  },
  {
    "text": "And for people who are wondering what the heck is going on inside the test clock, these methods are all here, pretty sure",
    "start": "2472899",
    "end": "2478300"
  },
  {
    "text": "they're all here, to essentially warn,",
    "start": "2478300",
    "end": "2481466"
  },
  {
    "text": "print a warning to users, like if, I think the await suspended...",
    "start": "2483433",
    "end": "2489699"
  },
  {
    "text": "Let's see, I forget from having written this a very long time ago, but I think the idea is that",
    "start": "2491533",
    "end": "2497033"
  },
  {
    "text": "you end up warning the user if like... In the suspended warning, for example, you",
    "start": "2497666",
    "end": "2504533"
  },
  {
    "text": "display a warning message of a test is advancing the test clock, but the fiber is not suspending, meaning you're essentially just blocking.",
    "start": "2505100",
    "end": "2511066"
  },
  {
    "text": "And I think the suspended does something similar.",
    "start": "2513133",
    "end": "2517965"
  },
  {
    "text": "Cool, the news is if I use the scheduler, it doesn't work.",
    "start": "2521533",
    "end": "2526766"
  },
  {
    "text": "Okay. So it has to be a timer at least.",
    "start": "2527433",
    "end": "2531699"
  },
  {
    "text": "Oh, well, it's fine.",
    "start": "2533133",
    "end": "2533866"
  },
  {
    "text": "Yeah, it has to be a timer. Let's say with priority one.",
    "start": "2539466",
    "end": "2546199"
  },
  {
    "text": "I was gonna say, if you change the priority of the task, does it help or no?",
    "start": "2554066",
    "end": "2558333"
  },
  {
    "text": "No, it does not. Oh, well. It has to be a timer. Yeah, it was worth trying.",
    "start": "2562133",
    "end": "2568399"
  },
  {
    "text": "Cool. Well, I like this. I mean, it could even slip one",
    "start": "2570966",
    "end": "2576899"
  },
  {
    "text": "millisecond, it doesn't have to be absolutely zero. Yeah, I know. Zero is fine",
    "start": "2576899",
    "end": "2583199"
  },
  {
    "text": "with me if all the effect tests are passing. Like, if",
    "start": "2583199",
    "end": "2588265"
  },
  {
    "text": "all of the effect tests... I mean, they're all",
    "start": "2588266",
    "end": "2593432"
  },
  {
    "text": "passing, so I'm fine with zero.",
    "start": "2593433",
    "end": "2595366"
  },
  {
    "text": "How fast is this running now? Two minutes.",
    "start": "2599166",
    "end": "2602300"
  },
  {
    "text": "Yeah, I mean, even the default scheduler after like 2048 yields,",
    "start": "2609866",
    "end": "2617933"
  },
  {
    "text": "actually yields to the timer with set time on zero. Before we were using set immediate or set",
    "start": "2618833",
    "end": "2626500"
  },
  {
    "text": "timeout based on the runtime, but it generated problems and set timeout zero",
    "start": "2626500",
    "end": "2632433"
  },
  {
    "text": "seems to be kind of the go-to.",
    "start": "2632433",
    "end": "2637833"
  },
  {
    "text": "Anyway, all the tests work, so we've just made it faster, that's good.",
    "start": "2638233",
    "end": "2643266"
  },
  {
    "text": "So it was the test timer at the end. In the end, it was the test clock.",
    "start": "2648300",
    "end": "2653432"
  },
  {
    "text": "The test clock, yes. Oh, well. We asserted",
    "start": "2654500",
    "end": "2660233"
  },
  {
    "text": "that everyone else work.",
    "start": "2660233",
    "end": "2661799"
  },
  {
    "text": "It's good to merge. Is that how you read this? Before you... No!",
    "start": "2666566",
    "end": "2671799"
  },
  {
    "text": "That's not how you find it coming. Before you encounter it.",
    "start": "2672866",
    "end": "2676966"
  },
  {
    "text": "Is that how you read this? LGTOs looks good to merge. I never read it that. I always read it as... It means looks good to merge. I read it",
    "start": "2677966",
    "end": "2684533"
  },
  {
    "text": "as looks good to me. Oh yeah, me as well. I",
    "start": "2684533",
    "end": "2689932"
  },
  {
    "text": "think it looks good to merge.",
    "start": "2689933",
    "end": "2694733"
  },
  {
    "text": "I also thought that it was... How many viewers do we have? Can we pull chat?",
    "start": "2695566",
    "end": "2699866"
  },
  {
    "text": "Now I'm very curious. I also thought that maybe that's my mind that",
    "start": "2700966",
    "end": "2706933"
  },
  {
    "text": "is kind of broken. I also also as legitimate. Yes, that used to be...",
    "start": "2706933",
    "end": "2713366"
  },
  {
    "text": "Legitimate. It could also be a valid interpretation of this, I guess.",
    "start": "2715566",
    "end": "2719265"
  },
  {
    "text": "There's many different interpretations. It's let's get this merged. Let's get this merged.",
    "start": "2721100",
    "end": "2726366"
  },
  {
    "text": "Let's get this money. Wow. Looks good to me. I think I'm gonna say that let's get this",
    "start": "2726633",
    "end": "2734366"
  },
  {
    "text": "money. Let's get this money. All right, let me... Show me the money!",
    "start": "2734366",
    "end": "2741500"
  },
  {
    "text": "Let me...",
    "start": "2742733",
    "end": "2743233"
  },
  {
    "text": "I'm gonna try rebasing again.",
    "start": "2748600",
    "end": "2750966"
  },
  {
    "text": "This was our thing that our Git overlords. It doesn't break anything",
    "start": "2758766",
    "end": "2764466"
  },
  {
    "text": "while you're rebasing. Please get gods. Don't add me. You should",
    "start": "2764466",
    "end": "2769765"
  },
  {
    "text": "note that Max is not doing this with screen sharing. Oh yeah.",
    "start": "2769766",
    "end": "2774933"
  },
  {
    "text": "Out of fear of messing things up. There's a lot of... There's fear right now.",
    "start": "2775366",
    "end": "2780465"
  },
  {
    "text": "Yeah. He's saying that it's rebasing, but actually it's just cloned again the repository and just copy and",
    "start": "2780800",
    "end": "2786465"
  },
  {
    "text": "paste it. All right, nerds. The first command was remove minus",
    "start": "2786466",
    "end": "2791533"
  },
  {
    "text": "France. It's fine. You know what, nerds? I'm gonna get... Oh, okay. Finally. Some true",
    "start": "2791533",
    "end": "2798432"
  },
  {
    "text": "entertainment, finally. All my Git commands, you can all judge me. Git again.",
    "start": "2798433",
    "end": "2805533"
  },
  {
    "text": "Looks good to me. Looks good to me. Git, rebase main.",
    "start": "2805566",
    "end": "2811366"
  },
  {
    "text": "Yay! Oh, yay. My Git overlords. Now we'll test, pnpm test, and let's see.",
    "start": "2814266",
    "end": "2819733"
  },
  {
    "text": "100 failures. Exactly. Yeah, really. Okay. I",
    "start": "2820899",
    "end": "2828633"
  },
  {
    "text": "wonder what my diff was in here. Oh, yeah, okay. That makes",
    "start": "2828633",
    "end": "2836633"
  },
  {
    "text": "sense. And then I was doing... don't know why, for a moment, instead of",
    "start": "2836633",
    "end": "2842566"
  },
  {
    "text": "reading Git stash pop, I read Git stash poop.",
    "start": "2842566",
    "end": "2846366"
  },
  {
    "text": "I'm changing... I'm setting a Git alias right now.",
    "start": "2848233",
    "end": "2856466"
  },
  {
    "text": "Git alias, yeah. Git poop. All right, great. So we're",
    "start": "2858333",
    "end": "2864632"
  },
  {
    "text": "still... Look how fast it is. Yeah. Excellent. It's not. It's not. We're still spending",
    "start": "2864633",
    "end": "2871666"
  },
  {
    "text": "two seconds on... Let's try to run it again. Sometimes.",
    "start": "2871766",
    "end": "2877399"
  },
  {
    "text": "Take some time to spin up.",
    "start": "2877766",
    "end": "2879866"
  },
  {
    "text": "You can also do... Oh, no.",
    "start": "2886566",
    "end": "2891233"
  },
  {
    "text": "Oh, but, well, before we test it, just the first part of the entire test,",
    "start": "2893433",
    "end": "2899766"
  },
  {
    "text": "now we are testing the whole test again, I think. And that test does quite a few things.",
    "start": "2899833",
    "end": "2904932"
  },
  {
    "text": "Yeah, but I'm running the simulation tests concurrently, so these...",
    "start": "2905633",
    "end": "2909566"
  },
  {
    "text": "Oh, yeah. But we are still one second and... Hey, listen. It's still better than what it",
    "start": "2911033",
    "end": "2917233"
  },
  {
    "text": "was before. Yeah, it was two seconds off. Yeah. We basically reduced by 30, 30%.",
    "start": "2917233",
    "end": "2923766"
  },
  {
    "text": "I mean, there is a lot of work going on inside the Shard Manager during these 10 minutes.",
    "start": "2924366",
    "end": "2927966"
  },
  {
    "text": "And we are forcing a lot of, I guess... I",
    "start": "2929966",
    "end": "2936133"
  },
  {
    "text": "used less rebalances than I was saying before, in 10 minutes. It would probably be done rebalancing within...",
    "start": "2936133",
    "end": "2941932"
  },
  {
    "text": "One minute. Because when we logged before, we have seen that after the third rebalance,",
    "start": "2943333",
    "end": "2949133"
  },
  {
    "text": "it was quickly immediate. So I think that after one minute, it should be okay.",
    "start": "2949500",
    "end": "2954133"
  },
  {
    "text": "Because that is basically based off the configuration, you have the max amount of,",
    "start": "2963966",
    "end": "2970033"
  },
  {
    "text": "the max percentage of chance to move each time you rebalance.",
    "start": "2970033",
    "end": "2975399"
  },
  {
    "text": "Yeah, after each trigger three times should be rebalanced. Yeah, because the full setting, I don't remember what, but should be something like 20 or",
    "start": "2976500",
    "end": "2982933"
  },
  {
    "text": "25. I don't remember the max, the full setting for... Yeah.",
    "start": "2983566",
    "end": "2989333"
  },
  {
    "text": "You should have rebalance.",
    "start": "2989433",
    "end": "2993166"
  },
  {
    "text": "My rebalance rate is currently... Yeah, exactly. Two over 100, which is... Yeah, exactly.",
    "start": "2994633",
    "end": "2999766"
  },
  {
    "text": "What is that? 0.05? Yeah. Yeah.",
    "start": "3000733",
    "end": "3005533"
  },
  {
    "text": "0.02. 0.02? Yeah. That's what I said. Yeah. Quite likely,",
    "start": "3005766",
    "end": "3011333"
  },
  {
    "text": "there are a few zeros before. That's what I said, Mike. You misheard me. Stupid microphone.",
    "start": "3011333",
    "end": "3018066"
  },
  {
    "text": "Yeah, it's 0.002 of 300 shards. And in",
    "start": "3019000",
    "end": "3026433"
  },
  {
    "text": "your test, you have 20 shards, if I remember correctly. Yeah, there's 20 shards initially set up.",
    "start": "3026433",
    "end": "3033199"
  },
  {
    "text": "And then in the scale up test... Yeah, it is 20 pods. Sorry, 20 pods.",
    "start": "3033199",
    "end": "3037233"
  },
  {
    "text": "20 pods set up in the initial test. And 300 shards by default,",
    "start": "3038533",
    "end": "3044033"
  },
  {
    "text": "because I use the default config. So it's getting... We are also starting the system from no shards",
    "start": "3044266",
    "end": "3052033"
  },
  {
    "text": "assigned to no pods. So that's why it triggers the fast path",
    "start": "3052033",
    "end": "3059233"
  },
  {
    "text": "of assigning everything it can, ignoring the error balance rate. Yeah, makes sense.",
    "start": "3059233",
    "end": "3064833"
  },
  {
    "text": "I bet you it will do this. If you first simulate just",
    "start": "3064866",
    "end": "3072000"
  },
  {
    "text": "resisting one pod and wait, and then register 19 pods, it would take more,",
    "start": "3072000",
    "end": "3078266"
  },
  {
    "text": "because that setting will kick in and move slowly from the",
    "start": "3078266",
    "end": "3084066"
  },
  {
    "text": "single pod handling everything to 20 pods. Right. Balancing, yeah.",
    "start": "3084066",
    "end": "3088833"
  },
  {
    "text": "Well, then we can at least... Yeah, well, I mean, realistically, if we just spend like a second",
    "start": "3089166",
    "end": "3094933"
  },
  {
    "text": "and 1.7 seconds, the full test suite takes two seconds to",
    "start": "3094933",
    "end": "3102933"
  },
  {
    "text": "test more than 10 minutes of correctness behavior, I think",
    "start": "3102933",
    "end": "3108866"
  },
  {
    "text": "it's fine. We can't ask more. No, I don't know. I think this is good.",
    "start": "3108866",
    "end": "3113866"
  },
  {
    "text": "And I changed this to be basically essentially one rebalance interval,",
    "start": "3113933",
    "end": "3120800"
  },
  {
    "text": "even though it really could be probably just like one second, just needs to force a tick.",
    "start": "3121600",
    "end": "3125666"
  },
  {
    "text": "Okay, that was a fun detour. I don't",
    "start": "3128433",
    "end": "3135233"
  },
  {
    "text": "really know what we want to talk about today, but...",
    "start": "3135233",
    "end": "3137399"
  },
  {
    "text": "I think we got to a good point, at least we know where the",
    "start": "3140833",
    "end": "3145932"
  },
  {
    "text": "slowness was coming from, and we know that there is nothing inherently wrong in the implementation.",
    "start": "3145933",
    "end": "3153666"
  },
  {
    "text": "Again, to summarize a little bit the detour that we've done offline in the",
    "start": "3155166",
    "end": "3161066"
  },
  {
    "text": "debugging, last time that we spoke, we were in the process of moving to use mutable data",
    "start": "3161066",
    "end": "3170033"
  },
  {
    "text": "structures, so mutable maps, mutable sets, thinking that there was a",
    "start": "3170033",
    "end": "3176866"
  },
  {
    "text": "huge performance bottleneck coming from there.",
    "start": "3176866",
    "end": "3181166"
  },
  {
    "text": "But upon investigation, we've actually discovered that there was...",
    "start": "3182300",
    "end": "3187333"
  },
  {
    "text": "No, it wasn't the hash. It wasn't the hash at all. The issue was",
    "start": "3189333",
    "end": "3194600"
  },
  {
    "text": "sortWith, which was assuming",
    "start": "3194600",
    "end": "3199533"
  },
  {
    "text": "that the function, the mapping function, so basically sortWith,",
    "start": "3199966",
    "end": "3204699"
  },
  {
    "text": "array.sortWith sorts an array based on a function, and it assumed that",
    "start": "3205300",
    "end": "3212866"
  },
  {
    "text": "that function was very fast, when in reality, in our case, that",
    "start": "3212866",
    "end": "3218366"
  },
  {
    "text": "function had a cost, and basically...",
    "start": "3218366",
    "end": "3224099"
  },
  {
    "text": "This was the change that Mike made? Oh, yeah.",
    "start": "3225833",
    "end": "3229965"
  },
  {
    "text": "Yes, this was the change, and this quite literally made it...",
    "start": "3231133",
    "end": "3236899"
  },
  {
    "text": "An order of magnitude faster. Absolutely fast. From three plus seconds was 200 milliseconds.",
    "start": "3237366",
    "end": "3244199"
  },
  {
    "text": "Again, we've made that faster. That's always great. Yeah, and the funny thing is, I pinged",
    "start": "3244433",
    "end": "3251833"
  },
  {
    "text": "Giulio, and I was like, \"Am I crazy?\" or... This is actually an order of magnitude",
    "start": "3251833",
    "end": "3258433"
  },
  {
    "text": "faster. He came back after a while saying, \"Classical type classes and functional",
    "start": "3258433",
    "end": "3264299"
  },
  {
    "text": "programming sucks every time.\" Because basically that order.map input is",
    "start": "3264300",
    "end": "3270833"
  },
  {
    "text": "calculating the div F. It's going to be",
    "start": "3270833",
    "end": "3276033"
  },
  {
    "text": "calculated like n squared times. So as if n... It will produce a new F",
    "start": "3276033",
    "end": "3281265"
  },
  {
    "text": "for each time it's called... It's not even that it would produce a new F, but at every comparison...",
    "start": "3281266",
    "end": "3287766"
  },
  {
    "text": "Yeah, at every comparison... It's calculated. Yeah, it will produce a new F every time, yeah. So you recalculated n squared times where",
    "start": "3288033",
    "end": "3296233"
  },
  {
    "text": "this is instead linear. It maps twice. If you have an array of 10 elements and the very fast F,",
    "start": "3296233",
    "end": "3303100"
  },
  {
    "text": "it's probably slower, but as soon as you have any large array or anything, even a small",
    "start": "3303833",
    "end": "3311666"
  },
  {
    "text": "cost on F, it changes drastically. So this was the reason why",
    "start": "3311666",
    "end": "3316600"
  },
  {
    "text": "our... We were noticing a performance hit. It had nothing to",
    "start": "3317500",
    "end": "3323466"
  },
  {
    "text": "do with the hash maps, which I'm happy about because it really",
    "start": "3323466",
    "end": "3329199"
  },
  {
    "text": "means I didn't mess up the internals of hash map. Yeah. It would have been way harder to fix than",
    "start": "3329199",
    "end": "3335733"
  },
  {
    "text": "fixing a sorting function. See, it's here.",
    "start": "3335733",
    "end": "3339733"
  },
  {
    "text": "Let me print. Okay. Yeah. I think next steps on these are working",
    "start": "3340866",
    "end": "3349765"
  },
  {
    "text": "maybe a little bit on the mailbox. Yeah. And... Yeah, once... I don't know. We have to...",
    "start": "3349766",
    "end": "3355133"
  },
  {
    "text": "Once... Then link pods to... We've done the shard manager pretty much.",
    "start": "3355166",
    "end": "3360733"
  },
  {
    "text": "Yeah. We need to actually... Trying to remember the order of the things I did",
    "start": "3360800",
    "end": "3369166"
  },
  {
    "text": "when I did the first implementation of the alpha. I think a really good next step. We need to expose the... Also the",
    "start": "3369166",
    "end": "3378066"
  },
  {
    "text": "HTTP interface or something for the shard manager because right now we have the shard manager with",
    "start": "3378066",
    "end": "3383666"
  },
  {
    "text": "methods, but we are not exposing that anyway. Well, we haven't talked about the client",
    "start": "3383666",
    "end": "3389533"
  },
  {
    "text": "interface to the cluster at all yet. So I would almost argue that we should focus",
    "start": "3389533",
    "end": "3395133"
  },
  {
    "text": "next on... Now that we have the shard manager and can use it, we should focus on how entities are",
    "start": "3395133",
    "end": "3401099"
  },
  {
    "text": "actually running on a given pod. And then once we know,",
    "start": "3401100",
    "end": "3406666"
  },
  {
    "text": "okay, we can call behaviors within the entities, entities are running, we're happy with at least",
    "start": "3406666",
    "end": "3412066"
  },
  {
    "text": "that piece, then we can start arguing about what the client interface should look like and how it should work. Because we basically are",
    "start": "3412066",
    "end": "3419833"
  },
  {
    "text": "ignoring that piece right now and I'm fine with that. We can always hack it in later. It's just more... I think that the",
    "start": "3419833",
    "end": "3426166"
  },
  {
    "text": "next logical area for us to go now that we have the shard manager piece is to continue working on",
    "start": "3426166",
    "end": "3432265"
  },
  {
    "text": "the entities that are running on the pod. Because especially after Tim finishes his mailbox",
    "start": "3432266",
    "end": "3439566"
  },
  {
    "text": "implementation, I feel like we'll have a really nice base for what the in-memory mailbox should",
    "start": "3439566",
    "end": "3444733"
  },
  {
    "text": "look like. And then we can start talking about how that interacts with",
    "start": "3444733",
    "end": "3450033"
  },
  {
    "text": "entities and what you have access to when you're actually defining a behavior, etc. The mailbox",
    "start": "3450033",
    "end": "3458033"
  },
  {
    "text": "that Tim is working on right",
    "start": "3458033",
    "end": "3463099"
  },
  {
    "text": "now, I've seen, is based on something in-memory. Do we want maybe to expect that mailbox to be something like",
    "start": "3463100",
    "end": "3470833"
  },
  {
    "text": "pluggable with something making it? Or that will be something that that Vending cluster, we will",
    "start": "3471966",
    "end": "3478433"
  },
  {
    "text": "kind of extend and keep something in-memory but pull from some storage.",
    "start": "3479100",
    "end": "3485433"
  },
  {
    "text": "The question is, do you think that we're",
    "start": "3487633",
    "end": "3493199"
  },
  {
    "text": "expecting cluster to have always made boxes in memory and add",
    "start": "3493199",
    "end": "3499566"
  },
  {
    "text": "persistence as something that happened by side of writing to actual",
    "start": "3499600",
    "end": "3505265"
  },
  {
    "text": "persistence? Or instead having a mailbox approach where everything is",
    "start": "3505266",
    "end": "3513432"
  },
  {
    "text": "baked by the persistence. So the comparison is like",
    "start": "3513433",
    "end": "3519933"
  },
  {
    "text": "writing always to the storage and fetching always from the storage, whatever the storage is in",
    "start": "3520600",
    "end": "3525966"
  },
  {
    "text": "memory or an actual database implementation. Or having something in-memory and when we also add",
    "start": "3525966",
    "end": "3532366"
  },
  {
    "text": "the persistence, we in parallel persist to the persistence and",
    "start": "3532366",
    "end": "3538566"
  },
  {
    "text": "add into the memory mailbox. And when we pull, we first mark pull and",
    "start": "3538566",
    "end": "3544166"
  },
  {
    "text": "then mark on the persistence that we have pulled, etc. I'm not sure if I explained. There is no",
    "start": "3544166",
    "end": "3551166"
  },
  {
    "text": "signaling when you pull. The only signaling is when when the behavior acknowledges the",
    "start": "3551166",
    "end": "3559166"
  },
  {
    "text": "message. So the point is you want to",
    "start": "3559166",
    "end": "3565333"
  },
  {
    "text": "touch the persistence as low as possible. There's no point in backing everything in",
    "start": "3565333",
    "end": "3571566"
  },
  {
    "text": "persistence and then every every single pull from the mailbox ends up producing an SQL query. That's just like one",
    "start": "3571566",
    "end": "3580132"
  },
  {
    "text": "of the reasons why temporal is low. It's because they constantly pull the queue and there's no point for us to constantly",
    "start": "3580133",
    "end": "3586733"
  },
  {
    "text": "pull the queue. We know where an entity is. We should only pull the queue at once and",
    "start": "3586733",
    "end": "3593133"
  },
  {
    "text": "just to get to the current state, once we are in current state, we can use the queue as write",
    "start": "3593133",
    "end": "3600166"
  },
  {
    "text": "only. So it's way less intensive.",
    "start": "3600166",
    "end": "3604733"
  },
  {
    "text": "The only difference I can think of and maybe I'm wrong, but what I",
    "start": "3605366",
    "end": "3611733"
  },
  {
    "text": "was maybe discussing is that if you have something that is based off in memory before",
    "start": "3611733",
    "end": "3618932"
  },
  {
    "text": "actually putting something, putting new messages into the mailbox,",
    "start": "3618933",
    "end": "3624100"
  },
  {
    "text": "you need first to have resurrected and processed messages",
    "start": "3624699",
    "end": "3631066"
  },
  {
    "text": "into the into the memory mailbox before putting new ones. Okay.",
    "start": "3631333",
    "end": "3637533"
  },
  {
    "text": "That would be part of the pod story. Yeah, exactly. And instead with something that is",
    "start": "3637633",
    "end": "3643966"
  },
  {
    "text": "fully baked off the persistence, you can have persistence. They don't care",
    "start": "3643966",
    "end": "3650366"
  },
  {
    "text": "about that because they can maybe already write into the persistence. And if you are up to",
    "start": "3650366",
    "end": "3657566"
  },
  {
    "text": "date with pulling, then you can also perform an optimistic",
    "start": "3657566",
    "end": "3663066"
  },
  {
    "text": "path where the persistence is both in memory and persistent on, let's say, a Postgres table.",
    "start": "3663266",
    "end": "3669000"
  },
  {
    "text": "And if you are not in sync, you can on the right side already",
    "start": "3670133",
    "end": "3676166"
  },
  {
    "text": "start writing without having the queue already. You cannot go in parallel",
    "start": "3676166",
    "end": "3682566"
  },
  {
    "text": "anywhere. You can't forward to the local process and persist",
    "start": "3682566",
    "end": "3688533"
  },
  {
    "text": "on the side because the persistence might fail. I mean, persisting",
    "start": "3688633",
    "end": "3693699"
  },
  {
    "text": "and then after you persist it, you offer that message to the in-memory",
    "start": "3693699",
    "end": "3699233"
  },
  {
    "text": "implementation. Okay. Yeah, that means you have any memory implementation. Oh, yeah, yeah, yeah. But the thing is that, are we",
    "start": "3699233",
    "end": "3706265"
  },
  {
    "text": "assuming that that's in-memory implementation is something of kind of an optimization of",
    "start": "3706266",
    "end": "3711899"
  },
  {
    "text": "the storage? Well, the storage, when the text that is up on sync will",
    "start": "3711899",
    "end": "3718599"
  },
  {
    "text": "start using an in-memory object and persisting.",
    "start": "3718600",
    "end": "3723465"
  },
  {
    "text": "So it will persist and then use the memory object. Or should we assume that",
    "start": "3724199",
    "end": "3730533"
  },
  {
    "text": "they are two completely different things? The design that we had talked about a while back and",
    "start": "3730533",
    "end": "3736233"
  },
  {
    "text": "that I had started to write, I think I got pretty far with it, was when a message is received",
    "start": "3736233",
    "end": "3742733"
  },
  {
    "text": "by a pod that the pod will forward, will write that message into",
    "start": "3742733",
    "end": "3750100"
  },
  {
    "text": "storage, will persist the message. If the message is successfully persisted, the pod receives",
    "start": "3750100",
    "end": "3756333"
  },
  {
    "text": "that back as part of the query response. And then with a sequence number. And then",
    "start": "3756333",
    "end": "3761933"
  },
  {
    "text": "that gets forwarded along to the mailbox, the in-memory mailbox. And then the entity that is",
    "start": "3761933",
    "end": "3768933"
  },
  {
    "text": "running has the ability, like now has that message. And depending on what behavior the user has specified,",
    "start": "3768933",
    "end": "3775300"
  },
  {
    "text": "they have the mailbox and they'll pull that message off the mailbox. The only other time",
    "start": "3775300",
    "end": "3780699"
  },
  {
    "text": "that the entity or pod will ever interact with storage, there's only two other times. One is",
    "start": "3781333",
    "end": "3787966"
  },
  {
    "text": "during cluster, the pod initialization. When the pod initializes itself, it pulls down state from",
    "start": "3788066",
    "end": "3794599"
  },
  {
    "text": "storage to rehydrate itself, like Mike was saying. And then the other time is when the user in their",
    "start": "3794866",
    "end": "3801000"
  },
  {
    "text": "behavior actually acknowledges or processes that message. We still have to refine that API a little bit. But when the behavior acknowledges a",
    "start": "3801000",
    "end": "3809733"
  },
  {
    "text": "message is processed, that's the only other time that you're actually, it's not an explicit",
    "start": "3809733",
    "end": "3815666"
  },
  {
    "text": "write that happens by the user actually marking that message. But those are the only three places where",
    "start": "3815666",
    "end": "3822366"
  },
  {
    "text": "the storage is actually accessed. And it's all basically write only. The only time it's",
    "start": "3822366",
    "end": "3829133"
  },
  {
    "text": "read is during initialization. So basically, initialization of the entity or the pod, or the entity, I",
    "start": "3829133",
    "end": "3836233"
  },
  {
    "text": "think, yeah. Because when the entity starts, it should pull out the pod. I think the",
    "start": "3836233",
    "end": "3842366"
  },
  {
    "text": "pod has to do the first thing to rehydrate the entities that",
    "start": "3842366",
    "end": "3849466"
  },
  {
    "text": "still have messages to be processed. So that's the first read. The second read happens when anytime you",
    "start": "3849466",
    "end": "3856633"
  },
  {
    "text": "receive a message and you do the persistence, you get back a sequence number. Then you have to check",
    "start": "3856633",
    "end": "3863833"
  },
  {
    "text": "what's the last sequence number that was placed in the",
    "start": "3863833",
    "end": "3869833"
  },
  {
    "text": "memory main pods? If that sequence number is your current sequence",
    "start": "3869899",
    "end": "3875733"
  },
  {
    "text": "number minus one, you can just forward them. That's it. If there's a gap, then you have to",
    "start": "3875733",
    "end": "3882333"
  },
  {
    "text": "do a pull to kind of... To reveal the memory queue. That was my",
    "start": "3882333",
    "end": "3890333"
  },
  {
    "text": "question. We are basically assuming that that kind of thing is performed by the",
    "start": "3890333",
    "end": "3896000"
  },
  {
    "text": "cluster mailbox. It's not something that is aliased away by the effect mailbox. It's",
    "start": "3896000",
    "end": "3906033"
  },
  {
    "text": "the implementation of the storage of the effect mailbox. I assume that when we're doing that check,",
    "start": "3906033",
    "end": "3912033"
  },
  {
    "text": "essentially, we get a message, we write it to storage, we get back a sequence number. We attempt to write that message into the",
    "start": "3913033",
    "end": "3919866"
  },
  {
    "text": "mailbox by checking if the sequence number is the next sequence number. If it's not, then we need to rehydrate that",
    "start": "3919866",
    "end": "3924932"
  },
  {
    "text": "mailbox again. When we do that check, we need to lock the mailbox, essentially, somehow. We need",
    "start": "3924933",
    "end": "3931300"
  },
  {
    "text": "to prevent the behavior from reading. Exactly. That was why I was asking if that kind...",
    "start": "3931300",
    "end": "3937432"
  },
  {
    "text": "Given that kind of behavior could be tricky to, let's see, get right",
    "start": "3937933",
    "end": "3945399"
  },
  {
    "text": "by staying one level up from the mailbox implementation, I would think that maybe",
    "start": "3945399",
    "end": "3950666"
  },
  {
    "text": "that could be something that could be inside the persistence used by the mailbox. That",
    "start": "3950666",
    "end": "3956533"
  },
  {
    "text": "could be in memory, it could be storage, it could be whatever. And then as a cluster user, we just get",
    "start": "3956533",
    "end": "3965600"
  },
  {
    "text": "that because we know that the effect mailbox is baked by providing basically two methods,",
    "start": "3965600",
    "end": "3972166"
  },
  {
    "text": "the write and the get starting from a cursor. If that's in memory... I don't think the",
    "start": "3972933",
    "end": "3979133"
  },
  {
    "text": "effect cluster ever exposes the right part. The right part is an internal detail. Yeah, it's an internal",
    "start": "3979133",
    "end": "3984932"
  },
  {
    "text": "detail. Yeah, exactly. It's something that is performed while you offer. While you offer, you have",
    "start": "3984933",
    "end": "3991466"
  },
  {
    "text": "somewhere to persist. On the other side, let's think about it for a second. If the",
    "start": "3991466",
    "end": "3998766"
  },
  {
    "text": "first thing that happens when I receive a message is persistence and I",
    "start": "3998766",
    "end": "4004333"
  },
  {
    "text": "get back the message with a sequence number, what would be the case where that message",
    "start": "4004333",
    "end": "4012833"
  },
  {
    "text": "might... Well, where the entity is not already rehydrated?",
    "start": "4012833",
    "end": "4020433"
  },
  {
    "text": "I always assume... Let me finish for a second. Because if when we",
    "start": "4021300",
    "end": "4027432"
  },
  {
    "text": "start, when we allocate a shard, we lock the receival of messages, which",
    "start": "4027433",
    "end": "4033833"
  },
  {
    "text": "means we locally enqueue that we have a number of messages to forward, but the allocation",
    "start": "4033833",
    "end": "4042333"
  },
  {
    "text": "process rehydrates all the entities that had pending messages. And only after that process is",
    "start": "4042333",
    "end": "4050566"
  },
  {
    "text": "finished, we actually do the persistence of new messages. I don't think there is any case where the",
    "start": "4050566",
    "end": "4058733"
  },
  {
    "text": "entity would not be up to date, because if the entity had no messages, then the new message has",
    "start": "4058733",
    "end": "4065899"
  },
  {
    "text": "to be directly forwarded. If the entity had messages to be processed, those messages have been",
    "start": "4065899",
    "end": "4071433"
  },
  {
    "text": "rehydrated when the shard is allocated. So I think the only",
    "start": "4071433",
    "end": "4077266"
  },
  {
    "text": "gap is persistence can only occur when",
    "start": "4077266",
    "end": "4083766"
  },
  {
    "text": "the shard is already allocated to the pod. It cannot occur",
    "start": "4083766",
    "end": "4088666"
  },
  {
    "text": "before the... If we avoid this race",
    "start": "4089199",
    "end": "4094966"
  },
  {
    "text": "condition, then we don't even need the check, which to be honest might be a better choice, because the",
    "start": "4094966",
    "end": "4103799"
  },
  {
    "text": "allocation of shards, like if the shard is not yet allocated, the entity is not going to be ready to",
    "start": "4103800",
    "end": "4109233"
  },
  {
    "text": "process the messages. Even more than the shard allocated is actually the entity's",
    "start": "4109233",
    "end": "4115833"
  },
  {
    "text": "spawn up to be more precise, because if you have to also persist in memory queue, that way",
    "start": "4115833",
    "end": "4123666"
  },
  {
    "text": "you have to have the actual in memory queue. What I always assumed,",
    "start": "4123666",
    "end": "4131233"
  },
  {
    "text": "and maybe my assumption was wrong, is that if we don't work on that assumption, we can also say that",
    "start": "4131466",
    "end": "4140566"
  },
  {
    "text": "the mailbox storage, that is what makes the mailbox, can actually persist messages without",
    "start": "4140566",
    "end": "4148333"
  },
  {
    "text": "having the actual mailbox instance on hand. If it has the",
    "start": "4148333",
    "end": "4153566"
  },
  {
    "text": "mailbox instance on hand. It will be immediate. Exactly. It won't wait for the entity to be",
    "start": "4153566",
    "end": "4159133"
  },
  {
    "text": "spawn up and alive. But in order to do something like this, you",
    "start": "4159166",
    "end": "4164532"
  },
  {
    "text": "either have to have a process when you start the entity and create the",
    "start": "4164533",
    "end": "4171366"
  },
  {
    "text": "entity that ensures that before putting new messages inside the memory queue, the memory queue",
    "start": "4171366",
    "end": "4177266"
  },
  {
    "text": "should be up on point where the last insert was, or you",
    "start": "4177266",
    "end": "4182600"
  },
  {
    "text": "demand that aspect to the actual mailbox itself. Without thinking of",
    "start": "4182633",
    "end": "4191000"
  },
  {
    "text": "cluster or whatever, you can be something called mailbox that basically has an offer method that puts",
    "start": "4191000",
    "end": "4197100"
  },
  {
    "text": "something on a queue, and a take method that takes something from a queue. You're just",
    "start": "4197100",
    "end": "4203833"
  },
  {
    "text": "moving where the synchronization happens. I'm moving that away from the",
    "start": "4203833",
    "end": "4211899"
  },
  {
    "text": "actual mailbox instance and revert to a persistence. We had this discussion in this world,",
    "start": "4211899",
    "end": "4217833"
  },
  {
    "text": "when you went through the details, you realized that you were just moving one problem from one side to another. It doesn't change",
    "start": "4218633",
    "end": "4225000"
  },
  {
    "text": "anything in this discussion. The point is either, do you persist as soon as you receive the message, or",
    "start": "4225000",
    "end": "4232333"
  },
  {
    "text": "can we afford to wait for the cluster to for the shard to be synced? Because if the shard",
    "start": "4232333",
    "end": "4239466"
  },
  {
    "text": "is in sync, all the entities within the shard are in sync, and then it's an easy path. We don't need",
    "start": "4239466",
    "end": "4246500"
  },
  {
    "text": "any kind of synchronization. On the other side, if we want to persist on the fly, then we",
    "start": "4246500",
    "end": "4254532"
  },
  {
    "text": "might have race conditions to manage. Assuming that the shard allocations don't",
    "start": "4254533",
    "end": "4260833"
  },
  {
    "text": "happen every second, I think it would be a safe assumption that the majority of the",
    "start": "4260833",
    "end": "4267199"
  },
  {
    "text": "time we have shards ready. Yeah. The only risk is we might lose",
    "start": "4267199",
    "end": "4272899"
  },
  {
    "text": "a message, but if we lose a message, it's basically on a crush.",
    "start": "4273266",
    "end": "4278966"
  },
  {
    "text": "If the process crushes, but if it crushed a millisecond before,",
    "start": "4279966",
    "end": "4286865"
  },
  {
    "text": "we would have lost the message anyway, because it would have not been received, and everything is idempotent, so the",
    "start": "4286866",
    "end": "4292733"
  },
  {
    "text": "sender can easily retry. The other moment when we could lose a",
    "start": "4292733",
    "end": "4298666"
  },
  {
    "text": "message, but that could be more a timeout than actually losing a message, is in the situation",
    "start": "4298666",
    "end": "4304566"
  },
  {
    "text": "when you, for some reason, a large amount of entities on the",
    "start": "4304566",
    "end": "4309800"
  },
  {
    "text": "same shard spawn up, and the actual hydration of the entity takes more than the right side",
    "start": "4309833",
    "end": "4317399"
  },
  {
    "text": "accepting messages, so that way you should lock them until you have the",
    "start": "4317500",
    "end": "4324033"
  },
  {
    "text": "entity, as we discussed, and then you would take more for the actual entity to receive the message.",
    "start": "4324033",
    "end": "4329633"
  },
  {
    "text": "Let's say that you have a system that is completely empty, and for some reason, all the",
    "start": "4329833",
    "end": "4336198"
  },
  {
    "text": "entities that are receiving messages are on one shard, because you are basically bloating a lot of",
    "start": "4336199",
    "end": "4342765"
  },
  {
    "text": "messages. That could be also a situation when the amount of time that takes for the entities to be",
    "start": "4342766",
    "end": "4349666"
  },
  {
    "text": "spawned up is greater, because you are basically sending a lot of messages that are targeted on a specific",
    "start": "4349666",
    "end": "4356399"
  },
  {
    "text": "shardID. But those are the only two situations that are pretty",
    "start": "4356399",
    "end": "4361532"
  },
  {
    "text": "rare. I mean, the second situation just means you",
    "start": "4361533",
    "end": "4367966"
  },
  {
    "text": "have to increase the number of pods, because you have too many shards on the same pod, it takes too much, and it got to the point of the",
    "start": "4368000",
    "end": "4376600"
  },
  {
    "text": "cluster being non-operational. Keep in mind that the time,",
    "start": "4376600",
    "end": "4384300"
  },
  {
    "text": "you only have to fetch non-processed messages. So what it means",
    "start": "4385566",
    "end": "4391166"
  },
  {
    "text": "if you get to that situation, that you include so many messages before",
    "start": "4391166",
    "end": "4396399"
  },
  {
    "text": "processing them, and crushed before, which is a",
    "start": "4396399",
    "end": "4401933"
  },
  {
    "text": "pretty odd circumstance. You're basically putting yourself in the situation that you're about to",
    "start": "4402733",
    "end": "4409032"
  },
  {
    "text": "re-shard the system, because you have put yourself in the situation when",
    "start": "4409100",
    "end": "4414866"
  },
  {
    "text": "you don't have enough shards, and you need more shards to try to",
    "start": "4414866",
    "end": "4419899"
  },
  {
    "text": "rebalance more the system. I think to an extent, these links to",
    "start": "4419899",
    "end": "4425198"
  },
  {
    "text": "another point that I wanted to make, because Maxwell mentioned the client protocol,",
    "start": "4425333",
    "end": "4432733"
  },
  {
    "text": "and I've been thinking about it for a while. I don't think we can",
    "start": "4433533",
    "end": "4442500"
  },
  {
    "text": "afford to have at most ones in the interface, because I cannot find",
    "start": "4442500",
    "end": "4451933"
  },
  {
    "text": "any way where the client can know if a message is just",
    "start": "4451933",
    "end": "4458233"
  },
  {
    "text": "lost, non-processed, or it's waiting for",
    "start": "4458233",
    "end": "4464466"
  },
  {
    "text": "a response. I think we should assume that all the messages are at least once, and idempotent, and",
    "start": "4464466",
    "end": "4471966"
  },
  {
    "text": "anytime you do the send, you wait for the response. If we want to do it at most once, then",
    "start": "4471966",
    "end": "4479233"
  },
  {
    "text": "it's quite literally the client has to, the behavior has to duplicate the",
    "start": "4479233",
    "end": "4485300"
  },
  {
    "text": "messages, basically, has to handle idempotency. And I think that's the only reasonable",
    "start": "4485300",
    "end": "4492500"
  },
  {
    "text": "choice where we don't make too many assumptions,",
    "start": "4492500",
    "end": "4497566"
  },
  {
    "text": "on the client. It's the one we basically always started with, saying",
    "start": "4497966",
    "end": "4504533"
  },
  {
    "text": "that we enforce the client to send the message until it gets a",
    "start": "4504533",
    "end": "4511166"
  },
  {
    "text": "response, that it may not be the actual response of the message being processed, but just saying,",
    "start": "4511166",
    "end": "4516300"
  },
  {
    "text": "\"Okay, I received.\" Yeah, but that's a tell. For that interface, you can do a tell and just not",
    "start": "4516300",
    "end": "4523100"
  },
  {
    "text": "wait for the receiving, that's fine. But the issue is, if you do an ask, you're then waiting for",
    "start": "4523100",
    "end": "4532000"
  },
  {
    "text": "the response. But what happens if the behavior got the message, acknowledged the",
    "start": "4532000",
    "end": "4538366"
  },
  {
    "text": "message, and then crushed? That message will never be processed, you will never have the result. But how",
    "start": "4538366",
    "end": "4546699"
  },
  {
    "text": "do you know? The message could take a minute, the message could take five minutes to process. Where do you put",
    "start": "4546699",
    "end": "4553899"
  },
  {
    "text": "the line on saying, \"Okay, the message is no longer processed.\" And if it's not, what do you",
    "start": "4553899",
    "end": "4559300"
  },
  {
    "text": "return? Do you return an error? In some respects, I think that was expected. The only thing",
    "start": "4559300",
    "end": "4564433"
  },
  {
    "text": "that you can do is just let the client decide how much he would",
    "start": "4564433",
    "end": "4570432"
  },
  {
    "text": "wait to get response. Maybe the client would do something like pull and the client will decide, \"I",
    "start": "4570433",
    "end": "4579433"
  },
  {
    "text": "will pull for, let's say, 10 minutes.\" And after that, \"Okay,\" the client will say, \"Not",
    "start": "4579433",
    "end": "4585000"
  },
  {
    "text": "processed.\" But again, it's at most once. The client wouldn't",
    "start": "4585000",
    "end": "4591233"
  },
  {
    "text": "know if actually got processed or not. I think in some respects, that also makes it easier for",
    "start": "4591233",
    "end": "4597033"
  },
  {
    "text": "us to design a client protocol. That makes sense with the way that we're planning on doing behaviors as long-running fibers that",
    "start": "4597033",
    "end": "4604233"
  },
  {
    "text": "never end. I think it makes it easier for us to actually design a client protocol that makes sense if we plan on at least",
    "start": "4604233",
    "end": "4610299"
  },
  {
    "text": "once processing and just make the only way. Yeah, I just think we can't",
    "start": "4610299",
    "end": "4619133"
  },
  {
    "text": "cover all the cases with also including at most once.",
    "start": "4619133",
    "end": "4625866"
  },
  {
    "text": "And those are things that if they go wrong, then you just have a client",
    "start": "4626533",
    "end": "4632500"
  },
  {
    "text": "waiting for eternity and the cluster could... If you think one",
    "start": "4632500",
    "end": "4638233"
  },
  {
    "text": "entity, pinging another entity, if that message then gets stuck, you have a whole chain",
    "start": "4638233",
    "end": "4646233"
  },
  {
    "text": "cascading. It quite literally brings down the whole thing.",
    "start": "4646233",
    "end": "4651399"
  },
  {
    "text": "If the assumption is every message has to be processed and an answer has to be given,",
    "start": "4652133",
    "end": "4659000"
  },
  {
    "text": "even if it's like the answer could take two years, an entity",
    "start": "4659933",
    "end": "4665200"
  },
  {
    "text": "could decide that a message takes two years to be processed. That's completely fine and not",
    "start": "4665200",
    "end": "4671299"
  },
  {
    "text": "acknowledge and not process it. But then the client knows, \"Okay, I'm having to wait two years to get there.\"",
    "start": "4671299",
    "end": "4678466"
  },
  {
    "text": "Because that's expected time to that... That's the expected thing. You always have",
    "start": "4678766",
    "end": "4684000"
  },
  {
    "text": "an expected response. So you basically don't assume a timeout and",
    "start": "4684066",
    "end": "4691533"
  },
  {
    "text": "even if the whole thing dies and it restarts again, that's fine. It's",
    "start": "4691533",
    "end": "4696899"
  },
  {
    "text": "gonna take two years. Now, obviously, that's not the default condition. Probably all the",
    "start": "4696899",
    "end": "4703399"
  },
  {
    "text": "messages will be processed quite fast. That could be one situation.",
    "start": "4703399",
    "end": "4708166"
  },
  {
    "text": "My point in my thinking was there is no way we can enforce a timeout",
    "start": "4710633",
    "end": "4716566"
  },
  {
    "text": "while also allowing for long running processes.",
    "start": "4716566",
    "end": "4720700"
  },
  {
    "text": "Because they kind of go and intend. And this, by the way, it's something that other workflow",
    "start": "4721933",
    "end": "4728066"
  },
  {
    "text": "engines have, which is like there are some workflow engines. I'm doing a comparison between",
    "start": "4728133",
    "end": "4733966"
  },
  {
    "text": "different types of workflow engines. There are some workflow engines where you have a server",
    "start": "4734533",
    "end": "4739599"
  },
  {
    "text": "that calls you. Temporal has a similar",
    "start": "4739666",
    "end": "4745366"
  },
  {
    "text": "design to us to an extent. They have workers and the workers constantly pull a queue, which is what",
    "start": "4745366",
    "end": "4752166"
  },
  {
    "text": "makes it slow, by the way. By clustering, we don't have that problem. For us, the queue is",
    "start": "4752166",
    "end": "4758733"
  },
  {
    "text": "mostly a write-only scenario, but they can handle long",
    "start": "4758733",
    "end": "4764066"
  },
  {
    "text": "running processes because it's the worker that knows when it's done. Things like ingest or restate,",
    "start": "4764066",
    "end": "4771032"
  },
  {
    "text": "they have a server that calls you via HTTP. So you can deploy your workflows",
    "start": "4771466",
    "end": "4778199"
  },
  {
    "text": "like serverless functions and so on and so forth. There's no way that those",
    "start": "4779100",
    "end": "4784865"
  },
  {
    "text": "workflow engines can support long running workflows because they would have to initiate",
    "start": "4784866",
    "end": "4790266"
  },
  {
    "text": "an HTTP call and that could timeout. There's a configuration that",
    "start": "4790266",
    "end": "4797933"
  },
  {
    "text": "says, okay, this is the maximum amount of time that I'm allowing",
    "start": "4797933",
    "end": "4803266"
  },
  {
    "text": "a workflow to do something to live for. They need that because they",
    "start": "4803366",
    "end": "4810666"
  },
  {
    "text": "call you and they basically have to decide at some point, is the",
    "start": "4810666",
    "end": "4816000"
  },
  {
    "text": "serverless thing dead? Am I going to keep waiting or am I going to just stop? Or is it normal that",
    "start": "4816000",
    "end": "4823633"
  },
  {
    "text": "it's an undecidable problem? You cannot assume the timing of anything.",
    "start": "4823633",
    "end": "4831432"
  },
  {
    "text": "It can be one millisecond, it can be 200 years, you don't know. That's exactly the point.",
    "start": "4832066",
    "end": "4838266"
  },
  {
    "text": "Okay, sorry guys. I'm sorry for interrupting.",
    "start": "4839566",
    "end": "4844133"
  },
  {
    "text": "Thank you for having joined. Bye bye. See you soon. Sorry for derailing the",
    "start": "4844633",
    "end": "4852200"
  },
  {
    "text": "conversation a little bit. I think that was a good conversation. I'm",
    "start": "4852200",
    "end": "4858566"
  },
  {
    "text": "just like no pun intended in jesting the whole",
    "start": "4858566",
    "end": "4863899"
  },
  {
    "text": "at least once. I think it makes a lot of sense to make that what the",
    "start": "4865500",
    "end": "4870899"
  },
  {
    "text": "supported messaging protocol is going to be. At least once, always in Dempaton, you can be guaranteed that your message is going",
    "start": "4870899",
    "end": "4876865"
  },
  {
    "text": "to get a response, whether you're asking or telling. If you're asking, you're going to get",
    "start": "4876866",
    "end": "4882566"
  },
  {
    "text": "the response that you expect unless the thing dies and you'll be notified. I think that that makes a lot more sense. If you're",
    "start": "4882633",
    "end": "4888700"
  },
  {
    "text": "telling, you just get the response back. Well, not the response, but when the message is processed. When",
    "start": "4888700",
    "end": "4896266"
  },
  {
    "text": "the message is persisted. Yes, because tell doesn't wait.",
    "start": "4896266",
    "end": "4901700"
  },
  {
    "text": "I'm telling you this, as soon as I'm sure that you will get the",
    "start": "4904133",
    "end": "4909466"
  },
  {
    "text": "message, I'm done, I'm fine. Even the signature of a tell doesn't",
    "start": "4909466",
    "end": "4915399"
  },
  {
    "text": "contain the response. It contains a void basically. Gone fine or rather maybe I don't know",
    "start": "4915399",
    "end": "4926333"
  },
  {
    "text": "what it could contain. Actually avoid because given that we have idempotency IDs, you",
    "start": "4926333",
    "end": "4933366"
  },
  {
    "text": "can always pull from the state, pull the state of a message",
    "start": "4933366",
    "end": "4937766"
  },
  {
    "text": "by its identifier. We have basically",
    "start": "4938766",
    "end": "4943866"
  },
  {
    "text": "three methods. We have ask, tell, and pull. Ask waits",
    "start": "4943866",
    "end": "4951433"
  },
  {
    "text": "for a response. Tell doesn't wait for the response, waits for the delivery, and pull retrieves the",
    "start": "4951500",
    "end": "4959633"
  },
  {
    "text": "current state of the message. It tells you it has been processed.",
    "start": "4959633",
    "end": "4964366"
  },
  {
    "text": "It's not yet been processed. Or basically just that. Or it doesn't",
    "start": "4964933",
    "end": "4970733"
  },
  {
    "text": "exist. I've never received this message whatsoever.",
    "start": "4970733",
    "end": "4975000"
  },
  {
    "text": "One is a read-only thing. The other is a write-only thing. The ask",
    "start": "4975866",
    "end": "4982000"
  },
  {
    "text": "instead is a read-write. So write then reads. I think this",
    "start": "4982000",
    "end": "4987333"
  },
  {
    "text": "simplifies the thing. I also think it makes it, I mean, personally, I think it makes it more robust for the type of",
    "start": "4987333",
    "end": "4992765"
  },
  {
    "text": "system that we're after. Like, again, the points you made about",
    "start": "4992766",
    "end": "4997799"
  },
  {
    "text": "we're looking to support long-running workflows as well. Like, I think it just makes more sense",
    "start": "4997799",
    "end": "5003633"
  },
  {
    "text": "for the type of system we're trying to build. Yeah, I mean, if we think about it,",
    "start": "5003633",
    "end": "5009166"
  },
  {
    "text": "really the reason we are even building cluster is because we want to support any type of",
    "start": "5011066",
    "end": "5021000"
  },
  {
    "text": "durable computation to occur. We want to support",
    "start": "5021000",
    "end": "5025733"
  },
  {
    "text": "plain workflows, a-la-temporal, short-lived transactions. We want to",
    "start": "5026233",
    "end": "5032265"
  },
  {
    "text": "support state machines that may span for years and they may",
    "start": "5032266",
    "end": "5038600"
  },
  {
    "text": "never end. You have a state machine, you query the state machine, you interact with the state",
    "start": "5038600",
    "end": "5044966"
  },
  {
    "text": "machines. They're just actors, you know? This type of",
    "start": "5044966",
    "end": "5050966"
  },
  {
    "text": "flexibility is really the differentiator between our solution versus",
    "start": "5050966",
    "end": "5057733"
  },
  {
    "text": "the other types of solutions. Other types of solution prescribe a",
    "start": "5057733",
    "end": "5063766"
  },
  {
    "text": "single type of workflow to occur. Again, ingest, restate.",
    "start": "5063766",
    "end": "5068433"
  },
  {
    "text": "They're fantastic. They're very easy to use. They are very nice.",
    "start": "5069266",
    "end": "5074033"
  },
  {
    "text": "They already have commercial products to some extent. It's much easier",
    "start": "5074566",
    "end": "5079766"
  },
  {
    "text": "if you just develop a server that calls, HTTP calls, and your workflows are just",
    "start": "5079766",
    "end": "5085233"
  },
  {
    "text": "repeated HTTP calls. You don't have to coordinate clusters or stuff like that.",
    "start": "5085233",
    "end": "5090966"
  },
  {
    "text": "Or you have, on the other side, stuff like Golem, where your runtime is different. They",
    "start": "5092033",
    "end": "5100633"
  },
  {
    "text": "have a custom runtime and everything is serialized.",
    "start": "5100633",
    "end": "5105566"
  },
  {
    "text": "There's on the side persistence. That's also very prescriptive",
    "start": "5105733",
    "end": "5112633"
  },
  {
    "text": "because can you do a state machine in that? Sure. You basically implement a",
    "start": "5112633",
    "end": "5118666"
  },
  {
    "text": "local reducer and you consider the function invocations like updating the state. But what happens",
    "start": "5118666",
    "end": "5128233"
  },
  {
    "text": "when you want to update the code? You have to make a new code and you have to replay everything from the beginning, which is",
    "start": "5128233",
    "end": "5135033"
  },
  {
    "text": "really not what you want in a state machine behavior. Because if that takes, I don't know, 100",
    "start": "5135033",
    "end": "5141533"
  },
  {
    "text": "messages per minute for a year, it's going to take a while to replay the",
    "start": "5141533",
    "end": "5147299"
  },
  {
    "text": "whole history if you want to do a bug fix in an action. And for those types of workflows,",
    "start": "5147299",
    "end": "5154966"
  },
  {
    "text": "explicit state machines are just better because you have your state. You can update the state with a",
    "start": "5155633",
    "end": "5163166"
  },
  {
    "text": "normal database transaction and you can update the",
    "start": "5163166",
    "end": "5169033"
  },
  {
    "text": "actions just by changing the code. But updating an action doesn't mean you want to replay from the",
    "start": "5169033",
    "end": "5174366"
  },
  {
    "text": "beginning. Let's say, for example, I don't know, you have an action that ends up calling Stripe",
    "start": "5174366",
    "end": "5180666"
  },
  {
    "text": "and Stripe changes in API. You just want to change the API code. You don't want to",
    "start": "5181233",
    "end": "5186933"
  },
  {
    "text": "replay all the calls from the history of time just because you",
    "start": "5187033",
    "end": "5192265"
  },
  {
    "text": "had to update the code. So there's pro and cons of every solution,",
    "start": "5192266",
    "end": "5198199"
  },
  {
    "text": "I think ours is kind of the most complete. What it assumes on the other hand is you",
    "start": "5198200",
    "end": "5206500"
  },
  {
    "text": "have at least one or two long-running processes that",
    "start": "5206500",
    "end": "5211766"
  },
  {
    "text": "basically you can't host that in a serverless environment at the 100%. You need at least a few",
    "start": "5212633",
    "end": "5219032"
  },
  {
    "text": "coordinators. If you have the few coordinators and you want to deploy everything in a serverless",
    "start": "5219033",
    "end": "5225166"
  },
  {
    "text": "environment, if you think about it, you could create one actor that",
    "start": "5225333",
    "end": "5230600"
  },
  {
    "text": "just calls an HTTP API and then you can host the workflow in the HTTP endpoint and you",
    "start": "5230600",
    "end": "5237500"
  },
  {
    "text": "basically have a restate-like solution. Or you can host your workers by",
    "start": "5237500",
    "end": "5243765"
  },
  {
    "text": "yourself with persistent processes and then you have a temporal-like solution that can also do",
    "start": "5243766",
    "end": "5250766"
  },
  {
    "text": "state machines and other stuff. So I think our objective is really to",
    "start": "5250766",
    "end": "5256399"
  },
  {
    "text": "cover the workflow, the workflow execution element, or rather not even workflow, the durable",
    "start": "5256399",
    "end": "5262399"
  },
  {
    "text": "computing element. That's the key difference",
    "start": "5262399",
    "end": "5267566"
  },
  {
    "text": "between effect cluster and shardcake. Shardcake was just sharding data",
    "start": "5267566",
    "end": "5273833"
  },
  {
    "text": "processing across nodes. Our use case is different. For durability, you really want",
    "start": "5273833",
    "end": "5280266"
  },
  {
    "text": "everything to be idempotent, everything. The golden path should be that. Maybe in a year time,",
    "start": "5280266",
    "end": "5287466"
  },
  {
    "text": "we find a different solution for at most once and we decide to re-implement it, but at least at the",
    "start": "5287466",
    "end": "5292933"
  },
  {
    "text": "beginning, we should put users in the golden path. And idempotency, repeated transactions are",
    "start": "5292933",
    "end": "5298899"
  },
  {
    "text": "really the golden path here. I agree. I agree. It also makes my job easier since I have to work",
    "start": "5298899",
    "end": "5304633"
  },
  {
    "text": "on it. So I'm happy with that. Good. Cool. Well, hopefully next time we",
    "start": "5304633",
    "end": "5311799"
  },
  {
    "text": "won't be working on test clock the whole time. Yes. But yeah. Now we're reactively start",
    "start": "5311799",
    "end": "5318766"
  },
  {
    "text": "hacking into cluster. Yeah, exactly. You have to work a little bit on the beautiful CLI, which",
    "start": "5318766",
    "end": "5326066"
  },
  {
    "text": "will be announced soon. Yeah, users will get access to that soon. Tm. Soon. Cool. All right.",
    "start": "5326066",
    "end": "5334966"
  },
  {
    "text": "Good. Well, thank you very much, Mike. Thanks to you. Speak soon. Cheers. Bye folks.",
    "start": "5334966",
    "end": "5340966"
  }
]