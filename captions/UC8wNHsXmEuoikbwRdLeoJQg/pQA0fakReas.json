[
  {
    "start": "0",
    "end": "134000"
  },
  {
    "text": "(audience applauds) Thank you very much, Johannes, for that introduction.",
    "start": "0",
    "end": "5375"
  },
  {
    "text": "Thank you, everybody, for coming today to Effect Days. I'm super excited to be here with you all this morning.",
    "start": "6333",
    "end": "12250"
  },
  {
    "text": "I wanna give a quick shout out again, like Johannes did, to all the organizers of this conference.",
    "start": "13000",
    "end": "17583"
  },
  {
    "text": "I think that, I don't know if you all know, but the whole reason I joined the company",
    "start": "18666",
    "end": "24125"
  },
  {
    "text": "was after Effect Days last year. I had crazy FOMO, and I just, I couldn't,",
    "start": "24125",
    "end": "29291"
  },
  {
    "text": "I couldn't keep going in life without, like, becoming a part of the company, and it's been a crazy journey over the last year,",
    "start": "29791",
    "end": "36125"
  },
  {
    "text": "but I wouldn't go back, I wouldn't change a thing. So thank you one more time to the conference organizers, so for always making this event amazing,",
    "start": "36125",
    "end": "43041"
  },
  {
    "text": "and hopefully continuing to do it every year. (audience applauds)",
    "start": "43041",
    "end": "49416"
  },
  {
    "text": "So today, I'm gonna be talking with you all about building effective agents, pun absolutely intended.",
    "start": "52041",
    "end": "57500"
  },
  {
    "text": "We're gonna be exploring how we can use Effect and its AI integration packages to enable building complex agentic systems,",
    "start": "58583",
    "end": "65083"
  },
  {
    "text": "but we're specifically gonna focus on taking a practical approach with the AI packages, looking at how we can actually use these software packages",
    "start": "65541",
    "end": "73250"
  },
  {
    "text": "to develop the simple composable building blocks we need in order to architect these more complex systems.",
    "start": "73500",
    "end": "79375"
  },
  {
    "text": "So, I don't even know if I need to introduce myself anymore, because clearly I'm the new mustachioed tech influencer",
    "start": "80500",
    "end": "89333"
  },
  {
    "text": "and YouTuber from Effectful, but for those of you who don't know me,",
    "start": "89500",
    "end": "94208"
  },
  {
    "text": "my name is Max, I'm one of the founding engineers at Effectful Technologies, which is the company driving the open source development",
    "start": "95083",
    "end": "101500"
  },
  {
    "text": "behind Effect and all its ecosystem packages. I'm based out of New Jersey in the States,",
    "start": "101625",
    "end": "106958"
  },
  {
    "text": "and you can usually find me hacking on something related to Effect, but I also really love anything",
    "start": "107666",
    "end": "112041"
  },
  {
    "text": "related to DevOps or infrastructure or systems administration, so if any of that speaks to you,",
    "start": "113166",
    "end": "118291"
  },
  {
    "text": "please come find me during the break, any of the breaks or at the end of the conference, and I'd love to chat.",
    "start": "118291",
    "end": "122583"
  },
  {
    "text": "So, I wanna talk today specifically about why we and many others are betting on Effect",
    "start": "124083",
    "end": "131083"
  },
  {
    "text": "and its AI integrations for building complex agentic systems.",
    "start": "131083",
    "end": "135875"
  },
  {
    "start": "134000",
    "end": "205000"
  },
  {
    "text": "I also wanna explore a little bit about what makes building these systems so complex in the first place,",
    "start": "136791",
    "end": "142916"
  },
  {
    "text": "and maybe talk a little bit about some strategies we can use to combat that complexity.",
    "start": "143166",
    "end": "147333"
  },
  {
    "text": "We should also probably talk about what we even mean by the term agentic system. These terms are thrown around a lot",
    "start": "149041",
    "end": "154833"
  },
  {
    "text": "and don't ever really get defined, so I'll talk about at least the definition I'll use during this talk,",
    "start": "154833",
    "end": "159916"
  },
  {
    "text": "but we're gonna spend the vast majority of the talk focusing on how we can use Effect",
    "start": "160916",
    "end": "166333"
  },
  {
    "text": "and its AI integration packages to build out these complex systems. A couple of disclaimers,",
    "start": "166666",
    "end": "172791"
  },
  {
    "text": "this talk is definitely geared towards folks with at least a basic understanding of Effect and how to use it.",
    "start": "173500",
    "end": "178415"
  },
  {
    "text": "We're gonna be looking at a lot of code samples during this talk, and there may or may not be,",
    "start": "179541",
    "end": "185333"
  },
  {
    "text": "there definitely will be, at least a few dad jokes. All right, so let's get into it.",
    "start": "185333",
    "end": "191000"
  },
  {
    "text": "So, let's start off by discussing why you should even consider utilizing Effect",
    "start": "192375",
    "end": "197625"
  },
  {
    "text": "for building applications that interact with large language models or LLMs. What does Effect bring to the table?",
    "start": "198125",
    "end": "203500"
  },
  {
    "text": "Why should we use it when there are so many other tools available? And I think probably one of the good arguments",
    "start": "204375",
    "end": "210750"
  },
  {
    "start": "205000",
    "end": "300000"
  },
  {
    "text": "that we can start with is, at the core of any of these systems,",
    "start": "210750",
    "end": "214958"
  },
  {
    "text": "they all have the same requirements as building any other production grade system, right?",
    "start": "216041",
    "end": "220458"
  },
  {
    "text": "Regardless of whether we're using AI or not, we still have the same requirements. And developing any production grade system is hard.",
    "start": "221500",
    "end": "229458"
  },
  {
    "text": "It's a hard thing to do. We as developers have to face a mountain of complexity that we have to overcome in order to get these systems",
    "start": "229708",
    "end": "236583"
  },
  {
    "text": "deployed into production, especially when using TypeScript. We need to make sure these systems are fault tolerant,",
    "start": "236833",
    "end": "242500"
  },
  {
    "text": "that we can reason about how they're gonna behave when faced with the errors that we're expecting,",
    "start": "242750",
    "end": "248333"
  },
  {
    "text": "but also that the system continues to function as expected in the face of unexpected errors.",
    "start": "248333",
    "end": "253083"
  },
  {
    "text": "We need to make sure these systems are testable, that we have the ability to reason about",
    "start": "253625",
    "end": "258625"
  },
  {
    "text": "how the system is going to behave and also have confidence that our code is gonna work as expected once deployed.",
    "start": "259125",
    "end": "265625"
  },
  {
    "text": "We need to make sure these systems are scalable, and I don't just mean scalable in terms of utilization, right, that's usually what we think about",
    "start": "266500",
    "end": "272500"
  },
  {
    "text": "when we think about scalability, but I also think the systems should be scalable in terms of the code base.",
    "start": "272500",
    "end": "277375"
  },
  {
    "text": "We should keep them maintainable, we should keep them easy to work with for developers, and onboarding new developers shouldn't be an arduous task.",
    "start": "277833",
    "end": "285166"
  },
  {
    "text": "And finally, these applications should be introspectable. We should have really good insight into the running application,",
    "start": "286041",
    "end": "291625"
  },
  {
    "text": "so that we can be alerted if something's going wrong, right? We wanna know if something's going wrong in our production systems.",
    "start": "292041",
    "end": "297457"
  },
  {
    "text": "And I think this goes with pretty much any system, as I said before, but the complexity is compounded",
    "start": "299458",
    "end": "306833"
  },
  {
    "start": "300000",
    "end": "378000"
  },
  {
    "text": "in applications that are powered by large language model interactions, and this is for one main reason.",
    "start": "307375",
    "end": "312957"
  },
  {
    "text": "Systems that are powered by LLM interactions are inherently non-deterministic. So what does that mean?",
    "start": "314750",
    "end": "319875"
  },
  {
    "text": "For the same inputs that you put into the system, you may get totally different outputs the next time you interact with that system.",
    "start": "320458",
    "end": "327583"
  },
  {
    "text": "It can introduce substantial randomness and variability as you're working with the system, so for example, if you call a system",
    "start": "329625",
    "end": "335666"
  },
  {
    "text": "that interacts with an LLM with the same prompt twice, you may get totally different output, totally different tool calls used,",
    "start": "335666",
    "end": "342041"
  },
  {
    "text": "and so this is really in kind of stark contrast to our traditional software systems,",
    "start": "343291",
    "end": "349500"
  },
  {
    "text": "where usually you get the same output for the same input. And this complexity gets further compounded",
    "start": "349875",
    "end": "356916"
  },
  {
    "text": "if the LLM is also dynamic in that it can control sort of which code paths it takes through the application,",
    "start": "358083",
    "end": "364541"
  },
  {
    "text": "and that gets a little more into the agentic side of things, where the LLM really has a little more control",
    "start": "364875",
    "end": "371125"
  },
  {
    "text": "over what it's doing, but again, the complexity just keeps growing as we add more features to these applications.",
    "start": "371125",
    "end": "377166"
  },
  {
    "start": "378000",
    "end": "480000"
  },
  {
    "text": "So how do we handle all of the added complexity that comes along with building systems that also leverage features of AI?",
    "start": "378708",
    "end": "385416"
  },
  {
    "text": "Well, I think the answer is that we need to lean into our tooling, and I'm not trying to be egotistical or anything, but I'm gonna quote myself",
    "start": "386875",
    "end": "393250"
  },
  {
    "text": "(audience laughing) from earlier this year or last year in the fall.",
    "start": "393250",
    "end": "398875"
  },
  {
    "text": "In systems where non-determinism is the default, I think that effect is the antidote.",
    "start": "400000",
    "end": "404916"
  },
  {
    "text": "We want to lean into tooling, which is expressive enough in order to model all of this complex behavior.",
    "start": "405916",
    "end": "411958"
  },
  {
    "text": "We wanna maintain reliability and scalability as we discussed before. We wanna be able to test our applications",
    "start": "412541",
    "end": "418207"
  },
  {
    "text": "without bending over backwards for whatever framework we're using, and we wanna have clear insight",
    "start": "418458",
    "end": "424083"
  },
  {
    "text": "into the running application as well. So all of the above and more come out of the box with effect,",
    "start": "424083",
    "end": "429916"
  },
  {
    "text": "and so that's why I think effect really shines in the face of all of this complexity. Effect provides many tools to deal with the chaos",
    "start": "430500",
    "end": "437791"
  },
  {
    "text": "that comes along with building AI-powered applications. It's composable by default, so we can build up",
    "start": "437791",
    "end": "444291"
  },
  {
    "text": "really complex systems from very simple composable building blocks. We can define and track errors as part of our domain,",
    "start": "444291",
    "end": "451000"
  },
  {
    "text": "which allows us to have very clear semantics over how we're gonna handle these errors.",
    "start": "451625",
    "end": "457750"
  },
  {
    "text": "We can abstract business logic into easily injectable services,",
    "start": "458750",
    "end": "463583"
  },
  {
    "text": "which means our code becomes very compartmentalized and easily testable,",
    "start": "464083",
    "end": "469083"
  },
  {
    "text": "and we have out-of-the-box instrumentation that we can litter throughout our code base",
    "start": "470125",
    "end": "475375"
  },
  {
    "text": "that gives us extremely clear insight into our running system. So hopefully I've convinced you that effect",
    "start": "475875",
    "end": "481541"
  },
  {
    "start": "480000",
    "end": "570000"
  },
  {
    "text": "is the correct tool for handling all of this complexity, but I've been kind of using the term agentic system",
    "start": "481833",
    "end": "488333"
  },
  {
    "text": "and AI and a lot of different buzzwords, all interchangeably at this point, so let's talk a tiny bit more about what I mean",
    "start": "488333",
    "end": "495041"
  },
  {
    "text": "by agentic system. So Anthropik published a blog post back in December",
    "start": "495416",
    "end": "501541"
  },
  {
    "text": "where they talked about patterns for building agentic systems, and they also gave a definition that I kind of like",
    "start": "501541",
    "end": "508000"
  },
  {
    "text": "because it's extremely simple. An agentic system is generally a system",
    "start": "508333",
    "end": "513875"
  },
  {
    "text": "that utilizes what they call an augmented LLM for interactions to accomplish different tasks.",
    "start": "513875",
    "end": "520875"
  },
  {
    "text": "So you interact with an augmented LLM, which we'll talk about in a second, to accomplish various tasks.",
    "start": "521250",
    "end": "526041"
  },
  {
    "text": "And then they classified agentic systems into two major categories, workflows where code paths and execution",
    "start": "527125",
    "end": "533541"
  },
  {
    "text": "is kind of predetermined. There's minimal dynamicism, so the developer's really in control over what's going on.",
    "start": "533541",
    "end": "541500"
  },
  {
    "text": "And then more agents or agentic systems plus, whatever you want to call it,",
    "start": "542166",
    "end": "547541"
  },
  {
    "text": "where execution is much more self-directed by the LLM based on it making decisions",
    "start": "547750",
    "end": "554041"
  },
  {
    "text": "about what the best thing to do next is. And so these two, I think the two categories",
    "start": "554041",
    "end": "561500"
  },
  {
    "text": "are helpful to keep in mind as we go through this. We're primarily going to focus on workflows during this talk,",
    "start": "561500",
    "end": "566958"
  },
  {
    "text": "but it's helpful to keep this in mind. And I mentioned before, the core of any agentic system",
    "start": "567416",
    "end": "572541"
  },
  {
    "start": "570000",
    "end": "687000"
  },
  {
    "text": "is what Anthropik refers to as the augmented LLM, where you've still got your",
    "start": "572541",
    "end": "577666"
  },
  {
    "text": "normal input-output interactions with the large language model,",
    "start": "577666",
    "end": "582041"
  },
  {
    "text": "but you've also enhanced that LLM with different features to kind of augment the response that you get, right?",
    "start": "583125",
    "end": "590750"
  },
  {
    "text": "We give the LLM the ability to retrieve information so it can generate search queries over different datasets.",
    "start": "591000",
    "end": "596333"
  },
  {
    "text": "For example, a company's internal knowledge base, you could give the LLM access to that,",
    "start": "597333",
    "end": "603000"
  },
  {
    "text": "to query for information in order to handle a input from a user.",
    "start": "603416",
    "end": "607916"
  },
  {
    "text": "You can give an LLM access to various tools that it can use to answer a query,",
    "start": "608666",
    "end": "614625"
  },
  {
    "text": "like giving it the ability to use a web browser to search for things. And you can also give an LLM memory,",
    "start": "615791",
    "end": "620583"
  },
  {
    "text": "which can either refer to expanding context windows available to the LLM, or you can also let the LLM",
    "start": "621666",
    "end": "628458"
  },
  {
    "text": "store information into external knowledge bases. And Anthropik makes the assertion that once you've got",
    "start": "628458",
    "end": "635958"
  },
  {
    "text": "this basic building block, you can basically use it to compose together much more complex agentic workflows.",
    "start": "635958",
    "end": "644916"
  },
  {
    "text": "From here on out in the presentation, we're gonna take a practical look at how we can use effects AI integration packages to achieve",
    "start": "646041",
    "end": "653291"
  },
  {
    "text": "the augmented LLM. Unfortunately, we don't have the time to actually get into implementing all of the agentic design patterns",
    "start": "653833",
    "end": "660000"
  },
  {
    "text": "that Anthropik published in their blog post. But I did start working on porting all of Anthropik's",
    "start": "660000",
    "end": "668541"
  },
  {
    "text": "examples from Python into effect. And the repository that I've just open sourced for this talk contains three out of the five patterns",
    "start": "668583",
    "end": "675916"
  },
  {
    "text": "already implemented with the effect AI packages that you guys can look at. And by the end of the week, we'll have the rest all done as well.",
    "start": "676375",
    "end": "682625"
  },
  {
    "text": "So let's keep going. We'll start off by talking about how we can use effect first",
    "start": "684833",
    "end": "692041"
  },
  {
    "start": "687000",
    "end": "915000"
  },
  {
    "text": "just to interact with LLM providers. Given that a core piece of the augmented LLM is first",
    "start": "692375",
    "end": "698041"
  },
  {
    "text": "just being able to talk to the large language model, right? And this past fall at the effect meetup in San Francisco,",
    "start": "698041",
    "end": "705500"
  },
  {
    "text": "I introduced effects AI integration packages, which first consists of the base effect AI package,",
    "start": "706833",
    "end": "713708"
  },
  {
    "text": "which is meant to provide a single unified interface that you can use to describe interactions",
    "start": "714375",
    "end": "721208"
  },
  {
    "text": "with LLM provider services. We'll talk a little bit more about that in specifics in a bit.",
    "start": "721416",
    "end": "726541"
  },
  {
    "text": "And then we also have so far two provider integration packages that allow us to provide concrete implementations",
    "start": "727166",
    "end": "734458"
  },
  {
    "text": "of the different generic services from the base package that allow you to talk directly to either one",
    "start": "735500",
    "end": "741791"
  },
  {
    "text": "or more providers in your application. So now we'll focus a little bit on how we can actually",
    "start": "741791",
    "end": "747125"
  },
  {
    "text": "use these packages to do what we want.",
    "start": "747125",
    "end": "748833"
  },
  {
    "text": "In order to talk about how we can interact with LLMs, we first need to describe how we can describe interacting",
    "start": "755458",
    "end": "764625"
  },
  {
    "text": "with an LLM using our AI integration packages. Our goal with these packages was to allow developers",
    "start": "764833",
    "end": "772625"
  },
  {
    "text": "to focus on the business logic of their application and then plug in the provider you want to use later",
    "start": "772833",
    "end": "779125"
  },
  {
    "text": "without having to worry about all of the different provider-specific quirks that come along with each provider.",
    "start": "779708",
    "end": "786333"
  },
  {
    "text": "So you get a generic interface that you can use. And you let us worry about the complexity",
    "start": "786333",
    "end": "792000"
  },
  {
    "text": "of unifying all of the different providers together. So in this example, we're using the completions service",
    "start": "792291",
    "end": "801541"
  },
  {
    "text": "from the base effect AI package, which is meant to represent generating text from an LLM.",
    "start": "802833",
    "end": "809000"
  },
  {
    "text": "And you can see here we're accessing the completion service in our program here.",
    "start": "810583",
    "end": "816166"
  },
  {
    "text": "And then we're using the completion service to generate some text.",
    "start": "816791",
    "end": "822666"
  },
  {
    "text": "In this case, we want to generate a hilarious dad joke, because everybody knows that I love dad jokes.",
    "start": "823583",
    "end": "829250"
  },
  {
    "text": "And then we're just logging the response that we get back from the LLM to the console.",
    "start": "830333",
    "end": "833916"
  },
  {
    "text": "And if we take a look at the type of this program, you'll see that now we end up with completions",
    "start": "836333",
    "end": "840708"
  },
  {
    "text": "in the requirements of our program, which means that at some point, we need to provide a concrete implementation of completions",
    "start": "841416",
    "end": "847500"
  },
  {
    "text": "to actually get this program to run. And that's what the provider integration packages are for.",
    "start": "847916",
    "end": "853250"
  },
  {
    "text": "So if we use the open AI provider integration package to provide the concrete implementation of completions,",
    "start": "853541",
    "end": "859291"
  },
  {
    "text": "we would end up interacting with open AI's chat completions API. But if we instead plug in anthropic,",
    "start": "859916",
    "end": "866416"
  },
  {
    "text": "we'd end up using their messages API. But the responses that you get back are unified into a unified data model",
    "start": "866416",
    "end": "875125"
  },
  {
    "text": "that we call AI response, so that again, you don't have to worry about all the different quirks that come along with these different providers.",
    "start": "875125",
    "end": "881708"
  },
  {
    "text": "So now that we've got a program where we're actually using an LLM",
    "start": "883208",
    "end": "890375"
  },
  {
    "text": "to do something, we're trying to generate some text here, we can actually use one of effects provider integration packages",
    "start": "890375",
    "end": "896500"
  },
  {
    "text": "to try to make the program runnable. So provider specific integration packages, like I said,",
    "start": "897291",
    "end": "904208"
  },
  {
    "text": "expose concrete implementations of services they support from the base package. So we're using completions in our get dad joke program.",
    "start": "904750",
    "end": "912916"
  },
  {
    "text": "So in order to make that program runnable, we decided we want to use open AI.",
    "start": "914458",
    "end": "919000"
  },
  {
    "start": "915000",
    "end": "1028000"
  },
  {
    "text": "So we can use the open AI completions module from the open AI integration",
    "start": "919958",
    "end": "926166"
  },
  {
    "text": "package in order to try to get this program to run.",
    "start": "926333",
    "end": "929500"
  },
  {
    "text": "So once we've imported open AI completions, we can then use open AI completions to create a data",
    "start": "931666",
    "end": "939083"
  },
  {
    "text": "type that we call in our AI integrations, the AI model.",
    "start": "939250",
    "end": "943625"
  },
  {
    "text": "And an AI model is meant to describe a type that",
    "start": "945000",
    "end": "950708"
  },
  {
    "text": "will provide a particular service, in this case, completions, and then will require a particular client or maybe other services",
    "start": "950708",
    "end": "957625"
  },
  {
    "text": "in order to actually be able to run. And bear with me for a second, because I know this looks a little bit different than maybe some",
    "start": "957625",
    "end": "963625"
  },
  {
    "text": "of the code you've seen written in effect before. We're going to get into why we took this approach and why I think it is definitely the right approach",
    "start": "963625",
    "end": "970500"
  },
  {
    "text": "and also provides you as the developer with a ton of flexibility.",
    "start": "970500",
    "end": "973208"
  },
  {
    "text": "So like I said, this AI model, using open AI completions.model, gives us a AI model that will",
    "start": "975958",
    "end": "982083"
  },
  {
    "text": "create us a completion service that we can provide to programs. It's going to use GPT-40 as its model.",
    "start": "982083",
    "end": "988333"
  },
  {
    "text": "And then we actually, when we want to use this, first need to build it in our program.",
    "start": "989541",
    "end": "994416"
  },
  {
    "text": "So in order to do that, you can just yield it or pipe it to another effect method, whatever.",
    "start": "994958",
    "end": "1000041"
  },
  {
    "text": "They're just effects themselves, so they're yieldable. And then we actually get the concrete built model itself,",
    "start": "1000041",
    "end": "1006833"
  },
  {
    "text": "which we can then provide to programs that we want to use it. So once we provide the GPT-40 model to our Get Dad Drug",
    "start": "1007458",
    "end": "1017250"
  },
  {
    "text": "program, the completions requirement is satisfied for that program. And now if we take a look at the type of our main program,",
    "start": "1017291",
    "end": "1023791"
  },
  {
    "text": "the open AI client is the only thing left that we need to provide. So again, this might look a little bit different",
    "start": "1024833",
    "end": "1031541"
  },
  {
    "start": "1028000",
    "end": "1140000"
  },
  {
    "text": "than some of the effect code you've seen written before. But what are the benefits of taking this approach?",
    "start": "1031833",
    "end": "1037416"
  },
  {
    "text": "Well, one of the major benefits is if we want to run many different programs with the same model,",
    "start": "1038416",
    "end": "1043875"
  },
  {
    "text": "we can do that. Once the model is built, we can provide it to as many programs as we want. That's kind of cool.",
    "start": "1045125",
    "end": "1050875"
  },
  {
    "text": "But we can also mix and match providers. So if we also want to--",
    "start": "1051666",
    "end": "1056833"
  },
  {
    "text": "let's say that we know that Claude does a fantastic job with generating dad jokes. It does not.",
    "start": "1057041",
    "end": "1061958"
  },
  {
    "text": "Anybody at my workshop yesterday knows that it does not. But if we think that Claude does a really great job,",
    "start": "1064125",
    "end": "1071208"
  },
  {
    "text": "and we want to use Claude to generate our last dad joke, because we want it to really pack a good punch,",
    "start": "1071500",
    "end": "1077791"
  },
  {
    "text": "we can also import anthropic completions, use the anthropic completions",
    "start": "1078916",
    "end": "1084250"
  },
  {
    "text": "to interact with Claude37sonnet, and then build that model in our program",
    "start": "1084250",
    "end": "1090083"
  },
  {
    "text": "and provide it to the last get dad joke program. So you can see how mixing and matching models is also",
    "start": "1090083",
    "end": "1095291"
  },
  {
    "text": "really easy with this particular approach. And I'll draw your attention again.",
    "start": "1095291",
    "end": "1100291"
  },
  {
    "text": "So this is a highlight of mixing and matching the different models. But I'll draw your attention again to the type of main, which now also tells us",
    "start": "1101500",
    "end": "1108458"
  },
  {
    "text": "we need to provide an anthropic client to our program. All of the get dad joke programs, completions, requirements",
    "start": "1108458",
    "end": "1114791"
  },
  {
    "text": "have been satisfied. But now our main program is telling us, OK, you just now have to give us the clients",
    "start": "1114791",
    "end": "1119916"
  },
  {
    "text": "to interact with anthropic and open AI.",
    "start": "1119916",
    "end": "1123500"
  },
  {
    "text": "And I think one of the main benefits of this approach is that it really cleanly",
    "start": "1125916",
    "end": "1131375"
  },
  {
    "text": "supports the service constructor pattern in effect. So for anybody who's leaning very heavily",
    "start": "1131375",
    "end": "1136833"
  },
  {
    "text": "into layers in their applications, which I think is a really good model to take for composing services",
    "start": "1137000",
    "end": "1143083"
  },
  {
    "start": "1140000",
    "end": "1350000"
  },
  {
    "text": "together, this approach really shines",
    "start": "1143166",
    "end": "1148000"
  },
  {
    "text": "when you want to take all your LLM interactions and package them up into different services.",
    "start": "1148541",
    "end": "1153166"
  },
  {
    "text": "So in this case, we've got our dad jokes service here. Great name for a service.",
    "start": "1154250",
    "end": "1160125"
  },
  {
    "text": "And inside the constructor for that service, we first build our two models.",
    "start": "1161791",
    "end": "1166750"
  },
  {
    "text": "So now we've built these two models, and we can use them in the actual interface that we expose as part of this service.",
    "start": "1167083",
    "end": "1174041"
  },
  {
    "text": "So forget dad joke. And if we want to get a really grown-inducing dad joke, we can provide Claude to that one.",
    "start": "1174958",
    "end": "1179958"
  },
  {
    "text": "And what's cool about this is the AI requirements are now hidden from the",
    "start": "1181083",
    "end": "1187416"
  },
  {
    "text": "interface of the service that's exposed. When we actually use this service in a program,",
    "start": "1187416",
    "end": "1192000"
  },
  {
    "text": "we're only left with the dad jokes service as our requirement.",
    "start": "1193791",
    "end": "1198625"
  },
  {
    "text": "And instead-- so we can use dad jokes as we would normally, and we don't have to worry about the fact that it's interacting with LLMs.",
    "start": "1199750",
    "end": "1206125"
  },
  {
    "text": "And instead, the fact that there's a requirement on Anthropic and OpenAI and whatever",
    "start": "1207000",
    "end": "1211416"
  },
  {
    "text": "is lifted into layer composition. So now you can deal with the fact that you need to provide an Anthropic client and an OpenAI",
    "start": "1212375",
    "end": "1220000"
  },
  {
    "text": "client and everything like that. So we're on the level of the layer. And for anybody who's not familiar with effect.service",
    "start": "1220125",
    "end": "1227041"
  },
  {
    "text": "yet, it is in the docs. It's kind of like our new way of creating services where you get a tag and a layer at the same time",
    "start": "1227208",
    "end": "1233333"
  },
  {
    "text": "instead of having to define them separately. And the default property here is the layer",
    "start": "1233333",
    "end": "1238875"
  },
  {
    "text": "that's generated for you. So we end up with a layer that produces a dad joke service but ends up requiring the actual clients we need.",
    "start": "1238875",
    "end": "1245875"
  },
  {
    "text": "So up until this point, we still have the need to create clients",
    "start": "1248458",
    "end": "1254125"
  },
  {
    "text": "that we can provide to our program so that we can finally make it runnable. And each of our integration packages",
    "start": "1254125",
    "end": "1261166"
  },
  {
    "text": "export modules that make it really easy to create clients",
    "start": "1261500",
    "end": "1267375"
  },
  {
    "text": "as well. In this case, we're using the OpenAI client module to create a layer that will produce us an OpenAI client.",
    "start": "1267375",
    "end": "1276750"
  },
  {
    "text": "And it ends up requiring an HTTP client implementation, which we'll talk about in a second.",
    "start": "1277166",
    "end": "1281208"
  },
  {
    "text": "And the layer constructor that is exposed from these modules is really helpful when you're prototyping because you can just",
    "start": "1282833",
    "end": "1288583"
  },
  {
    "text": "hard code your API key right there. But obviously, when you get into production, you want to do things a little bit more properly.",
    "start": "1289416",
    "end": "1296500"
  },
  {
    "text": "So we also export a layer config constructor where you can provide configs inline to give the layer",
    "start": "1297000",
    "end": "1304833"
  },
  {
    "text": "everything it needs. And you're still ending up with a layer that produces the client that you want.",
    "start": "1304916",
    "end": "1309375"
  },
  {
    "text": "And when you're ready to then make the program runnable, we also have to provide some implementation of an HTTP",
    "start": "1310583",
    "end": "1318416"
  },
  {
    "text": "client to these LLM provider clients because all of them",
    "start": "1318625",
    "end": "1323958"
  },
  {
    "text": "use HTTP at the moment, I think. So in this case, we know we're",
    "start": "1323958",
    "end": "1329208"
  },
  {
    "text": "going to be running with node. So in order to avoid becoming dependent on some platform, we keep HTTP client in your requirements.",
    "start": "1329208",
    "end": "1335875"
  },
  {
    "text": "You give it the one that you want. And once we provide this to our main program, all of our requirements are satisfied",
    "start": "1336083",
    "end": "1342000"
  },
  {
    "text": "if we're not using Anthropic as well. And we can now run this program.",
    "start": "1343333",
    "end": "1347583"
  },
  {
    "start": "1350000",
    "end": "1582000"
  },
  {
    "text": "But this is all well and good. This is cool. It's fine. But we haven't really talked about handling failures.",
    "start": "1350416",
    "end": "1358166"
  },
  {
    "text": "Obviously, when interacting with applications that depend on calling out to an LLM, there are many different failure scenarios",
    "start": "1358416",
    "end": "1364083"
  },
  {
    "text": "that need to be handled-- network outages, provider outages.",
    "start": "1364083",
    "end": "1367583"
  },
  {
    "text": "Providers do go down. Incorrect request response formats.",
    "start": "1369708",
    "end": "1374750"
  },
  {
    "text": "And in particular, as a dad, it's extremely important to me that when I think of a really great dad joke,",
    "start": "1375625",
    "end": "1382125"
  },
  {
    "text": "that it's delivered to my victims-- I mean, children-- without any issues.",
    "start": "1383666",
    "end": "1389500"
  },
  {
    "text": "Even if it fails, I need to retry that as many times as possible. So we've actually introduced something",
    "start": "1390458",
    "end": "1397000"
  },
  {
    "text": "that I want to talk about in this-- to motivate error handling-- something called the AI plan.",
    "start": "1397416",
    "end": "1402541"
  },
  {
    "text": "So let's motivate that for a few moments. If we rewritten the logic of our dad joke to now potentially fail with either a network error",
    "start": "1403125",
    "end": "1411625"
  },
  {
    "text": "or a provider outage error, our main program will now reflect that.",
    "start": "1411625",
    "end": "1416833"
  },
  {
    "text": "We could potentially have a get dad joke error coming from our program here.",
    "start": "1418083",
    "end": "1424000"
  },
  {
    "text": "Even if we've satisfied the model. So how can we potentially deal with this?",
    "start": "1424125",
    "end": "1430416"
  },
  {
    "text": "Well, AI plan is a new module that we've just released, I think, a week or two ago.",
    "start": "1430541",
    "end": "1436416"
  },
  {
    "text": "And what it allows you to do is build up a plan of execution by composing different models together.",
    "start": "1438000",
    "end": "1444166"
  },
  {
    "text": "So we'll go through this example, and then maybe it'll become a little more clear. But in this case, the first thing that we do",
    "start": "1445416",
    "end": "1452500"
  },
  {
    "text": "is we create a base AI plan from our GPT-40 open AI completions model.",
    "start": "1452708",
    "end": "1458166"
  },
  {
    "text": "And from model takes some arguments here. So what we're saying here is, as this first step in this plan,",
    "start": "1459458",
    "end": "1464958"
  },
  {
    "text": "we want to try interacting with open AI three times with an exponential back off.",
    "start": "1465458",
    "end": "1470708"
  },
  {
    "text": "And we only want to do that while the error is a network error. So if we have a provider outage, we",
    "start": "1471166",
    "end": "1477833"
  },
  {
    "text": "want to fall back to the next provider immediately. So how can we actually do the fallback?",
    "start": "1477833",
    "end": "1483333"
  },
  {
    "text": "Well, AI plans are composable. So you can then add a second step to your plan, where you say,",
    "start": "1484791",
    "end": "1491166"
  },
  {
    "text": "OK, if everything with open AI fails, if all the conditions we have in our retry logic and everything",
    "start": "1492000",
    "end": "1497291"
  },
  {
    "text": "fail, fall back to anthropic and use Claude to generate our dad joke. And only do that if there's a provider outage.",
    "start": "1497291",
    "end": "1503958"
  },
  {
    "text": "And what you end up with is now this new AI plan data type, which is kind of similar to AI model.",
    "start": "1508708",
    "end": "1513458"
  },
  {
    "text": "We generate a completions service for you, and you end up requiring some clients. But you also have the error in the type here",
    "start": "1513791",
    "end": "1521500"
  },
  {
    "text": "that's going to be handled by the plan. And we're still working on adding more methods",
    "start": "1521500",
    "end": "1527541"
  },
  {
    "text": "to the AI plan module. So at the moment, the error still shows up in our main program because it's possible that we",
    "start": "1527875",
    "end": "1533208"
  },
  {
    "text": "go through all of the retry logic and everything, and we still have that error as part of our program.",
    "start": "1533208",
    "end": "1538083"
  },
  {
    "text": "But you can yield the plan just like you would yield a model, and then provide the plan to the get dad joke.",
    "start": "1539041",
    "end": "1544916"
  },
  {
    "text": "So what's going to happen here? It's going to try open AI. If all of the conditions we've set fail,",
    "start": "1545375",
    "end": "1552041"
  },
  {
    "text": "it'll fall back to anthropic. And then if everything else fails, the program will fail as a whole.",
    "start": "1552500",
    "end": "1557500"
  },
  {
    "text": "So what's really powerful about this is that it allows you to very granularly describe and plan",
    "start": "1558291",
    "end": "1565833"
  },
  {
    "text": "out how you're going to execute with different providers as part of your AI interactions.",
    "start": "1565833",
    "end": "1570208"
  },
  {
    "text": "If you have suggestions, by the way, about different things we could add to AI plan, please let us know.",
    "start": "1572125",
    "end": "1577291"
  },
  {
    "text": "We've kept the API surface intentionally small for now because we're looking for feedback.",
    "start": "1577291",
    "end": "1581500"
  },
  {
    "start": "1582000",
    "end": "1698000"
  },
  {
    "text": "Now, there may be some of you out there saying, yeah, that's all pretty cool. But I still can't write my code in a provider agnostic way.",
    "start": "1583125",
    "end": "1590000"
  },
  {
    "text": "I need to set this very specific configuration parameter in this very specific location",
    "start": "1590000",
    "end": "1595416"
  },
  {
    "text": "for my code to work perfectly. And I know this is exactly what I need because it's fantastic.",
    "start": "1595416",
    "end": "1599458"
  },
  {
    "text": "And we support that too. So if we have our get that program here again,",
    "start": "1601250",
    "end": "1606083"
  },
  {
    "text": "let's say that we want to set some specific open AI configuration.",
    "start": "1607458",
    "end": "1612208"
  },
  {
    "text": "Well, all of our modules that support overriding",
    "start": "1612500",
    "end": "1617291"
  },
  {
    "text": "particular pieces of configuration, export a configure override method, that allows you to override those configuration variables.",
    "start": "1617708",
    "end": "1624125"
  },
  {
    "text": "And what's cool about this is you can mix and match them together in your program. And those configuration variables will only be used",
    "start": "1625250",
    "end": "1632750"
  },
  {
    "text": "if that client is in use. So if you're not using Anthropic to run this program, great, we won't use those configs.",
    "start": "1632750",
    "end": "1639166"
  },
  {
    "text": "But if you are, then those configs will be applied. And what's even cooler is the AI models themselves",
    "start": "1639166",
    "end": "1645208"
  },
  {
    "text": "take default configurations. So if you want to set defaults for this particular model,",
    "start": "1645500",
    "end": "1650208"
  },
  {
    "text": "you can do that. You can override them in particular programs if you want to. So the configuration that you apply",
    "start": "1651125",
    "end": "1657583"
  },
  {
    "text": "as part of these LLM interactions is extremely flexible.",
    "start": "1657750",
    "end": "1660291"
  },
  {
    "text": "So we've seen a bunch of features, effects AI integrations have, but we still haven't really",
    "start": "1663375",
    "end": "1668583"
  },
  {
    "text": "explored any of the three pieces that make an LLM interaction augmented.",
    "start": "1668583",
    "end": "1673375"
  },
  {
    "text": "Though some providers like OpenAI are offering vector stores and file search and other cool things to simplify,",
    "start": "1675000",
    "end": "1681041"
  },
  {
    "text": "building up the augmented LLM, my personal opinion is that the features and services to support this, to support retrieval and memory",
    "start": "1682125",
    "end": "1691333"
  },
  {
    "text": "specifically should be our best implemented in user land to avoid vendor lock-in and other things.",
    "start": "1691416",
    "end": "1697041"
  },
  {
    "start": "1698000",
    "end": "1892000"
  },
  {
    "text": "And one feature of the augmented LLM though that we can help with, that we want to help with",
    "start": "1698583",
    "end": "1705208"
  },
  {
    "text": "is dealing with tool calls. Effect AI has a, I think a pretty cool way",
    "start": "1705416",
    "end": "1712500"
  },
  {
    "text": "of defining tool calls at the moment. We have a module called the AI toolkit, which allows you to package up all the different tools",
    "start": "1712500",
    "end": "1719125"
  },
  {
    "text": "that you use into a toolkit, that you can then build as part of your program",
    "start": "1719125",
    "end": "1724875"
  },
  {
    "text": "and provide along with interacting with an LLM. So, and for those unfamiliar with how tool calls work,",
    "start": "1726000",
    "end": "1733166"
  },
  {
    "text": "it's basically you, when you submit a query to an LLM, you can also submit a list of tools it can use",
    "start": "1733166",
    "end": "1739250"
  },
  {
    "text": "and it will respond to you saying like, \"I wanna use X tool.\" So you use that tool and then send it back to the LLM.",
    "start": "1739250",
    "end": "1744833"
  },
  {
    "text": "But before we can let the LLM use the tools, we have to define the tools. So first, let's talk about how we do that",
    "start": "1747041",
    "end": "1753000"
  },
  {
    "text": "with the AI toolkit. So here we're using schema.tagged_request, which most of you are probably familiar with",
    "start": "1753000",
    "end": "1759250"
  },
  {
    "text": "from like a lot of other APIs that we have. We lean very heavily into schema for doing things",
    "start": "1759250",
    "end": "1765291"
  },
  {
    "text": "that need to be serializable. And in this case, we're saying our tool call is gonna have a search term that we can provide.",
    "start": "1765291",
    "end": "1771791"
  },
  {
    "text": "It's gonna succeed with the string and never fail. And this tool call is gonna allow us",
    "start": "1772958",
    "end": "1778208"
  },
  {
    "text": "to get some random dad joke from the icon has dad joke API.",
    "start": "1778208",
    "end": "1782125"
  },
  {
    "text": "Once we have our requests defined, right? We're saying this is the payload we're gonna submit",
    "start": "1784500",
    "end": "1789666"
  },
  {
    "text": "is a search term, we're gonna get back a string, we're never gonna fail.",
    "start": "1789833",
    "end": "1792625"
  },
  {
    "text": "Once we have that request defined, we can add it to an empty AI toolkit. And now we have our dad joke tools.",
    "start": "1795041",
    "end": "1801208"
  },
  {
    "text": "We can keep adding tools if we want to. And then for those of you familiar with the HTTP APIs",
    "start": "1801208",
    "end": "1807333"
  },
  {
    "text": "that we have, you can then implement the handlers for each of your tools. So dad joke tools, a toolkit comes with an implement method",
    "start": "1807625",
    "end": "1815333"
  },
  {
    "text": "which allows you to provide an actual implementation of each one of the different tools you have in the toolkit.",
    "start": "1815750",
    "end": "1821875"
  },
  {
    "text": "And if you run, if you, the implement method itself then returns a layer that has all of the different tools",
    "start": "1822875",
    "end": "1830583"
  },
  {
    "text": "already packaged up in it, which you can then provide to your program. And your program now has all of the things that it needs",
    "start": "1831000",
    "end": "1836750"
  },
  {
    "text": "to be able to use these tools. So this is great, but how do we actually use the tools?",
    "start": "1836750",
    "end": "1842791"
  },
  {
    "text": "Well, once you've got your layer built and your toolkit defined, you can just yield the toolkit",
    "start": "1843500",
    "end": "1850041"
  },
  {
    "text": "in a generator the same way that you yield the models, the same way that you yield plans, you have to build the toolkit.",
    "start": "1850416",
    "end": "1855666"
  },
  {
    "text": "And then you can just pass it along with any request to an LLM using the completions.toolkit.",
    "start": "1856000",
    "end": "1861000"
  },
  {
    "text": "And then we also have streaming versions of these APIs as well. But we try to make it as easy as we can to use tools",
    "start": "1862000",
    "end": "1871291"
  },
  {
    "text": "as part of your LLM interactions. So again, this API is early.",
    "start": "1871291",
    "end": "1877291"
  },
  {
    "text": "Everything in the AI packages is considered alpha at this point. So if you all have suggestions or things that you think",
    "start": "1878375",
    "end": "1883958"
  },
  {
    "text": "that we can do better or missing features, please let us know. We really, really, really are looking for feedback",
    "start": "1883958",
    "end": "1889083"
  },
  {
    "text": "and we need it. So some future directions and we'll close out.",
    "start": "1890000",
    "end": "1896083"
  },
  {
    "start": "1892000",
    "end": "1961000"
  },
  {
    "text": "We are still working on implementing different provider services at the moment. We just support completions or text generation",
    "start": "1897541",
    "end": "1904833"
  },
  {
    "text": "and embeddings, but we're working on implementing others. I have thoughts about the real-time API from OpenAI.",
    "start": "1905458",
    "end": "1912000"
  },
  {
    "text": "I have thoughts about batching and things like that, which other providers offer. So we're working on those things.",
    "start": "1912333",
    "end": "1918083"
  },
  {
    "text": "Tomorrow you can find us hacking on potentially an MCP implementation that's native",
    "start": "1918833",
    "end": "1924166"
  },
  {
    "text": "to the effect AI packages. And then we have some other unimplemented features that I have, that I wanna work on.",
    "start": "1924166",
    "end": "1931208"
  },
  {
    "text": "Just like different features that I think our AI integrations can probably support.",
    "start": "1932875",
    "end": "1937500"
  },
  {
    "text": "So for those of you who want access to the code, feel free to scan the QR code. That'll give you access to the repo.",
    "start": "1938750",
    "end": "1944458"
  },
  {
    "text": "I'm working on building the other patterns that I mentioned before, but I really wanna thank you all for your attention this morning and I also wanna thank you",
    "start": "1945083",
    "end": "1951500"
  },
  {
    "text": "all for coming to Effect Days. So yeah, very much appreciated. Thank you.",
    "start": "1951500",
    "end": "1956125"
  },
  {
    "text": "(audience applauds)",
    "start": "1956666",
    "end": "1958958"
  }
]